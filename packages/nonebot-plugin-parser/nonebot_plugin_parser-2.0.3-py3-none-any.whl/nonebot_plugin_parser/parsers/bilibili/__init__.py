import asyncio
import json
import re
from typing import Any, ClassVar
from typing_extensions import override

from bilibili_api import HEADERS, Credential, request_settings, select_client
from bilibili_api.video import Video
import msgspec
from nonebot import logger

from ...config import pconfig
from ...download import DOWNLOADER
from ...exception import DownloadException, DurationLimitException, ParseException
from ..base import BaseParser
from ..cookie import ck2dict
from ..data import (
    GraphicsContent,
    ImageContent,
    MediaContent,
    Platform,
)


class BilibiliParser(BaseParser):
    # å¹³å°ä¿¡æ¯
    platform: ClassVar[Platform] = Platform(name="bilibili", display_name="å“”å“©å“”å“©")

    # URL æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆkeyword, patternï¼‰
    patterns: ClassVar[list[tuple[str, str]]] = [
        ("bilibili", r"https?://(?:space|www|live|m|t)?\.?bilibili\.com/[A-Za-z\d\._?%&+\-=/#]+()()"),
        ("bili2233", r"https?://bili2233\.cn/[A-Za-z\d\._?%&+\-=/#]+()()"),
        ("b23", r"https?://b23\.tv/[A-Za-z\d\._?%&+\-=/#]+()()"),
        ("BV", r"(BV[1-9a-zA-Z]{10})(?:\s)?(\d{1,3})?"),
        ("av", r"av(\d{6,})(?:\s)?(\d{1,3})?"),
    ]

    def __init__(self):
        self.headers = HEADERS.copy()
        self._credential: Credential | None = None
        self._cookies_file = pconfig.config_dir / "bilibili_cookies.json"
        # é€‰æ‹©å®¢æˆ·ç«¯
        select_client("curl_cffi")
        # æ¨¡ä»¿æµè§ˆå™¨
        request_settings.set("impersonate", "chrome131")
        # ç¬¬äºŒå‚æ•°æ•°å€¼å‚è€ƒ curl_cffi æ–‡æ¡£
        # https://curl-cffi.readthedocs.io/en/latest/impersonate.html

    @override
    async def parse(self, matched: re.Match[str]):
        """è§£æ URL è·å–å†…å®¹ä¿¡æ¯å¹¶ä¸‹è½½èµ„æº

        Args:
            matched: æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¯¹è±¡ï¼Œç”±å¹³å°å¯¹åº”çš„æ¨¡å¼åŒ¹é…å¾—åˆ°

        Returns:
            ParseResult: è§£æç»“æœï¼ˆå·²ä¸‹è½½èµ„æºï¼ŒåŒ…å« Pathï¼‰

        Raises:
            ParseException: è§£æå¤±è´¥æ—¶æŠ›å‡º
        """
        # ä»åŒ¹é…å¯¹è±¡ä¸­è·å–åŸå§‹URL, è§†é¢‘ID, é¡µç 
        url, video_id, page_num = str(matched.group(0)), str(matched.group(1)), matched.group(2)

        # å¤„ç†çŸ­é“¾
        if "b23.tv" in url or "bili2233.cn" in url:
            url = await self.get_redirect_url(url, self.headers)

        avid, bvid = None, None
        # é“¾æ¥ä¸­æ˜¯å¦åŒ…å«BVï¼Œavå·
        if video_id:
            if video_id.isdigit():
                avid = int(video_id)
            else:
                bvid = video_id
        else:
            if _matched := re.search(r"(BV[\dA-Za-z]{10})[^?]*?(?:\?[^#]*?p=(\d{1,3}))?", url):
                bvid = _matched.group(1)
                page_num = _matched.group(2)
            elif _matched := re.search(r"av(\d{6,})[^?]*?(?:\?[^#]*?p=(\d{1,3}))?", url):
                avid = int(_matched.group(1))
                page_num = _matched.group(2)
            else:
                return await self.parse_others(url)

        page_num = int(page_num) if page_num and page_num.isdigit() else 1

        # è§£æè§†é¢‘ä¿¡æ¯
        return await self.parse_video(bvid=bvid, avid=avid, page_num=page_num)

    async def parse_video(
        self,
        *,
        bvid: str | None = None,
        avid: int | None = None,
        page_num: int = 1,
    ):
        """è§£æè§†é¢‘ä¿¡æ¯

        Args:
            bvid (str | None): bvid
            avid (int | None): avid
            page_num (int): é¡µç 
        """

        from .video import AIConclusion, VideoInfo

        video = await self._parse_video(bvid=bvid, avid=avid)

        # è½¬æ¢ä¸º msgspec struct
        video_info = msgspec.convert(await video.get_info(), VideoInfo)

        # å¤„ç†åˆ† p
        page_idx, title, duration, timestamp, cover_url = video_info.extract_info_with_page(page_num)

        # è·å– AI æ€»ç»“
        if self._credential:
            cid = await video.get_cid(page_idx)
            ai_conclusion = await video.get_ai_conclusion(cid)
            ai_conclusion = msgspec.convert(ai_conclusion, AIConclusion)
            ai_summary = ai_conclusion.summary
        else:
            ai_summary: str = "å“”å“©å“”å“© cookie æœªé…ç½®æˆ–å¤±æ•ˆ, æ— æ³•ä½¿ç”¨ AI æ€»ç»“"

        url = f"https://bilibili.com/{video_info.bvid}" + (f"?p={page_idx + 1}" if page_idx > 0 else "")

        # è§†é¢‘ä¸‹è½½ task
        async def download_video():
            output_path = pconfig.cache_dir / f"{video_info.bvid}-{page_num}.mp4"
            if output_path.exists():
                return output_path
            v_url, a_url = await self.get_download_urls(video=video, page_index=page_idx)
            if duration > pconfig.duration_maximum:
                raise DurationLimitException
            if a_url is not None:
                return await DOWNLOADER.download_av_and_merge(
                    v_url, a_url, output_path=output_path, ext_headers=self.headers
                )
            else:
                return await DOWNLOADER.streamd(v_url, file_name=output_path.name, ext_headers=self.headers)

        video_task = asyncio.create_task(download_video())

        return self.result(
            url=url,
            title=title,
            timestamp=timestamp,
            text=f"ç®€ä»‹ï¼š{video_info.desc}",
            author=self.create_author(video_info.owner.name, video_info.owner.face),
            contents=[self.create_video_content(video_task, cover_url, duration)],
            extra={"info": ai_summary},
        )

    async def parse_others(self, url: str):
        """è§£æå…¶ä»–ç±»å‹é“¾æ¥"""
        # åˆ¤æ–­é“¾æ¥ç±»å‹å¹¶è§£æ
        # 1. åŠ¨æ€/å›¾æ–‡ (opus)
        if "t.bilibili.com" in url or "/opus" in url:
            matched = re.search(r"/(\d+)", url)
            if not matched:
                raise ParseException("æ— æ•ˆçš„åŠ¨æ€é“¾æ¥")
            opus_id = int(matched.group(1))
            img_urls, text = await self.parse_opus(opus_id)

            # ä¸‹è½½å›¾ç‰‡
            contents: list[MediaContent] = []
            if img_urls:
                img_tasks = [DOWNLOADER.download_img(url, ext_headers=self.headers) for url in img_urls]
                contents.extend(ImageContent(task) for task in img_tasks)

            return self.result(title=f"åŠ¨æ€ - {opus_id}", text=text, contents=contents)

        # 2. ç›´æ’­
        if "/live" in url:
            match_result = re.search(r"/(\d+)", url)
            if not match_result:
                raise ParseException("æ— æ•ˆçš„ç›´æ’­é“¾æ¥")
            room_id = int(match_result.group(1))
            title, cover, keyframe = await self.parse_live(room_id)

            contents = []
            # ä¸‹è½½å°é¢
            if cover:
                cover_task = DOWNLOADER.download_img(cover, ext_headers=self.headers)
                contents.append(ImageContent(cover_task))

            # ä¸‹è½½å…³é”®å¸§
            if keyframe:
                keyframe_task = DOWNLOADER.download_img(keyframe, ext_headers=self.headers)
                contents.append(ImageContent(keyframe_task))

            return self.result(title="ç›´æ’­ - " + title, contents=contents)

        # 3. ä¸“æ 
        if "/read" in url:
            match_result = re.search(r"/cv(\d+)", url)
            if not match_result:
                raise ParseException("æ— æ•ˆçš„ä¸“æ é“¾æ¥")
            read_id = int(match_result.group(1))
            texts, img_urls = await self.parse_read(read_id)
            combined_text = "\n".join(texts)

            # ä¸‹è½½å›¾ç‰‡
            contents = []
            if img_urls:
                img_tasks = [DOWNLOADER.download_img(url, ext_headers=self.headers) for url in img_urls]
                contents.extend(ImageContent(task) for task in img_tasks)

            return self.result(title=f"ä¸“æ  - {read_id}", text=combined_text, contents=contents)

        # 4. æ”¶è—å¤¹
        if "/favlist" in url:
            match_result = re.search(r"fid=(\d+)", url)
            if not match_result:
                raise ParseException("æ— æ•ˆçš„æ”¶è—å¤¹é“¾æ¥")
            fav_id = int(match_result.group(1))
            titles, cover_urls = await self.parse_favlist(fav_id)

            # å¹¶å‘ä¸‹è½½å°é¢
            cover_tasks = [DOWNLOADER.download_img(url, ext_headers=self.headers) for url in cover_urls]
            contents = [GraphicsContent(task, title) for task, title in zip(cover_tasks, titles)]
            return self.result(
                title=f"æ”¶è—å¤¹ - {fav_id}",
                contents=contents,
            )

        raise ParseException("ä¸æ”¯æŒçš„ Bilibili é“¾æ¥")

    async def _init_credential(self) -> Credential | None:
        """åˆå§‹åŒ– bilibili api"""

        if not pconfig.bili_ck:
            logger.warning("æœªé…ç½® r_bili_ck, æ— æ³•ä½¿ç”¨å“”å“©å“”å“© AI æ€»ç»“, å¯èƒ½æ— æ³•è§£æ 720p ä»¥ä¸Šç”»è´¨è§†é¢‘")
            return None

        credential = Credential.from_cookies(ck2dict(pconfig.bili_ck))
        if not await credential.check_valid() and self._cookies_file.exists():
            logger.info(f"r_bili_ck å·²è¿‡æœŸ, å°è¯•ä» {self._cookies_file} åŠ è½½")
            credential = Credential.from_cookies(json.loads(self._cookies_file.read_text()))
        else:
            logger.info(f"r_bili_ck æœ‰æ•ˆ, ä¿å­˜åˆ° {self._cookies_file}")
            self._cookies_file.write_text(json.dumps(credential.get_cookies()))

        return credential

    @property
    async def credential(self) -> Credential | None:
        """è·å–å“”å“©å“”å“©ç™»å½•å‡­è¯"""

        if self._credential is None:
            self._credential = await self._init_credential()
            if self._credential is None:
                return None

        if not await self._credential.check_valid():
            logger.warning("å“”å“©å“”å“© cookies å·²è¿‡æœŸ, è¯·é‡æ–°é…ç½® r_bili_ck")
            return self._credential

        if await self._credential.check_refresh():
            logger.info("å“”å“©å“”å“© cookies éœ€è¦åˆ·æ–°")
            if self._credential.has_ac_time_value() and self._credential.has_bili_jct():
                await self._credential.refresh()
                logger.info(f"å“”å“©å“”å“© cookies åˆ·æ–°æˆåŠŸ, ä¿å­˜åˆ° {self._cookies_file}")
                self._cookies_file.write_text(json.dumps(self._credential.get_cookies()))
            else:
                logger.warning("å“”å“©å“”å“© cookies åˆ·æ–°éœ€è¦åŒ…å« SESSDATA, ac_time_value, bili_jct")

        return self._credential

    async def parse_opus(self, opus_id: int) -> tuple[list[str], str]:
        """è§£æåŠ¨æ€ä¿¡æ¯

        Args:
            opus_id (int): åŠ¨æ€ id

        Returns:
            tuple[list[str], str]: å›¾ç‰‡ url åˆ—è¡¨å’ŒåŠ¨æ€ä¿¡æ¯
        """
        from bilibili_api.opus import Opus

        opus = Opus(opus_id, await self.credential)
        opus_info = await opus.get_info()
        if not isinstance(opus_info, dict):
            raise ParseException("è·å–åŠ¨æ€ä¿¡æ¯å¤±è´¥")

        # è·å–å›¾ç‰‡ä¿¡æ¯
        urls = await opus.get_images_raw_info()
        urls = [url["url"] for url in urls]

        dynamic = opus.turn_to_dynamic()
        dynamic_info: dict[str, Any] = await dynamic.get_info()
        orig_text = (
            dynamic_info.get("item", {})
            .get("modules", {})
            .get("module_dynamic", {})
            .get("major", {})
            .get("opus", {})
            .get("summary", {})
            .get("rich_text_nodes", [{}])[0]
            .get("orig_text", "")
        )
        return urls, orig_text

    async def parse_live(self, room_id: int) -> tuple[str, str, str]:
        """è§£æç›´æ’­ä¿¡æ¯

        Args:
            room_id (int): ç›´æ’­ id

        Returns:
            tuple[str, str, str]: æ ‡é¢˜ã€å°é¢ã€å…³é”®å¸§
        """
        from bilibili_api.live import LiveRoom

        room = LiveRoom(room_display_id=room_id, credential=await self.credential)
        room_info: dict[str, Any] = (await room.get_room_info())["room_info"]
        title, cover, keyframe = (
            room_info["title"],
            room_info["cover"],
            room_info["keyframe"],
        )
        return (title, cover, keyframe)

    async def parse_read(self, read_id: int) -> tuple[list[str], list[str]]:
        """ä¸“æ è§£æ

        Args:
            read_id (int): ä¸“æ  id

        Returns:
            texts: list[str], urls: list[str]
        """
        from bilibili_api.article import Article

        ar = Article(read_id)

        # åŠ è½½å†…å®¹
        await ar.fetch_content()
        data = ar.json()

        def accumulate_text(node: dict):
            text = ""
            if "children" in node:
                for child in node["children"]:
                    text += accumulate_text(child) + " "
            if _text := node.get("text"):
                text += _text if isinstance(_text, str) else str(_text) + node["url"]
            return text

        urls: list[str] = []
        texts: list[str] = []
        for node in data.get("children", []):
            node_type = node.get("type")
            if node_type == "ImageNode":
                if img_url := node.get("url", "").strip():
                    urls.append(img_url)
                    # è¡¥ç©ºä¸²å ä½ç¬¦
                    texts.append("")
            elif node_type == "ParagraphNode":
                if text := accumulate_text(node).strip():
                    texts.append(text)
            elif node_type == "TextNode":
                if text := node.get("text", "").strip():
                    texts.append(text)
        return texts, urls

    async def parse_favlist(self, fav_id: int) -> tuple[list[str], list[str]]:
        """è§£ææ”¶è—å¤¹ä¿¡æ¯

        Args:
            fav_id (int): æ”¶è—å¤¹ id

        Returns:
            tuple[list[str], list[str]]: æ ‡é¢˜ã€å°é¢ã€ç®€ä»‹ã€é“¾æ¥
        """
        from bilibili_api.favorite_list import get_video_favorite_list_content

        fav_list: dict[str, Any] = await get_video_favorite_list_content(fav_id)
        if fav_list["medias"] is None:
            raise ParseException("æ”¶è—å¤¹å†…å®¹ä¸ºç©º, æˆ–è¢«é£æ§")
        # å–å‰ 50 ä¸ª
        medias_50: list[dict[str, Any]] = fav_list["medias"][:50]
        texts: list[str] = []
        urls: list[str] = []
        for fav in medias_50:
            title, cover, intro, link = (
                fav["title"],
                fav["cover"],
                fav["intro"],
                fav["link"],
            )
            link: str = link.replace("bilibili://video/", "https://bilibili.com/video/av")
            urls.append(cover)
            texts.append(f"ğŸ§‰ æ ‡é¢˜ï¼š{title}\nğŸ“ ç®€ä»‹ï¼š{intro}\nğŸ”— é“¾æ¥: {link}")
        return texts, urls

    async def _parse_video(self, *, bvid: str | None = None, avid: int | None = None) -> Video:
        """è§£æè§†é¢‘ä¿¡æ¯

        Args:
            bvid (str | None): bvid
            avid (int | None): avid
        """
        if avid:
            return Video(aid=avid, credential=await self.credential)
        elif bvid:
            return Video(bvid=bvid, credential=await self.credential)
        else:
            raise ParseException("avid å’Œ bvid è‡³å°‘æŒ‡å®šä¸€é¡¹")

    async def get_download_urls(
        self,
        *,
        video: Video | None = None,
        bvid: str | None = None,
        avid: int | None = None,
        page_index: int = 0,
    ) -> tuple[str, str | None]:
        """è§£æè§†é¢‘ä¸‹è½½é“¾æ¥

        Args:
            bvid (str | None): bvid
            avid (int | None): avid
            page_index (int): é¡µç´¢å¼• = é¡µç  - 1
        """

        from bilibili_api.video import (
            AudioStreamDownloadURL,
            VideoDownloadURLDataDetecter,
            VideoQuality,
            VideoStreamDownloadURL,
        )

        if video is None:
            video = await self._parse_video(bvid=bvid, avid=avid)
        # è·å–ä¸‹è½½æ•°æ®
        download_url_data = await video.get_download_url(page_index=page_index)
        detecter = VideoDownloadURLDataDetecter(download_url_data)
        streams = detecter.detect_best_streams(
            video_max_quality=VideoQuality._1080P,
            codecs=pconfig.bili_video_codes,
            no_dolby_video=True,
            no_hdr=True,
        )
        video_stream = streams[0]
        if not isinstance(video_stream, VideoStreamDownloadURL):
            raise DownloadException("æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘æµ")
        logger.debug(f"è§†é¢‘æµè´¨é‡: {video_stream.video_quality.name}, ç¼–ç : {video_stream.video_codecs}")

        audio_stream = streams[1]
        if not isinstance(audio_stream, AudioStreamDownloadURL):
            return video_stream.url, None
        logger.debug(f"éŸ³é¢‘æµè´¨é‡: {audio_stream.audio_quality.name}")
        return video_stream.url, audio_stream.url
