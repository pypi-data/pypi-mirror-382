//! Comprehensive test suite for clustering module.
//!
//! Tests cover:
//! - Helper functions (distance, nearest centroid, k-means)
//! - Intrinsic dimension estimation (Two-NN)
//! - Calinski-Harabasz variance ratio for K selection
//! - Threshold derivation
//! - OptimalKHeuristic end-to-end
//! - Edge cases (small N, high-dimensional, degenerate data)

use crate::clustering::{
    euclidean_dist, kmeans_lloyd, nearest_centroid, ClusteringHeuristic,
};

pub struct OptimalKHeuristic;

impl ClusteringHeuristic for OptimalKHeuristic {}
// -------------------- Helper function tests --------------------

#[test]
fn test_euclidean_dist_basic() {
    let a = vec![0.0, 0.0, 0.0];
    let b = vec![1.0, 1.0, 1.0];
    let dist = euclidean_dist(&a, &b);
    assert!((dist - 3.0_f64.sqrt()).abs() < 1e-10);
}

#[test]
fn test_euclidean_dist_identity() {
    let a = vec![3.5, -2.1, 4.8];
    let dist = euclidean_dist(&a, &a);
    assert!(dist.abs() < 1e-10);
}

#[test]
fn test_euclidean_dist_one_dimensional() {
    let a = vec![5.0];
    let b = vec![2.0];
    let dist = euclidean_dist(&a, &b);
    assert!((dist - 3.0).abs() < 1e-10);
}

#[test]
fn test_nearest_centroid_single() {
    let centroids = vec![vec![1.0, 2.0], vec![5.0, 6.0], vec![9.0, 10.0]];
    let query = vec![1.1, 2.1];
    let (idx, dist2) = nearest_centroid(&query, &centroids);
    assert_eq!(idx, 0);
    assert!(dist2 < 0.03);
}

#[test]
fn test_nearest_centroid_middle() {
    let centroids = vec![vec![0.0, 0.0], vec![5.0, 5.0], vec![10.0, 10.0]];
    let query = vec![4.9, 5.1];
    let (idx, _dist2) = nearest_centroid(&query, &centroids);
    assert_eq!(idx, 1);
}

#[test]
fn test_kmeans_lloyd_simple_clusters() {
    let rows = vec![
        vec![0.0, 0.0],
        vec![0.1, 0.1],
        vec![0.0, 0.1],
        vec![10.0, 10.0],
        vec![10.1, 10.1],
        vec![10.0, 10.1],
    ];

    let assignments = kmeans_lloyd(&rows, 2, 50, 128);

    let label0 = assignments[0];
    assert_eq!(assignments[1], label0);
    assert_eq!(assignments[2], label0);

    let label1 = assignments[3];
    assert_ne!(label0, label1);
    assert_eq!(assignments[4], label1);
    assert_eq!(assignments[5], label1);
}

#[test]
fn test_kmeans_lloyd_k_equals_n() {
    let rows = vec![vec![1.0], vec![2.0], vec![3.0]];
    let assignments = kmeans_lloyd(&rows, 3, 10, 128);
    let unique: std::collections::HashSet<_> = assignments.iter().collect();
    assert_eq!(unique.len(), 3);
}

// -------------------- Intrinsic dimension estimation --------------------

#[test]
fn test_intrinsic_dimension_line() {
    let mut rows = Vec::new();
    for i in 0..100 {
        let t = i as f64 / 10.0;
        rows.push(vec![t, 2.0 * t, 3.0 * t]);
    }

    let heuristic = OptimalKHeuristic;
    let id = heuristic.estimate_intrinsic_dimension(&rows, rows.len(), 3);

    println!("Estimated ID for 1D line: {}", id);
    assert!(id >= 1 && id <= 3, "Expected ID near 1, got {}", id);
}

#[test]
fn test_intrinsic_dimension_plane() {
    let mut rows = Vec::new();
    for i in 0..100 {
        let x = (i as f64 / 10.0).sin();
        let y = (i as f64 / 10.0).cos();
        rows.push(vec![x, y, 0.0]);
    }

    let heuristic = OptimalKHeuristic;
    let id = heuristic.estimate_intrinsic_dimension(&rows, rows.len(), 3);

    println!("Estimated ID for 2D plane: {}", id);
    assert!(id >= 1 && id <= 3, "Expected ID near 2, got {}", id);
}

#[test]
fn test_intrinsic_dimension_full_space() {
    let mut rows = Vec::new();
    for _ in 0..200 {
        rows.push(vec![
            rand::random(),
            rand::random(),
            rand::random(),
            rand::random(),
            rand::random(),
        ]);
    }

    let heuristic = OptimalKHeuristic;
    let id = heuristic.estimate_intrinsic_dimension(&rows, rows.len(), 5);

    println!("Estimated ID for 5D full space: {}", id);
    assert!(id >= 2 && id <= 5, "Expected ID near 5, got {}", id);
}

#[test]
fn test_intrinsic_dimension_small_n() {
    let rows = vec![vec![1.0, 2.0], vec![3.0, 4.0]];
    let heuristic = OptimalKHeuristic;
    let id = heuristic.estimate_intrinsic_dimension(&rows, 2, 2);
    assert!(id <= 2);
}

// -------------------- step 1: Bounds testing --------------------

#[test]
fn test_step1_bounds_small_dataset() {
    let rows = vec![vec![1.0]; 10];
    let heuristic = OptimalKHeuristic;
    let (k_min, k_max, _id) = heuristic.step1_bounds(&rows, 10, 1);

    println!("step 1 bounds (N=10, F=1): [{}, {}]", k_min, k_max);
    assert!(k_min >= 2, "k_min should be at least 2");
    assert!(k_max >= k_min, "k_max should be >= k_min");
    assert!(k_max <= 10, "k_max should not exceed N");
}

#[test]
fn test_step1_bounds_large_n_small_f() {
    let rows = vec![vec![0.0; 5]; 1000];
    let heuristic = OptimalKHeuristic;
    let (k_min, k_max, _id) = heuristic.step1_bounds(&rows, 1000, 5);

    println!("step 1 bounds (N=1000, F=5): [{}, {}]", k_min, k_max);
    assert!(k_min <= k_max);
    assert!(k_max <= 1000 / 10, "k_max should respect N/10 constraint");
}

#[test]
fn test_step1_bounds_high_dimensional() {
    let rows = vec![vec![0.0; 100]; 50];
    let heuristic = OptimalKHeuristic;
    let (k_min, k_max, _id) = heuristic.step1_bounds(&rows, 50, 100);

    println!("step 1 bounds (N=50, F=100): [{}, {}]", k_min, k_max);
    assert!(k_min >= 2);
    assert!(k_max <= 25, "k_max should not exceed N/2");
}

// -------------------- step 2: Calinski-Harabasz testing --------------------

#[test]
fn test_calinski_harabasz_well_separated() {
    use rand::Rng;
    let mut rng = rand::rng();
    let mut rows = Vec::new();

    // Two well-separated Gaussian clusters
    for _ in 0..50 {
        rows.push(vec![rng.random_range(-0.5..0.5), rng.random_range(-0.5..0.5)]);
    }
    for _ in 0..50 {
        rows.push(vec![
            10.0 + rng.random_range(-0.5..0.5),
            10.0 + rng.random_range(-0.5..0.5),
        ]);
    }

    let heuristic = OptimalKHeuristic;
    let k_suggested = heuristic.step2_calinski_harabasz(&rows, 2, 10);

    println!("Calinski-Harabasz suggested K: {} (expected 2)", k_suggested);
    assert!(
        k_suggested >= 2 && k_suggested <= 4,
        "Expected K around 2, got {}",
        k_suggested
    );
}

#[test]
fn test_calinski_harabasz_three_clusters() {
    use rand::Rng;
    let mut rng = rand::rng();
    let mut rows = Vec::new();

    for _ in 0..50 {
        rows.push(vec![rng.random_range(-0.5..0.5), rng.random_range(-0.5..0.5)]);
    }
    for _ in 0..50 {
        rows.push(vec![
            5.0 + rng.random_range(-0.5..0.5),
            5.0 + rng.random_range(-0.5..0.5),
        ]);
    }
    for _ in 0..50 {
        rows.push(vec![
            10.0 + rng.random_range(-0.5..0.5),
            10.0 + rng.random_range(-0.5..0.5),
        ]);
    }

    let heuristic = OptimalKHeuristic;
    let k_suggested = heuristic.step2_calinski_harabasz(&rows, 2, 10);

    println!("Calinski-Harabasz suggested K: {} (expected 3)", k_suggested);
    assert!(
        k_suggested >= 2 && k_suggested <= 5,
        "Expected K around 3, got {}",
        k_suggested
    );
}

#[test]
fn test_calinski_harabasz_single_cluster() {
    let mut rows = Vec::new();
    for i in 0..100 {
        let noise = (i as f64) * 0.001;
        rows.push(vec![5.0 + noise, 5.0 + noise]);
    }

    let heuristic = OptimalKHeuristic;
    let k_suggested = heuristic.step2_calinski_harabasz(&rows, 2, 10);

    println!("Calinski-Harabasz K for single cluster: {}", k_suggested);
    assert!(k_suggested >= 2, "Should return at least k_min");
}

// -------------------- Threshold derivation --------------------

#[test]
fn test_threshold_from_pilot_two_clusters() {
    let mut rows = Vec::new();
    for _ in 0..50 {
        rows.push(vec![0.0, 0.0]);
    }
    for _ in 0..50 {
        rows.push(vec![10.0, 10.0]);
    }

    let heuristic = OptimalKHeuristic;
    let radius = heuristic.compute_threshold_from_pilot(&rows, 2);

    println!("Threshold radius for two tight clusters: {:.6}", radius);

    // Points are IDENTICAL within each cluster (variance = 0), so fallback uses
    // inter-centroid distance: sqrt((10-0)^2 + (10-0)^2) = 14.14
    // Squared: 200, Ã— 0.15 = 30
    assert!(
        radius > 1.0 && radius < 50.0,
        "Expected moderate threshold for zero-variance clusters with inter-centroid gap, got {}",
        radius
    );
}

#[test]
fn test_threshold_from_pilot_large_variance() {
    let mut rows = Vec::new();
    for i in 0..100 {
        let noise = (i as f64 - 50.0) * 0.5;
        rows.push(vec![noise, noise]);
    }

    let heuristic = OptimalKHeuristic;
    let radius = heuristic.compute_threshold_from_pilot(&rows, 3);

    println!("Threshold radius for spread cluster: {:.6}", radius);
    assert!(radius > 1.0, "Expected larger threshold for spread data, got {}", radius);
}

#[test]
fn test_threshold_from_pilot_single_point_per_cluster() {
    let rows = vec![vec![0.0], vec![10.0], vec![20.0]];
    let heuristic = OptimalKHeuristic;
    let radius = heuristic.compute_threshold_from_pilot(&rows, 3);
    assert!(radius >= 0.0);
}

#[test]
fn test_threshold_zero_variance_clusters() {
    let rows = vec![vec![0.0, 0.0], vec![0.0, 0.0], vec![10.0, 10.0], vec![10.0, 10.0]];

    let heuristic = OptimalKHeuristic;
    let radius = heuristic.compute_threshold_from_pilot(&rows, 2);

    println!("Threshold for zero-variance clusters: {:.6}", radius);
    assert!(radius > 0.0, "Should use inter-centroid fallback for zero variance");
    assert!(radius > 1.0, "Inter-centroid fallback should give meaningful threshold");
}

#[test]
fn test_threshold_all_points_identical() {
    let rows = vec![vec![5.0, 5.0]; 10];
    let heuristic = OptimalKHeuristic;
    let radius = heuristic.compute_threshold_from_pilot(&rows, 3);

    println!("Threshold for identical points: {:.6}", radius);
    assert!(radius >= 1e-6, "Should return minimum threshold for degenerate data");
}

#[test]
fn test_threshold_very_tight_clusters() {
    let mut rows = Vec::new();
    for _ in 0..20 {
        rows.push(vec![0.0 + rand::random::<f64>() * 0.0001, 0.0]);
    }
    for _ in 0..20 {
        rows.push(vec![100.0 + rand::random::<f64>() * 0.0001, 0.0]);
    }

    let heuristic = OptimalKHeuristic;
    let radius = heuristic.compute_threshold_from_pilot(&rows, 2);

    println!("Threshold for very tight clusters: {:.6}", radius);
    assert!(
        radius > 0.01,
        "Should use inter-centroid distance, not tiny intra-cluster variance"
    );
}

// -------------------- End-to-end OptimalKHeuristic --------------------

#[test]
fn test_optimal_k_heuristic_synthetic_three_clusters() {
    use rand::Rng;
    let mut rng = rand::rng();
    let mut rows = Vec::new();

    for _ in 0..100 {
        rows.push(vec![
            rng.random_range(-0.5..0.5),
            rng.random_range(-0.5..0.5),
            rng.random_range(-0.5..0.5),
        ]);
    }

    for _ in 0..100 {
        rows.push(vec![
            5.0 + rng.random_range(-0.5..0.5),
            5.0 + rng.random_range(-0.5..0.5),
            5.0 + rng.random_range(-0.5..0.5),
        ]);
    }

    for _ in 0..100 {
        rows.push(vec![
            10.0 + rng.random_range(-0.5..0.5),
            10.0 + rng.random_range(-0.5..0.5),
            10.0 + rng.random_range(-0.5..0.5),
        ]);
    }

    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, rows.len(), 3);

    println!(
        "Optimal K={}, radius={:.6}, ID={} for 3-cluster synthetic",
        k, radius, id
    );
    assert!(k >= 2 && k <= 7, "Expected K around 3 for three clusters, got {}", k);
    assert!(radius > 0.0, "radius should be positive");
    assert!(id >= 1 && id <= 3, "Intrinsic dimension should be 1-3");
}

#[test]
fn test_optimal_k_heuristic_spherical_clusters() {
    use rand::Rng;
    let mut rng = rand::rng();
    let mut rows = Vec::new();

    let centers =
        vec![vec![0.0, 0.0], vec![10.0, 0.0], vec![0.0, 10.0], vec![10.0, 10.0]];

    for center in centers {
        for _ in 0..75 {
            rows.push(vec![
                center[0] + rng.random_range(-0.5..0.5),
                center[1] + rng.random_range(-0.5..0.5),
            ]);
        }
    }

    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, rows.len(), 2);

    println!(
        "Optimal K={}, radius={:.6}, ID={} for 4 spherical clusters",
        k, radius, id
    );
    assert!(k >= 3 && k <= 6, "Expected K around 4 for four clusters, got {}", k);
    assert!(radius > 0.0, "radius should be positive");
    assert!(id >= 1 && id <= 2, "Intrinsic dimension should be 1-2");
}

#[test]
fn test_optimal_k_heuristic_high_dimensional_random() {
    let mut rows = Vec::new();
    for _ in 0..200 {
        rows.push(vec![
            rand::random(),
            rand::random(),
            rand::random(),
            rand::random(),
            rand::random(),
            rand::random(),
            rand::random(),
            rand::random(),
        ]);
    }

    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, rows.len(), 8);

    println!("Optimal K={}, radius={:.6}, ID={} for 8D random", k, radius, id);
    assert!(k >= 2, "K should be at least 2");
    assert!(k <= 100, "K should respect N/10 constraint");
    assert!(radius > 0.0);
    assert!(id <= 8, "ID should not exceed F");
}

#[test]
fn test_optimal_k_heuristic_small_n() {
    let rows = vec![vec![1.0, 2.0], vec![1.1, 2.1], vec![5.0, 6.0], vec![5.1, 6.1]];

    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, 4, 2);

    println!("Optimal K={}, radius={:.6}, ID={} for N=4", k, radius, id);
    assert!(k >= 2, "K should be at least 2");
    assert!(k <= 4, "K should not exceed N");
    assert!(radius > 0.0);
}

#[test]
fn test_optimal_k_heuristic_degenerate_identical() {
    let rows = vec![vec![3.0, 4.0]; 100];
    let heuristic = OptimalKHeuristic;
    let (k, radius, _id) = heuristic.compute_optimal_k(&rows, 100, 2);

    println!("Optimal K={}, radius={:.6} for identical points", k, radius);
    assert!(k >= 2, "K should be at least 2 even for degenerate data");
    assert!(radius >= 0.0);
}

#[test]
fn test_optimal_k_heuristic_single_feature() {
    let mut rows = Vec::new();
    for i in 0..100 {
        rows.push(vec![i as f64]);
    }

    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, 100, 1);

    println!("Optimal K={}, radius={:.6}, ID={} for 1D uniform", k, radius, id);
    assert!(k >= 2, "K should be at least 2");
    assert_eq!(id, 1, "Intrinsic dimension should be 1 for 1D data");
    assert!(radius > 0.0);
}

// -------------------- Edge cases --------------------

#[test]
fn test_optimal_k_minimum_viable_dataset() {
    let rows = vec![vec![0.0, 0.0], vec![1.0, 1.0]];
    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, 2, 2);

    println!("Optimal K={}, radius={:.6}, ID={} for N=2", k, radius, id);
    assert!(k >= 2, "K should be at least 2");
    assert!(radius >= 0.0);
}

#[test]
fn test_optimal_k_very_high_dimensional() {
    let rows = vec![vec![0.0; 1000]; 20];
    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, 20, 1000);

    println!("Optimal K={}, radius={:.6}, ID={} for N=20, F=1000", k, radius, id);
    assert!(k >= 2);
    assert!(k <= 10, "K should not exceed N/2");
    assert!(id <= 1000);
}

#[test]
fn test_optimal_k_mixed_scale_features() {
    let mut rows = Vec::new();
    for i in 0..100 {
        rows.push(vec![(i as f64) * 0.001, (i as f64) * 1000.0]);
    }

    let heuristic = OptimalKHeuristic;
    let (k, radius, _id) = heuristic.compute_optimal_k(&rows, 100, 2);

    println!("Optimal K={}, radius={:.6} for mixed-scale features", k, radius);
    assert!(k >= 2);
    assert!(radius > 0.0);
}

// -------------------- K-means edge cases --------------------

#[test]
fn test_kmeans_k_greater_than_n() {
    let rows = vec![vec![1.0], vec![2.0]];
    let assignments = kmeans_lloyd(&rows, 5, 10, 128);
    assert_eq!(assignments.len(), 2);
    for &a in &assignments {
        assert!(a < 2, "Assignment {} is out of bounds for k=2", a);
    }
}

#[test]
#[should_panic]
fn test_kmeans_k_equals_zero() {
    let rows = vec![vec![1.0], vec![2.0]];
    let assignments = kmeans_lloyd(&rows, 0, 10, 128);
    assert!(assignments.is_empty(), "k=0 should return empty assignments");
}

#[test]
#[should_panic]
fn test_kmeans_single_row() {
    let rows = vec![vec![1.0, 2.0]];
    let assignments = kmeans_lloyd(&rows, 3, 10, 128);
    assert_eq!(assignments.len(), 1);
    assert_eq!(assignments[0], 0, "Single row should be in cluster 0");
}

#[test]
fn test_kmeans_empty_cluster_recovery() {
    let rows = vec![vec![0.0, 0.0], vec![0.001, 0.001], vec![100.0, 100.0]];

    let assignments = kmeans_lloyd(&rows, 3, 20, 128);

    assert_eq!(assignments.len(), 3);
    for &a in &assignments {
        assert!(a < 3, "Assignment out of bounds");
    }
}

#[test]
fn test_kmeans_convergence_early_stop() {
    let rows = vec![vec![5.0, 5.0]; 20];

    let assignments = kmeans_lloyd(&rows, 3, 100, 128);

    assert_eq!(assignments.len(), 20);
    let first_cluster = assignments[0];
    assert!(assignments.iter().all(|&a| a == first_cluster));
}

// -------------------- Integration test --------------------

#[test]
fn test_clustering_heuristic_trait_interface() {
    let rows = vec![vec![0.0, 0.0], vec![0.1, 0.1], vec![10.0, 10.0], vec![10.1, 10.1]];

    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, 4, 2);

    println!("Trait interface: K={}, radius={:.6}, ID={}", k, radius, id);
    assert!(k >= 2);
    assert!(radius > 0.0, "Radius should be positive, got {}", radius);
    assert!(id <= 2);
}

// -------------------- Benchmark-style test --------------------

#[test]
#[ignore]
fn test_optimal_k_performance_large_dataset() {
    use std::time::Instant;

    let mut rows = Vec::new();
    for _ in 0..10000 {
        rows.push(vec![
            rand::random::<f64>(),
            rand::random::<f64>(),
            rand::random::<f64>(),
            rand::random::<f64>(),
        ]);
    }

    let heuristic = OptimalKHeuristic;
    let start = Instant::now();
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, rows.len(), 4);
    let elapsed = start.elapsed();

    println!(
        "Large dataset (N=10000, F=4): K={}, radius={:.6}, ID={}, time={:?}",
        k, radius, id, elapsed
    );
    assert!(elapsed.as_secs() < 30, "Should complete within 30s");
}

// -------------------- Regression tests --------------------

#[test]
fn test_consistent_results_with_seed() {
    let rows = vec![vec![0.0, 0.0], vec![0.1, 0.1], vec![5.0, 5.0], vec![5.1, 5.1]];

    let heuristic = OptimalKHeuristic;
    let (k1, radius_1, id1) = heuristic.compute_optimal_k(&rows, 4, 2);
    let (k2, radius_2, id2) = heuristic.compute_optimal_k(&rows, 4, 2);

    assert_eq!(k1, k2, "K should be consistent");
    assert!((radius_1 - radius_2).abs() < radius_1 * 0.5, "radius should be similar");
    assert_eq!(id1, id2, "ID should be consistent");
}

// -------------------- Documentation example test --------------------

#[test]
fn test_readme_example() {
    let mut rows = Vec::new();
    for i in 0..50 {
        rows.push(vec![(i as f64) * 0.1, (i as f64) * 0.1]);
    }
    for i in 0..50 {
        rows.push(vec![10.0 + (i as f64) * 0.1, 10.0 + (i as f64) * 0.1]);
    }

    let heuristic = OptimalKHeuristic;
    let (k, radius, id) = heuristic.compute_optimal_k(&rows, rows.len(), 2);

    println!("README example: K={}, radius={:.6}, ID={}", k, radius, id);
    assert!(k >= 2, "Should detect at least 2 clusters");
    assert!(radius > 0.0);
}
