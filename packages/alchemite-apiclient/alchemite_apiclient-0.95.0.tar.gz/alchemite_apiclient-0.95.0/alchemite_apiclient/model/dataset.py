"""
    Alchemite

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)  # noqa: E501
    Contact: support@intellegens.com
    Generated by: https://openapi-generator.tech
"""


import re  # noqa: F401
import sys  # noqa: F401

from alchemite_apiclient.model_utils import (  # noqa: F401
    ApiTypeError,
    ModelComposed,
    ModelNormal,
    ModelSimple,
    cached_property,
    change_keys_js_to_python,
    convert_js_args_to_python_args,
    date,
    datetime,
    file_type,
    none_type,
    validate_get_composed_info,
    OpenApiModel
)
from alchemite_apiclient.exceptions import ApiAttributeError


def lazy_import():
    from alchemite_apiclient.model.categorical_column import CategoricalColumn
    from alchemite_apiclient.model.column_info import ColumnInfo
    from alchemite_apiclient.model.dataset_calculated_columns import DatasetCalculatedColumns
    from alchemite_apiclient.model.dataset_sharing import DatasetSharing
    from alchemite_apiclient.model.ordinal_column import OrdinalColumn
    globals()['CategoricalColumn'] = CategoricalColumn
    globals()['ColumnInfo'] = ColumnInfo
    globals()['DatasetCalculatedColumns'] = DatasetCalculatedColumns
    globals()['DatasetSharing'] = DatasetSharing
    globals()['OrdinalColumn'] = OrdinalColumn


class Dataset(ModelNormal):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.

    Attributes:
      allowed_values (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          with a capitalized key describing the allowed value and an allowed
          value. These dicts store the allowed enum values.
      attribute_map (dict): The key is attribute name
          and the value is json key in definition.
      discriminator_value_class_map (dict): A dict to go from the discriminator
          variable value to the discriminator class name.
      validations (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          that stores validations for max_length, min_length, max_items,
          min_items, exclusive_maximum, inclusive_maximum, exclusive_minimum,
          inclusive_minimum, and regex.
      additional_properties_type (tuple): A tuple of classes accepted
          as additional properties values.
    """

    allowed_values = {
        ('descriptor_columns',): {
            '0': 0,
            '1': 1,
        },
        ('status',): {
            'UPLOADING': "uploading",
            'PROCESSING': "processing",
            'UPLOADED': "uploaded",
            'FAILED': "failed",
        },
        ('complete_columns',): {
            '0': 0,
            '1': 1,
        },
    }

    validations = {
        ('tags',): {
        },
        ('calculated_columns',): {
            'min_items': 1,
        },
        ('vector_pairs',): {
            'min_items': 1,
        },
        ('model_count',): {
            'inclusive_minimum': 0,
        },
    }

    @cached_property
    def additional_properties_type():  # noqa
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded
        """
        return (bool, date, datetime, dict, float, int, list, str, none_type,)  # noqa: E501

    _nullable = False

    @cached_property
    def openapi_types():  # noqa
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded

        Returns
            openapi_types (dict): The key is attribute name
                and the value is attribute type.
        """
        lazy_import()
        return {
            'name': (str,),  # noqa: E501
            'row_count': (int,),  # noqa: E501
            'column_headers': ([str],),  # noqa: E501
            'descriptor_columns': ([int],),  # noqa: E501
            'id': (str,),  # noqa: E501
            'tags': ([str],),  # noqa: E501
            'notes': (str,),  # noqa: E501
            'status': (str,),  # noqa: E501
            'detail': (str,),  # noqa: E501
            'revises_id': (str,),  # noqa: E501
            'revision_ids': ([str],),  # noqa: E501
            'column_count': (int,),  # noqa: E501
            'initial_column_headers': ([str],),  # noqa: E501
            'categorical_columns': ([CategoricalColumn], none_type,),  # noqa: E501
            'ordinal_columns': ([OrdinalColumn], none_type,),  # noqa: E501
            'column_info': ([ColumnInfo],),  # noqa: E501
            'auto_detect_complete_columns': (bool,),  # noqa: E501
            'complete_columns': ([int],),  # noqa: E501
            'calculated_columns': ([DatasetCalculatedColumns],),  # noqa: E501
            'measurement_groups': ([int], none_type,),  # noqa: E501
            'column_fraction_data_present': ([float],),  # noqa: E501
            'data': (str,),  # noqa: E501
            'vector_pairs': ([[str]], none_type,),  # noqa: E501
            'model_count': (int,),  # noqa: E501
            'created_at': (int,),  # noqa: E501
            'shared_through': ([str],),  # noqa: E501
            'sharing': (DatasetSharing,),  # noqa: E501
            'extensions': ([bool, date, datetime, dict, float, int, list, str, none_type],),  # noqa: E501
        }

    @cached_property
    def discriminator():  # noqa
        return None


    attribute_map = {
        'name': 'name',  # noqa: E501
        'row_count': 'rowCount',  # noqa: E501
        'column_headers': 'columnHeaders',  # noqa: E501
        'descriptor_columns': 'descriptorColumns',  # noqa: E501
        'id': 'id',  # noqa: E501
        'tags': 'tags',  # noqa: E501
        'notes': 'notes',  # noqa: E501
        'status': 'status',  # noqa: E501
        'detail': 'detail',  # noqa: E501
        'revises_id': 'revisesId',  # noqa: E501
        'revision_ids': 'revisionIds',  # noqa: E501
        'column_count': 'columnCount',  # noqa: E501
        'initial_column_headers': 'initialColumnHeaders',  # noqa: E501
        'categorical_columns': 'categoricalColumns',  # noqa: E501
        'ordinal_columns': 'ordinalColumns',  # noqa: E501
        'column_info': 'columnInfo',  # noqa: E501
        'auto_detect_complete_columns': 'autoDetectCompleteColumns',  # noqa: E501
        'complete_columns': 'completeColumns',  # noqa: E501
        'calculated_columns': 'calculatedColumns',  # noqa: E501
        'measurement_groups': 'measurementGroups',  # noqa: E501
        'column_fraction_data_present': 'columnFractionDataPresent',  # noqa: E501
        'data': 'data',  # noqa: E501
        'vector_pairs': 'vectorPairs',  # noqa: E501
        'model_count': 'modelCount',  # noqa: E501
        'created_at': 'createdAt',  # noqa: E501
        'shared_through': 'sharedThrough',  # noqa: E501
        'sharing': 'sharing',  # noqa: E501
        'extensions': 'extensions',  # noqa: E501
    }

    read_only_vars = {
        'id',  # noqa: E501
        'status',  # noqa: E501
        'detail',  # noqa: E501
        'revision_ids',  # noqa: E501
        'column_count',  # noqa: E501
        'initial_column_headers',  # noqa: E501
        'column_info',  # noqa: E501
        'column_fraction_data_present',  # noqa: E501
        'model_count',  # noqa: E501
        'created_at',  # noqa: E501
        'shared_through',  # noqa: E501
    }

    _composed_schemas = {}

    @classmethod
    @convert_js_args_to_python_args
    def _from_openapi_data(cls, name, row_count, column_headers, descriptor_columns, *args, **kwargs):  # noqa: E501
        """Dataset - a model defined in OpenAPI

        Args:
            name (str):
            row_count (int): The number of rows in the array, not including column headers.
            column_headers ([str]): List of all column headers in the order they appear in the dataset.
            descriptor_columns ([int]): List of length equal to the number of columns where each element is 1 or 0.  A value of 1 denotes that the corresponding column is a descriptor column.  A descriptor column is an input-only column whose values will not need to be predicted.

        Keyword Args:
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            id (str): Unique identifier for the dataset.. [optional]  # noqa: E501
            tags ([str]): Optional tags to attach to the dataset. [optional]  # noqa: E501
            notes (str): An optional free field for notes about the model. [optional]  # noqa: E501
            status (str): Status of the dataset during different stages of ingestion. The dataset is set to 'uploading' if uploading in chunks. The dataset is set to 'processing' during ingestion into the datastore. The dataset is set to 'uploaded' once all processing tasks are finished. Models can only be trained on datasets that are 'processing' or 'uploaded'. [optional]  # noqa: E501
            detail (str): The error provided for why the dataset failed to upload if an error occured during dataset ingestion . [optional]  # noqa: E501
            revises_id (str): The UUID of the dataset this revisesId (its parent).. [optional]  # noqa: E501
            revision_ids ([str]): The UUIDs of the datasets that are revisions of this dataset (its children).. [optional]  # noqa: E501
            column_count (int): The number of columns in the array, not including row headers.. [optional]  # noqa: E501
            initial_column_headers ([str]): List of initial column headers, before extensions and calculatedColumns are applied.. [optional]  # noqa: E501
            categorical_columns ([CategoricalColumn], none_type): The possible categorical values for each categorical column.  There cannot be more than 1023 unique categorical values per column, and each value cannot be longer than 128 characters.  Categorical values can be wrapped in speech marks (\") in the csv to represent more complex strings containing special characters (i.e. commas), but speech marks are not allowed to appear anywhere apart from the beginning and end of a value.  Quoted categorical values cannot be used in vector categorical columns.  Quoted categoricals will be reduced to a form without explicit speech marks where possible, e.g. the values of \"red\" and red will be treated as identical.  Categorical values cannot consist of purely whitespace and cannot contain semicolons.  Leading/trailing whitespace around a categorical cell will be trimmed away, although surrounding whitespace enclosed within speech marks will be preserved.  Categorical values also cannot be words reserved for special numerical types, such as NaN, +NaN, -NaN and further variations. Categorical integers are deprecated, please use string values instead. . [optional]  # noqa: E501
            ordinal_columns ([OrdinalColumn], none_type): The possible ordinal values for each ordinal column.  . [optional]  # noqa: E501
            column_info ([ColumnInfo]): Additional information/statistics for each column, listed in the order they appear in the dataset.. [optional]  # noqa: E501
            auto_detect_complete_columns (bool): Whether to automatically tag descriptors in the dataset with no missing values as completeColumns or not. If set to 'true', then completeColumns cannot be specified.. [optional] if omitted the server will use the default value of False  # noqa: E501
            complete_columns ([int]): List of length equal to the number of columns where each element is 1 or 0.  A value of 1 denotes that the corresponding column is a \"complete column\".  This means the column must have no missing values in the dataset.  It is also recommended to not ask a model trained on this dataset to make predictions with missing values in a \"complete column\".  All \"complete columns\" must be descriptor columns as well.  Marking columns as \"complete columns\" can significantly speed up model training.  If `completeColumns` is not given then none of the columns will be marked as \"complete columns\". If given, autoDetectCompleteColumns must be set to 'false'.. [optional]  # noqa: E501
            calculated_columns ([DatasetCalculatedColumns]): Additional columns to be added to the dataset using an closed-form expression of other columns. They will be calculated for each row before being passed to Alchemite for training or predicting values. If any columns referenced in the expression are missing, the value of the calculated column in that row will also be missing. Each calculated column's expression may only reference columns in the original dataset, or those defined earlier in the list. The column name must not already appear in the dataset. No referenced column may be categorical. Each new column will be a non-complete descriptor column, unless all referenced columns are complete, when it will be a complete descriptor column. If present, calculated columns will be inserted as the last columns of the dataset. . [optional]  # noqa: E501
            measurement_groups ([int], none_type): A \"measurement group\" is a group of columns that are usually measured at the same time.  So when making predictions for one of these columns it is expected that the other columns in the measurement group will not be present.  The measurementGroups argument can be specified to avoid training a model that relies on values in a measurement group to predict other values in the same group.  measurementGroups is a list of length equal to the number columns in the training dataset specifying which measurement group (denoted by in integer) each column belongs to.  The order of measurementGroups must correspond to the training dataset's 'columnHeaders' parameter.  Descriptor columns should be included in measurementGroups but they will always be used, regardless of the measurement group they are in.  For example, if measurementGroups=[1,2,3,1] then the first and last columns are expected to be known simultaneously and so are in the same measurement group, while the second and third columns may be known or unknown regardless of the knowledge of other columns and so are in their own measurement groups.  If measurementGroups is not provided then it is assumed that every column is in its own measurement group. . [optional]  # noqa: E501
            column_fraction_data_present ([float]): Lists the fraction of values which are given in each of the columns.. [optional]  # noqa: E501
            data (str): The CSV specification we conform to can be found at https://www.rfc-editor.org/rfc/rfc4180.  A string in CSV format corresponding to a 2D array with row and column headers.  Row and column headers must be unique. Row and column headers containing leading/trailing whitespace will not be trimmed and will be interpreted as they appear in the data. Categorical and vector values are defined outside of the set specification, although rules for their implementation can be found under their respective sections.    Sets of 2D vectors can be included by mapping each axis to a column and separating the values corresponding to each vector with a semicolon. If these vectors are used in the dataset then the columns which are paired as vectors must be provided in the 'vectorPairs' argument as part of the POST request.  In the example below the 'time' and 'temperature' columns are paired as vectors so in the first line their values map to the vectors (0,10), (1,28), (2,35), (4,42).  , heat applied, time   , temperature A, 30         , 0;1;2;4, 10;28;35;42 B, 10         , 0;5    , 10;18 . [optional]  # noqa: E501
            vector_pairs ([[str]], none_type): A list of pairs of column names.  The columns in each pair are the axes for a 2D coordinate system. Deprecated, it is recommended that series-based data is split out over separate columns for each series point. . [optional]  # noqa: E501
            model_count (int): The number of models trained on the dataset. Only returned if user owns the dataset.. [optional]  # noqa: E501
            created_at (int): The Unix Timestamp in seconds when POST /datasets was called. If `0` (Unix system time zero) then creation timestamp unavailable. This can happen for older datasets. . [optional]  # noqa: E501
            shared_through ([str]): If a dataset has been shared with the user then this will show through which group(s) it has been shared. Won't be set if the user requesting the resource owns it. Deprecated: Please use `sharing` to determine how the access for the dataset was achieved. . [optional]  # noqa: E501
            sharing (DatasetSharing): [optional]  # noqa: E501
            extensions ([bool, date, datetime, dict, float, int, list, str, none_type]): Specify preprocessing methods to be ran on the dataset before it is uploaded. For any Alchemite methods that require new data being provided, the required data for the extension must be present to ensure the preprocessing can be done on the new data to match the dataset.  Alchemite can then use any generated data from the preprocessing step to make more informed predictions.  Use of extensions requires the equivalent alchemiteapi.extensions.<extension name> scope. This feature is experimental and any existing extensions are subject to breaking changes. If you would like access to extensions please contact Intellegens at support@intellegens.com . [optional]  # noqa: E501
        """

        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', False)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        self = super(OpenApiModel, cls).__new__(cls)

        if args:
            raise ApiTypeError(
                "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments." % (
                    args,
                    self.__class__.__name__,
                ),
                path_to_item=_path_to_item,
                valid_classes=(self.__class__,),
            )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        self.name = name
        self.row_count = row_count
        self.column_headers = column_headers
        self.descriptor_columns = descriptor_columns
        for var_name, var_value in kwargs.items():
            if var_name not in self.attribute_map and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        self.additional_properties_type is None:
                # discard variable.
                continue
            setattr(self, var_name, var_value)
        return self

    required_properties = set([
        '_data_store',
        '_check_type',
        '_spec_property_naming',
        '_path_to_item',
        '_configuration',
        '_visited_composed_classes',
    ])

    @convert_js_args_to_python_args
    def __init__(self, name, row_count, column_headers, descriptor_columns, *args, **kwargs):  # noqa: E501
        """Dataset - a model defined in OpenAPI

        Args:
            name (str):
            row_count (int): The number of rows in the array, not including column headers.
            column_headers ([str]): List of all column headers in the order they appear in the dataset.
            descriptor_columns ([int]): List of length equal to the number of columns where each element is 1 or 0.  A value of 1 denotes that the corresponding column is a descriptor column.  A descriptor column is an input-only column whose values will not need to be predicted.

        Keyword Args:
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            id (str): Unique identifier for the dataset.. [optional]  # noqa: E501
            tags ([str]): Optional tags to attach to the dataset. [optional]  # noqa: E501
            notes (str): An optional free field for notes about the model. [optional]  # noqa: E501
            status (str): Status of the dataset during different stages of ingestion. The dataset is set to 'uploading' if uploading in chunks. The dataset is set to 'processing' during ingestion into the datastore. The dataset is set to 'uploaded' once all processing tasks are finished. Models can only be trained on datasets that are 'processing' or 'uploaded'. [optional]  # noqa: E501
            detail (str): The error provided for why the dataset failed to upload if an error occured during dataset ingestion . [optional]  # noqa: E501
            revises_id (str): The UUID of the dataset this revisesId (its parent).. [optional]  # noqa: E501
            revision_ids ([str]): The UUIDs of the datasets that are revisions of this dataset (its children).. [optional]  # noqa: E501
            column_count (int): The number of columns in the array, not including row headers.. [optional]  # noqa: E501
            initial_column_headers ([str]): List of initial column headers, before extensions and calculatedColumns are applied.. [optional]  # noqa: E501
            categorical_columns ([CategoricalColumn], none_type): The possible categorical values for each categorical column.  There cannot be more than 1023 unique categorical values per column, and each value cannot be longer than 128 characters.  Categorical values can be wrapped in speech marks (\") in the csv to represent more complex strings containing special characters (i.e. commas), but speech marks are not allowed to appear anywhere apart from the beginning and end of a value.  Quoted categorical values cannot be used in vector categorical columns.  Quoted categoricals will be reduced to a form without explicit speech marks where possible, e.g. the values of \"red\" and red will be treated as identical.  Categorical values cannot consist of purely whitespace and cannot contain semicolons.  Leading/trailing whitespace around a categorical cell will be trimmed away, although surrounding whitespace enclosed within speech marks will be preserved.  Categorical values also cannot be words reserved for special numerical types, such as NaN, +NaN, -NaN and further variations. Categorical integers are deprecated, please use string values instead. . [optional]  # noqa: E501
            ordinal_columns ([OrdinalColumn], none_type): The possible ordinal values for each ordinal column.  . [optional]  # noqa: E501
            column_info ([ColumnInfo]): Additional information/statistics for each column, listed in the order they appear in the dataset.. [optional]  # noqa: E501
            auto_detect_complete_columns (bool): Whether to automatically tag descriptors in the dataset with no missing values as completeColumns or not. If set to 'true', then completeColumns cannot be specified.. [optional] if omitted the server will use the default value of False  # noqa: E501
            complete_columns ([int]): List of length equal to the number of columns where each element is 1 or 0.  A value of 1 denotes that the corresponding column is a \"complete column\".  This means the column must have no missing values in the dataset.  It is also recommended to not ask a model trained on this dataset to make predictions with missing values in a \"complete column\".  All \"complete columns\" must be descriptor columns as well.  Marking columns as \"complete columns\" can significantly speed up model training.  If `completeColumns` is not given then none of the columns will be marked as \"complete columns\". If given, autoDetectCompleteColumns must be set to 'false'.. [optional]  # noqa: E501
            calculated_columns ([DatasetCalculatedColumns]): Additional columns to be added to the dataset using an closed-form expression of other columns. They will be calculated for each row before being passed to Alchemite for training or predicting values. If any columns referenced in the expression are missing, the value of the calculated column in that row will also be missing. Each calculated column's expression may only reference columns in the original dataset, or those defined earlier in the list. The column name must not already appear in the dataset. No referenced column may be categorical. Each new column will be a non-complete descriptor column, unless all referenced columns are complete, when it will be a complete descriptor column. If present, calculated columns will be inserted as the last columns of the dataset. . [optional]  # noqa: E501
            measurement_groups ([int], none_type): A \"measurement group\" is a group of columns that are usually measured at the same time.  So when making predictions for one of these columns it is expected that the other columns in the measurement group will not be present.  The measurementGroups argument can be specified to avoid training a model that relies on values in a measurement group to predict other values in the same group.  measurementGroups is a list of length equal to the number columns in the training dataset specifying which measurement group (denoted by in integer) each column belongs to.  The order of measurementGroups must correspond to the training dataset's 'columnHeaders' parameter.  Descriptor columns should be included in measurementGroups but they will always be used, regardless of the measurement group they are in.  For example, if measurementGroups=[1,2,3,1] then the first and last columns are expected to be known simultaneously and so are in the same measurement group, while the second and third columns may be known or unknown regardless of the knowledge of other columns and so are in their own measurement groups.  If measurementGroups is not provided then it is assumed that every column is in its own measurement group. . [optional]  # noqa: E501
            column_fraction_data_present ([float]): Lists the fraction of values which are given in each of the columns.. [optional]  # noqa: E501
            data (str): The CSV specification we conform to can be found at https://www.rfc-editor.org/rfc/rfc4180.  A string in CSV format corresponding to a 2D array with row and column headers.  Row and column headers must be unique. Row and column headers containing leading/trailing whitespace will not be trimmed and will be interpreted as they appear in the data. Categorical and vector values are defined outside of the set specification, although rules for their implementation can be found under their respective sections.    Sets of 2D vectors can be included by mapping each axis to a column and separating the values corresponding to each vector with a semicolon. If these vectors are used in the dataset then the columns which are paired as vectors must be provided in the 'vectorPairs' argument as part of the POST request.  In the example below the 'time' and 'temperature' columns are paired as vectors so in the first line their values map to the vectors (0,10), (1,28), (2,35), (4,42).  , heat applied, time   , temperature A, 30         , 0;1;2;4, 10;28;35;42 B, 10         , 0;5    , 10;18 . [optional]  # noqa: E501
            vector_pairs ([[str]], none_type): A list of pairs of column names.  The columns in each pair are the axes for a 2D coordinate system. Deprecated, it is recommended that series-based data is split out over separate columns for each series point. . [optional]  # noqa: E501
            model_count (int): The number of models trained on the dataset. Only returned if user owns the dataset.. [optional]  # noqa: E501
            created_at (int): The Unix Timestamp in seconds when POST /datasets was called. If `0` (Unix system time zero) then creation timestamp unavailable. This can happen for older datasets. . [optional]  # noqa: E501
            shared_through ([str]): If a dataset has been shared with the user then this will show through which group(s) it has been shared. Won't be set if the user requesting the resource owns it. Deprecated: Please use `sharing` to determine how the access for the dataset was achieved. . [optional]  # noqa: E501
            sharing (DatasetSharing): [optional]  # noqa: E501
            extensions ([bool, date, datetime, dict, float, int, list, str, none_type]): Specify preprocessing methods to be ran on the dataset before it is uploaded. For any Alchemite methods that require new data being provided, the required data for the extension must be present to ensure the preprocessing can be done on the new data to match the dataset.  Alchemite can then use any generated data from the preprocessing step to make more informed predictions.  Use of extensions requires the equivalent alchemiteapi.extensions.<extension name> scope. This feature is experimental and any existing extensions are subject to breaking changes. If you would like access to extensions please contact Intellegens at support@intellegens.com . [optional]  # noqa: E501
        """

        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', False)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        if args:
            raise ApiTypeError(
                "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments." % (
                    args,
                    self.__class__.__name__,
                ),
                path_to_item=_path_to_item,
                valid_classes=(self.__class__,),
            )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        self.name = name
        self.row_count = row_count
        self.column_headers = column_headers
        self.descriptor_columns = descriptor_columns
        for var_name, var_value in kwargs.items():
            if var_name not in self.attribute_map and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        self.additional_properties_type is None:
                # discard variable.
                continue
            setattr(self, var_name, var_value)
            if var_name in self.read_only_vars:
                raise ApiAttributeError(f"`{var_name}` is a read-only attribute. Use `from_openapi_data` to instantiate "
                                     f"class with read only attributes.")
