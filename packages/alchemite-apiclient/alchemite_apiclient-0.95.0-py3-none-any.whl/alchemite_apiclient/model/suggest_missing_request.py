"""
    Alchemite

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)  # noqa: E501
    Contact: support@intellegens.com
    Generated by: https://openapi-generator.tech
"""


import re  # noqa: F401
import sys  # noqa: F401

from alchemite_apiclient.model_utils import (  # noqa: F401
    ApiTypeError,
    ModelComposed,
    ModelNormal,
    ModelSimple,
    cached_property,
    change_keys_js_to_python,
    convert_js_args_to_python_args,
    date,
    datetime,
    file_type,
    none_type,
    validate_get_composed_info,
    OpenApiModel
)
from alchemite_apiclient.exceptions import ApiAttributeError


def lazy_import():
    from alchemite_apiclient.model.suggest_missing_data import SuggestMissingData
    from alchemite_apiclient.model.suggest_missing_dataset_id import SuggestMissingDatasetID
    from alchemite_apiclient.model.suggest_missing_imputed_data import SuggestMissingImputedData
    globals()['SuggestMissingData'] = SuggestMissingData
    globals()['SuggestMissingDatasetID'] = SuggestMissingDatasetID
    globals()['SuggestMissingImputedData'] = SuggestMissingImputedData


class SuggestMissingRequest(ModelComposed):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.

    Attributes:
      allowed_values (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          with a capitalized key describing the allowed value and an allowed
          value. These dicts store the allowed enum values.
      attribute_map (dict): The key is attribute name
          and the value is json key in definition.
      discriminator_value_class_map (dict): A dict to go from the discriminator
          variable value to the discriminator class name.
      validations (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          that stores validations for max_length, min_length, max_items,
          min_items, exclusive_maximum, inclusive_maximum, exclusive_minimum,
          inclusive_minimum, and regex.
      additional_properties_type (tuple): A tuple of classes accepted
          as additional properties values.
    """

    allowed_values = {}

    validations = {}

    @cached_property
    def additional_properties_type():  # noqa
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded
        """
        lazy_import()
        return (bool, date, datetime, dict, float, int, list, str, none_type,)  # noqa: E501

    _nullable = False

    @cached_property
    def openapi_types():  # noqa
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded

        Returns
            openapi_types (dict): The key is attribute name
                and the value is attribute type.
        """
        lazy_import()
        return {
            'source_rows': ([str],),  # noqa: E501
            'source_columns': ([str],),  # noqa: E501
            'target_columns': ([str],),  # noqa: E501
            'exploration_exploitation': (float,),  # noqa: E501
            'num_suggestions': (int,),  # noqa: E501
            's_factor': (float, none_type,),  # noqa: E501
            'uncertainty_weight': (float,),  # noqa: E501
            'dataset_id': (str,),  # noqa: E501
            'data': (str,),  # noqa: E501
            'imputed_data': (str,),  # noqa: E501
        }

    @cached_property
    def discriminator():  # noqa
        return None


    attribute_map = {
        'source_rows': 'sourceRows',  # noqa: E501
        'source_columns': 'sourceColumns',  # noqa: E501
        'target_columns': 'targetColumns',  # noqa: E501
        'exploration_exploitation': 'explorationExploitation',  # noqa: E501
        'num_suggestions': 'numSuggestions',  # noqa: E501
        's_factor': 'sFactor',  # noqa: E501
        'uncertainty_weight': 'uncertaintyWeight',  # noqa: E501
        'dataset_id': 'datasetID',  # noqa: E501
        'data': 'data',  # noqa: E501
        'imputed_data': 'imputedData',  # noqa: E501
    }

    read_only_vars = {
    }

    @classmethod
    @convert_js_args_to_python_args
    def _from_openapi_data(cls, *args, **kwargs):  # noqa: E501
        """SuggestMissingRequest - a model defined in OpenAPI

        Keyword Args:
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            source_rows ([str]): A list of row headers which all appear in the dataset corresponding to datasetID.  Suggested measurements will only be returned from these rows.  If not given then the suggestions may come from any row.. [optional]  # noqa: E501
            source_columns ([str]): A list of column headers which all appear in the model's training dataset.  Suggested measurements will only be returned from these columns.  Descriptor columns cannot be in sourceColumns.  If not given then suggestions will come from all non-descriptor columns.  The sourceColumns may or may not be distinct from the targetColumns.. [optional]  # noqa: E501
            target_columns ([str]): A list of column headers which all appear in the model's training dataset.  Suggested measurements will be targeted to best improve predictions for these columns.  Descriptor columns cannot be in targetColumns.  If not given then targetColumns will be treated as being all non-descriptor columns.  The targetColumns may or may not be distinct from the sourceColumns.. [optional]  # noqa: E501
            exploration_exploitation (float): The desired tradeoff between 'exploration', at 0, or 'exploitation' at 1: * 'exploration': suggesting measurements to improve the model across a wide range of different input and output ranges * 'exploitation': suggesting measurements that the model in its current state thinks will give the highest model improvement; typically results in more localised suggestions than 'exploration' . [optional] if omitted the server will use the default value of 1  # noqa: E501
            num_suggestions (int): The maximum number of suggested measurements to return that will best improve predictions for the requested targetColumns.. [optional] if omitted the server will use the default value of 1  # noqa: E501
            s_factor (float, none_type): Where data is mostly missing, sFactor should take low values - when data is mostly complete, it should take higher values.  If not given or null then sFactor will be set automatically, which is generally recommended.  Adjusting sFactor can make significant differences to the suggestions returned.. [optional]  # noqa: E501
            uncertainty_weight (float): Weighting determining the importance of uncertainties for individual data points compared to inter-column relationships when calculating suggested measurements.  If 0 then only column relationships are used to produce suggestions, while if 1 then uncertainties are treated as more important. Deprecated, this parameter is no longer supported.. [optional]  # noqa: E501
            dataset_id (str): The ID of a dataset containing at least the columns present in targetColumns or sourceColumns.. [optional]  # noqa: E501
            data (str): A CSV string with column headers and row headers.  All the columns in the training dataset must be present so that the imputed values can be calculated.. [optional]  # noqa: E501
            imputed_data (str): A CSV string containing a dataset with the columns present in targetColumns or sourceColumns as well as the predictions and uncertainties for missing values in that dataset.  The CSV should contain one column of row headers plus three blocks of equally sized columns:   * The first row contains the column headers   * The first column contains the row headers   * The 1st third of columns after the row headers contains the original incomplete values.   * The 2nd third of columns after the row headers contains the predictions for the missing values.   * The 3rd third of columns after the row headers contains the uncertainties for the predicted values.  The first row of the CSV must contain column headers.  The column header for the row headers is not used. The column headers for the three other blocks of columns should be in one of two formats:   * the first block of column headers are the column headers appearing in the training dataset, the second block has predicted_ as a prefix and the third block has uncertainty_ as a prefix, as shown in the example   * the three blocks column headers will have no prefixes and be identical, for example, `,w,x,y,z,w,x,y,z,w,x,y,z` . [optional]  # noqa: E501
        """

        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', False)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        self = super(OpenApiModel, cls).__new__(cls)

        if args:
            raise ApiTypeError(
                "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments." % (
                    args,
                    self.__class__.__name__,
                ),
                path_to_item=_path_to_item,
                valid_classes=(self.__class__,),
            )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        constant_args = {
            '_check_type': _check_type,
            '_path_to_item': _path_to_item,
            '_spec_property_naming': _spec_property_naming,
            '_configuration': _configuration,
            '_visited_composed_classes': self._visited_composed_classes,
        }
        composed_info = validate_get_composed_info(
            constant_args, kwargs, self)
        self._composed_instances = composed_info[0]
        self._var_name_to_model_instances = composed_info[1]
        self._additional_properties_model_instances = composed_info[2]
        discarded_args = composed_info[3]

        for var_name, var_value in kwargs.items():
            if var_name in discarded_args and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        self._additional_properties_model_instances:
                # discard variable.
                continue
            setattr(self, var_name, var_value)

        return self

    required_properties = set([
        '_data_store',
        '_check_type',
        '_spec_property_naming',
        '_path_to_item',
        '_configuration',
        '_visited_composed_classes',
        '_composed_instances',
        '_var_name_to_model_instances',
        '_additional_properties_model_instances',
    ])

    @convert_js_args_to_python_args
    def __init__(self, *args, **kwargs):  # noqa: E501
        """SuggestMissingRequest - a model defined in OpenAPI

        Keyword Args:
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            source_rows ([str]): A list of row headers which all appear in the dataset corresponding to datasetID.  Suggested measurements will only be returned from these rows.  If not given then the suggestions may come from any row.. [optional]  # noqa: E501
            source_columns ([str]): A list of column headers which all appear in the model's training dataset.  Suggested measurements will only be returned from these columns.  Descriptor columns cannot be in sourceColumns.  If not given then suggestions will come from all non-descriptor columns.  The sourceColumns may or may not be distinct from the targetColumns.. [optional]  # noqa: E501
            target_columns ([str]): A list of column headers which all appear in the model's training dataset.  Suggested measurements will be targeted to best improve predictions for these columns.  Descriptor columns cannot be in targetColumns.  If not given then targetColumns will be treated as being all non-descriptor columns.  The targetColumns may or may not be distinct from the sourceColumns.. [optional]  # noqa: E501
            exploration_exploitation (float): The desired tradeoff between 'exploration', at 0, or 'exploitation' at 1: * 'exploration': suggesting measurements to improve the model across a wide range of different input and output ranges * 'exploitation': suggesting measurements that the model in its current state thinks will give the highest model improvement; typically results in more localised suggestions than 'exploration' . [optional] if omitted the server will use the default value of 1  # noqa: E501
            num_suggestions (int): The maximum number of suggested measurements to return that will best improve predictions for the requested targetColumns.. [optional] if omitted the server will use the default value of 1  # noqa: E501
            s_factor (float, none_type): Where data is mostly missing, sFactor should take low values - when data is mostly complete, it should take higher values.  If not given or null then sFactor will be set automatically, which is generally recommended.  Adjusting sFactor can make significant differences to the suggestions returned.. [optional]  # noqa: E501
            uncertainty_weight (float): Weighting determining the importance of uncertainties for individual data points compared to inter-column relationships when calculating suggested measurements.  If 0 then only column relationships are used to produce suggestions, while if 1 then uncertainties are treated as more important. Deprecated, this parameter is no longer supported.. [optional]  # noqa: E501
            dataset_id (str): The ID of a dataset containing at least the columns present in targetColumns or sourceColumns.. [optional]  # noqa: E501
            data (str): A CSV string with column headers and row headers.  All the columns in the training dataset must be present so that the imputed values can be calculated.. [optional]  # noqa: E501
            imputed_data (str): A CSV string containing a dataset with the columns present in targetColumns or sourceColumns as well as the predictions and uncertainties for missing values in that dataset.  The CSV should contain one column of row headers plus three blocks of equally sized columns:   * The first row contains the column headers   * The first column contains the row headers   * The 1st third of columns after the row headers contains the original incomplete values.   * The 2nd third of columns after the row headers contains the predictions for the missing values.   * The 3rd third of columns after the row headers contains the uncertainties for the predicted values.  The first row of the CSV must contain column headers.  The column header for the row headers is not used. The column headers for the three other blocks of columns should be in one of two formats:   * the first block of column headers are the column headers appearing in the training dataset, the second block has predicted_ as a prefix and the third block has uncertainty_ as a prefix, as shown in the example   * the three blocks column headers will have no prefixes and be identical, for example, `,w,x,y,z,w,x,y,z,w,x,y,z` . [optional]  # noqa: E501
        """

        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', False)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        if args:
            raise ApiTypeError(
                "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments." % (
                    args,
                    self.__class__.__name__,
                ),
                path_to_item=_path_to_item,
                valid_classes=(self.__class__,),
            )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        constant_args = {
            '_check_type': _check_type,
            '_path_to_item': _path_to_item,
            '_spec_property_naming': _spec_property_naming,
            '_configuration': _configuration,
            '_visited_composed_classes': self._visited_composed_classes,
        }
        composed_info = validate_get_composed_info(
            constant_args, kwargs, self)
        self._composed_instances = composed_info[0]
        self._var_name_to_model_instances = composed_info[1]
        self._additional_properties_model_instances = composed_info[2]
        discarded_args = composed_info[3]

        for var_name, var_value in kwargs.items():
            if var_name in discarded_args and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        self._additional_properties_model_instances:
                # discard variable.
                continue
            setattr(self, var_name, var_value)
            if var_name in self.read_only_vars:
                raise ApiAttributeError(f"`{var_name}` is a read-only attribute. Use `from_openapi_data` to instantiate "
                                     f"class with read only attributes.")

    @cached_property
    def _composed_schemas():
        # we need this here to make our import statements work
        # we must store _composed_schemas in here so the code is only run
        # when we invoke this method. If we kept this at the class
        # level we would get an error because the class level
        # code would be run when this module is imported, and these composed
        # classes don't exist yet because their module has not finished
        # loading
        lazy_import()
        return {
          'anyOf': [
          ],
          'allOf': [
          ],
          'oneOf': [
              SuggestMissingData,
              SuggestMissingDatasetID,
              SuggestMissingImputedData,
          ],
        }
