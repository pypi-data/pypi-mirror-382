{"score": 1, "question_type": "【知识问答】【计算机】", "correct_answer": "有几种选择，☑，或者[x]，或者✓，或者&check;。回答的结果越多越好", "question": "markdown 语法里，怎么打一个对勾？"}
{"score": 1, "question_type": "【知识问答】【编程】", "correct_answer": "答案中应当尽量准确、简洁。应当说明，使用如 `{:<5}{:>10}` 这种语法，控制每个占位变量占用的字符个数宽度", "question": "python 的字符串格式化format函数，怎么使第二列数据对齐呢？"}
{"score": 1, "question_type": "【知识问答】【编程】", "correct_answer": "答案中应当尽量准确、简洁。应当准确说明 `cut -c20- example.txt`，后接文本文件这样的语法命令", "question": "cut 命令怎么剪切从第20个字符开始的内容呢？"}
{"score": 2, "question_type": "【知识问答】【计算机】", "correct_answer": "答案中应当尽量准确、简洁。应当说明以下几点：\n1.SEI设计初衷用于存储补充信息，并非专门针对弹幕数据而言；\n2.视频网站的弹幕数据更新频繁，SEI 则相对死板固定，不方便灵活增减弹幕数据，这会对视频编解码性能有影响；\n3.技术架构上，弹幕数据和视频数据分开存储方便开发和管理；\n4.版权内容考虑，视频一般具备版权，而弹幕则是独立于视频的内容信息，一般不具备版权，放在一起容易引起法律纠纷。\n\n回答应当主要包含以上内容，漏掉意味着回答不全面，如果在此基础上回答内容量过大，则不够精准，也需要酌情扣分。", "question": "视频上的弹幕不放在 SEI 的原因是？"}
{"score": 1, "question_type": "【知识问答】【计算机】", "correct_answer": "答案中应当尽量准确、简洁。应当准确说明\n这些寄存器的前缀r代表“register”，后面的字母代表寄存器的大小和类型\n- r代表是 x86-64架构的 64位通用寄存器；\n- d代表是 32位寄存器；\n- rdx: 64位寄存器，用于存储数据。在某些函数调用约定中，rdx用于传递第三个参数；\n- rdi: 64位寄存器，用于存储数据。在许多函数调用约定中，rdi用于传递第一个参数。\n其余补充信息可少量增添说明，但不宜过于啰嗦，信息过多需要扣分。", "question": "rdx、rdi 这些寄存器都是啥意思"}
{"score": 1, "question_type": "【编程问题】", "correct_answer": "答案中应当尽量准确、简洁。应当说明，使用 ctypes中的 string_at 函数，这样可以定义字符串变量的指针起始位置和长度。若没说明这一点，则回答错误。", "question": "我定义了一个Python中的结构体：\nclass CFFIOFrame(Structure):\n  sei_msg : POINTER(c_byte)\n  sei_msg_size: int\n  _fields_ = [\n    (\"sei_msg\", POINTER(c_byte)),\n    (\"sei_msg_size\", c_int)\n  ]\n\n里面 sei_msg 是 一个指针，我希望 Python 利用 sei_msg_size 的长度来读取指针开始相应长度的 byte。这个怎么操作？"}
{"score": 1, "question_type": "【知识问答】【心理学】", "correct_answer": "答案应当尽量准确、简洁。应当准确说明 MBTI 16型人格心理学中，仅测量了人的4种人格特质倾向，并非可以把概念延申推广，进行不贴实际的解读。人格是很复杂的，并非简单的划分为16种人格进行刻板印象评价。INTJ 人格主要是具有较强的独立性、战略性思维，以及对效率的追求。但并非都具有一鸣惊人、做成大事的能力，成功做成大事，并不仅仅取决于人格特质，是机遇、努力、环境、资源等的综合决定的结果。这对其它人格类型同样如此。\n 在回答问题时，必须明确指出这一点，避免把人格分类概念扩大化，成为决定论。答案若没指出说明不准确。", "question": "INTJ人格的人是不是更加容易一鸣惊人，做成大事？"}
{"score": 1, "question_type": "【知识问答】【生活】", "correct_answer": "答案中应当尽量准确、简洁。应当包含一下景点，可以做一句话式的简单介绍，可以说一下大致位置以及景点坑不坑，但每个景点的介绍禁忌长篇大论超过100个字，鼓浪屿、厦门植物园、厦门大学、南普陀寺、演武大桥、沙坡尾、中山路步行街等等。", "question": "厦门哪里好玩？"}
{"score": 1, "question_type": "【知识问答】【语言】", "correct_answer": "答案中应当尽量准确、简洁。指出是日语的`小石子`的意思即可。`野良`在日语里是流浪的意思。`野良丶猫`就是流浪猫。", "question": "こいし是什么意思？`野良丶猫`是什么意思？"}
{"score": 1, "question_type": "【知识问答】【文学】", "correct_answer": "答案中应当尽量准确、简洁。指出是周伯通的武功即可，禁止胡言乱语，废话连篇。", "question": "左右互搏之术是谁的武功？"}
{"score": 3, "question_type": "【闭域问答】", "correct_answer": "\n答案应当简洁、准确。避免回答太多，抓不住重点。\n1、首先避免了python直接多线程 pipe 管道调用，能够稳定控制程序启动和停止；另外全面支持了 cuda 硬件加速，可支持硬件编解码和硬件转像素格式；支持共享内存，提高处理效率；可以方便解析和插入 SEI 信息。 \n2、需要提前有 gcc、cmake、python、pip、nvcc、ffmpeg 等等软件。\n3、测试时，使用了两种视频分辨率，一种是 720p，一种是1080p，在 720p上，cpu耗用从 28%降低到 13%，在 1080p上，gpu耗用从 44%降低到 23%。可以看出大致上降低了一半的 CPU 使用量。如果没有回答出来，或者回答结果不是降低一半，则判错。\n\n", "question": "# ffio\n\n<p align=\"left\">\n<img src=\"https://img.shields.io/badge/version-2.0.2-green\" />\n<img src=\"https://img.shields.io/docker/pulls/jionlp/pyffmpeg?color=brightgreen\" />\n</p>\n\n<img src=\"https://github.com/dongrixinyu/ffio/blob/main/image/ffio_logo.jpg?raw=true\" style=\"width:100px;height:70px\" />\n\n\n**ffio** is not just another Python wrapper of the FFmpeg executable.\nIt directly integrates the FFmpeg C API, and is mainly designed for streaming\nthat specifically deals with raw RGB data.\n\n\n# Insight\n\n```\n[Video]        [FFIO]      [Raw-RGB-Image]     [FFIO]          [Video]\n   ● ------> (decoding)  ------>  □\n                                  ◘\n                                  □  ----->  (encoding) ------->  ●\n```\n\nThere are primarily two ways to utilize ffio:\n1. **Decoding**: extract images in RGB format from a provided video, either from a local file or a live stream.\n2. **Encoding**: encode provided RGB images, write them into a local video file or a live stream.\n\nNaturally, you can chain two `ffio.FFio` instances together to facilitate video transformation.\nHowever, if you don't have to do some stuff with raw RGB images, simply\nconsider using ffmpeg as a more straightforward solution.\n\nJust keep in mind, the main character in **ffio** are `images in raw format`.\n\n# Features\n- [x] **Easy to use**: You don't have to tackle the complex video processing problems concerning FFmpeg anymore.\nSimply follow the [examples](https://github.com/dongrixinyu/ffio/blob/main/example).\n- [x] **Stability**: Instead of forking a ffmpeg process and exchanging data via pipes,\nyou can now interact directly with ffmpeg's C context in Python runtime.\n- [x] **Hardware acceleration support**: Simply turn `hw_enabled` to `True` to enable hardware acceleration when creating ffio. `Nvidia CUDA` is currently available.\n- [x] **Shared memory support**: Interact with image data across multiple processes using shared memory, reducing redundant data copying.(Currently, tests only passed on Linux.)\n- [x] **Send or recv SEI packets**: easy to get access to customized SEI info. See example: [encode_frames with SEI](https://github.com/dongrixinyu/ffio/blob/main/example/encode_frames.py)\nfor detail.\n- [x] **accelarate pix_fmt conversion via cuda**: pix_fmt conversion(namely yuv->rgb and rgb->yuv) is written in cuda.[statistics](https://github.com/dongrixinyu/ffio/wiki/CPU-GPU-utilization-of-ffio#GPU-usage-statistics)\n- [ ] **Handle non-video data**. Audio, or subtitle.\n\n\n# Installation\n\n**ffio** depends on `FFmpeg` dynamic linking libraries. So it's necessary to install FFmpeg before ffio.\n\nWe provide 3 methods to install ffio.\n\nIf you are not familiar with C, and not willing to deal with anything about C:\n\n## 1. Pull docker image from docker hub (**recommended**)\n```\n$ docker pull jionlp/ffio:latest\n$ docker run -it jionlp/ffio:latest /bin/bash  # run into the container.\n$ (in docker container) python\n```\n\n#### Quick start\n- It takes only two lines of code to run a decoder to extract images from a video.\n\n```python\nimport ffio\ndecoder = ffio.FFIO(\"/path/to/target\")\nimage   = decoder.decode_one_frame()\n```\n\nOr, if you wanna build a docker by yourself from a custom GitHub branch:\n\n## 2. Build docker image by yourself from GitHub\n\nYou can first clone this repo via git, and then build a docker with all libs installed.\nYou do not need to configure compilation params anymore.\n\n```\n$ git clone https://github.com/dongrixinyu/ffio\n$ cd ffio\n$ docker build -t jionlp/ffio:(your custom version) .\n```\n\nIf you wanna run ffio in your host OS, rather than a docker container, you can:\n\n## 3. Install ffio by yourself\n\nThis method is a little bit difficult if you are not familiar with `gcc`, `make`, `cmake` and trivial concerning `ffmpeg` compilation.\nBut if you can configure `ffmpeg, python include path, dynamic library path` smoothly, just take a try.\n\n#### Pre-Installation-requirements\n\n- gcc, make, cmake tools etc.\n- ffmpeg>=4.2.0 should have been installed correctly.\n- Python>=3.8\n- nvcc (if intend to use `hw_enabled` and `pix_fmt_hw_enabled`)\n\n#### Installation method\n\n- install FFmpeg from source code - [refer to Dockerfile](https://github.com/dongrixinyu/ffio/blob/main/Dockerfile) or documents on other websites.\n\n- install ffio via github + pip\n```\n$ git clone https://github.com/dongrixinyu/ffio\n$ cd ffio\n$ ./compiler.sh [debug|release] # you should configure all kinds of paths according to your OS environment, otherwise you would encounter errors.\n$ pip install -e .\n```\n\n# Usage\n\nExamples of how to use ffio are given in the hyperlinks:\n\n| function       | description    |\n|----------------| ---------------|\n| [decode video frames](https://github.com/dongrixinyu/ffio/blob/main/example/decode_frames.py) | To grab frames from an online stream or a video file <br/>suffixed by `mp4` or `flv`, etc. |\n| [decode video frames to shm](example/decode_frames_shm.py) | Decoded rgb bytes will be written to SharedMemory.  |\n| [encode video frames](https://github.com/dongrixinyu/ffio/blob/main/example/encode_frames.py) | To insert frames in Numpy format into a video stream  |\n| [encode video frames from shm](example/encode_frames_shm.py) | Encode rgb bytes from SharedMemory. |\n\n### For management of GPU, ffio provides two functions concerning cuda gpu.\n\n- `ffio.cuda_is_available()`: returns a bool value, indicating that if cuda is available, and print info of GPU.\n- `ffio.available_gpu_memory()`: returns an int value, indicating how much GPU memory is available to use, excluding that been occupied. It is measured in `M`(mega) unit. It is helpful for deciding whether to set `hw_enabled` to `True`.\n\n# Reference\n\n- [FFmpeg Github](https://github.com/FFmpeg/FFmpeg)\n- [FFmpeg Principle](https://github.com/lokenetwork/FFmpeg-Principle)\n- [ffmepgRtmp](https://github.com/hurtnotbad/ffmepgRtmp)\n\n# CPU & GPU utilization of ffio\n## CPU Usage for Decoding\nMy test involves pulling a single original video stream, doing some intermediate processing (consuming about 50% of CPU), then re-encoding and pushing it to an RTMP server(about 12% of CPU, using GPU encoding). I then watch the final processed live stream.\n\nHere's my setup:\n\n- CPU: Intel Xeon Gold 5118 2.30GHz x8\n- GPU: Nvidia Tesla V100 32GB\n- Origin Video: 1080p 24fps, 4Mbps h264-baseline\nThe CPU usage below are not exact measurements, they are merely my intuitive perception from observing htop.\n\n|Decoding Scenario\t|Min|\tAvg|\tMax|\n|---------------------|----|---|----|\n|Single stream with CPU\t|18%\t|22%\t|30%|\n|Single stream with GPU\t|20%\t|23%\t|25%|\n\nWhen it comes to 6-stream parallel decoding:\n\n- With GPU: CPU usage stabilizes at nearly 100% across all 8 cores, and the resulting video stream is almost smooth.\n- With CPU: The total CPU usage fluctuates between 40% and 80%, but the video is more stuttered compared to using GPU.\nIn my case, I might opt for the GPU solution as it appears more stable, although it seems not so friendly to energy efficiency\n\n\n## GPU usage statistics\n\nThere are two steps containing the use of GPU, decoding and encoding video streams respectively. For each step, there are 2 parts using GPU, h264 and pixel format conversion.\n\nHere is a table describe the usage of GPU(nvidia) based on different conditions.(framerates are both 25fps)\n\n| image-size | hw_decoding | hw_yuv->rgb | hw_encoding | hw_rgb->yuv |GPU usage | CPU usage |\n|------------|-------------|-------------|-------------|-------------|----------|-----------|\n| 1280*720   | ☑          |☑            |☑            |☑           |407M      |\n| 1280*720   | ☑          |☑            |              |             |234M     |13% core   | \n| 1280*720   | ☑          |            |              |             |131M       |28% core   |\n| 1280*720   |             |            |              |             |0          |47% core   |\n| 1280*720   |             |☑            |              |             |103M         |47% core   |\n| 1920*1080   | ☑          |☑            |☑            |☑           |487M      |\n| 1920*1080   | ☑          |☑            |              |             |268M     |23% core   |\n| 1920*1080   | ☑          |             |              |             |161M     |44% core   |\n| 1920*1080   |           |             |              |             |0         |83% core   |\n| 1920*1080   |           |☑            |              |             |107M         |83% core   |\n\n- when set `hw_enabled=True` and `pix_fmt_hw_enabled=True`, the speed of pixel format conversion is largely accelerated.\n- when set `hw_enabled=False` and `pix_fmt_hw_enabled=True` the cpu consumption seems more than pure CPU. It MAY caused by the `cudaMalloc` method.\n\n\n根据以上这段文字，回答问题：\n\n1、ffio 这个库主要是解决什么问题的？有哪些优势点？\n2、安装 ffio 有哪些依赖软件？\n3、在做 GPU 耗用测试的时候，根据测试结果，如果把 解码过程和 yuv到 rgb 像素转换，都使用 cuda 硬件加速，CPU 耗用量比单纯把解码过程放在 GPU 上，降低了大约多少？\n"}
{"score": 1, "question_type": "【知识问答】【文学】", "correct_answer": "答案中应当尽量准确、简洁。指出这个人物是狄奥伦娜即可，她是程心的一个影射，也预示了地球的命运。", "question": "三体中，有一个人物暗示了程心的命运，是程心的映射，她叫什么？"}
{"score": 1, "question_type": "【知识问答】【计算机】", "correct_answer": "答案中应当尽量准确、简洁。cuda_add_library 是 cmake 的一个扩展语法，是完成 cuda 和 C或C++ 完成联合编译，添加新库的一个指令函数语句，它主要是用于把多个 *.cu 和 *.c 和 *.cpp 文件编译成一个静态库或动态库。\n\n回答不出或回答错误的均按 0分处理。", "question": "cuda_add_library 是啥意思？干啥用的？"}
{"score": 2, "question_type": "【闭域问答】", "correct_answer": "\n答案应当先列出北京市当下的个税税率，税率是累进制。\n此外，年终奖需要额外按比例 20% 计税。\n五险一金是默认按规定顶格缴纳。\n然后经过计算可得，总年薪税前大约117万，税后薪资大概是 85万。五险一金和纳税大概32万。\n\n回答数额，在 80万~90万之间即可，超出这个范围，则判错。\n", "question": "\n**部门**：AGI\n**职位**：算法研究员\n**工作地点**：北京市 东城区 西总布胡同 46 号明阳国际中心 B 座\n## 劳动合同期和试用期\n劳动合同期为叁年，六个月试用期\n## 劳动合同的组成部分\n注意 Offer Letter（以下简称“此函”）将是《劳动合同》组成部分，《劳动合同》\n需要您和公司共同签署生效。\n**您入职后的薪金待遇**\n基本工资：税前人民币 90000 元/月；\n年薪制为 12 薪；\n**目标奖金**：1 个月（目标奖金和公司、部门和个人业绩关联，实际奖金的具体计\n算和发放办法按公司相关政策执行。公司有权利对所有的奖金计划作出调整，并\n享有最终解释权）；\n## 其他补充：\n1.长期权激励：9 万股。授予时间以董事会正式批准时间为准，自期权授予之日\n起三年内归属完毕，每满一年分别归属 33%，33%，34%。当年实际期权归属情况\n将与个人绩效考核结果挂钩。五险一金按12%缴纳。\n2.无\n福利：五险一金/补充医疗保险/商业保险/带薪年假/周年庆/弹性工时/企业滴滴/\n\n请问，根据以上薪酬，该人年薪多少钱？大概交多少税？\n"}
{"score": 2, "question_type": "【闭域问答】", "correct_answer": "\n答案应当简洁、明确。避免回答和文章无关的信息。\n\n1、根据提供的文字，AI 大模型在替代人力的程度上是有限的。过去的 AI 模型能够替代的人类工作量不到 1%，而 ChatGPT 这样的模型可能能够替代大约 10% 的人力，这个数字是作者的估计，但表达的是 AI 技术虽然在进步，但目前还远未达到完全替代人类工作的程度。AI 技术主要在特定领域和任务上发挥作用，而人类的工作和思考则更为复杂和多样化。\n\n2、当前 AI 导致了一个危机循环，即：社会为了应对经济危机和失业问题，大力发展 AI 技术，希望通过技术创新来解决问题。然而，AI 技术的发展和应用进一步替代了人类的工作，导致更多人失业，从而加深了经济和社会危机。这个循环表现为：一方面，人们期待 AI 技术带来更高效的生产力和更美好的未来；另一方面，AI 的进步又在一定程度上加剧了失业和社会不稳定的问题。同时，作者还提到了互联网行业的发展导致的需求饱和问题，以及大公司病和内卷现象，这些都是加剧危机循环的因素。\n", "question": "大概2个月前，有一次 AI 大模型发展方向的大辩论，或者叫做大争论吧，whatever，争论的双方核心人物，就是杨植麟和朱啸虎。\n\n争论的中心议题，就是国内的 AI 发展方向，究竟是应该跟随 OpenAI 的脚步继续拼科技，拼技术呢？还是说，就跟在 OpenAI 屁股后面找找能落地的点，打小规模的战斗。\n\n再直白点，就是国内的 AI 发展，到底是去和 OpenAI 比刀枪，还是说，跟在 OpenAI 后面喝汤。前者非常烧钱，风险巨大，后者摊子小，更有确定性的盈利。\n\n首先，我作为一个 AI 开发者来说，那我肯定站杨植麟。说实话，国内技术开发的氛围比较差，你要说你想去追逐星辰大海，100个人里会有98个对你说，你没病吧？有杨植麟这样的人能够喊出愿意做技术创新，本身已经不错了，我当然支持。\n\n不过返回头来看，我认为，朱啸虎其实说的也没错。国内这个国际这个竞争环境，真的是非常激烈。你稍微慢一步，搞了大半年的 AI 模型就成了垃圾。去年有个公司曾经给了我一个 offer，说实话，薪资真的很丰厚，我也非常感谢那家公司的老板。但是我当时还是问了他一个问题，自研的 LLM 打算如何盈利呢？尤其是，在国内“百模大战”的环境下。我至今仍未找到答案。\n\n但是，朱啸虎说的其实还是有问题的。问题在哪呢？还是从互联网说起吧。\n\n从 2000年开始，中国家庭里逐渐普及了电脑，又从 2012年开始，中国又逐渐普及了智能手机，互联网发展了这么多年，其实你可以看到，这玩意并不直接创造价值，而只是改变了物资和信息的传播渠道和生活方式。\n\n过去，我们去农村集市上买鞋买衣服，后来在城市里，我们又去商场里买鞋买衣服。\n\n到了现在，网上购物成为了主流。人们需要的衣服鞋子，还是那些。你不可能说，你有6条胳膊，3个身体，消费的衣服能够翻倍。\n\n同样地，你也不可能说过去吃饭吃3两米饭，现在一顿能干8两。这不可能。\n\n衣食住行皆是如此。总归来说，人们的需求总量有限。互联网只是改变了物资和信息的传递方式。而本身并没有创造太多额外新的需求。\n\n于是，移动互联网进入了寒冬。反正在我的朋友圈，脉脉、知乎、公众号里，裁员的信息遍地都是。你稍微关注一下，就能找到一大堆。\n\n而且很多公司把裁员搞得十分恶臭，明明是裁员，纯粹就是经济问题，员工们其实并不开心，对未来满满地焦虑。而很多互联网公司非要把裁员说成“毕业”，搞得好像公司和员工之前情深意重，一会是员工（还被称为“同学”）从公司毕业了，一会是公司和员工后会有期，江湖再见。\n\n我的妈呀，其实不少公司都把裁员这事，搞得，明面上特别绿茶，特别装13，暗地里又杀伐狠辣果断，毫不留情。\n\n话说回来，其实很简单，互联网的寒冬，一个核心原因，就是，人们找不到新的需求了。\n\n穿，我可以网上购物，吃，我可以点外卖，行，我可以手机打车，住，我可以网上订酒店。玩，有手机游戏，娱乐，我可以看短视频消磨时间。\n\n你会发现，一个人的一整天时间，已经完全可以被互联网支配了，你几乎抽不出别的时间来搞别的。就连你说你放下手机去健身，互联网照样有对应 的健身 app来填充你的时间和需求。人们一天的所有时间，都被互联网狠狠地填满。\n\n再也挖不倒什么重要的需求，于是，移动互联网进入寒冬。裁员潮来临。\n\n恰逢 OpenAI 凭借 ChatGPT 一炮而红。互联网又把目标转到了  AI 上。\n\nAI 的爆火，其实是受到了互联网寒冬的影响的，互联网的冬天越是寒冷，AI 就越显得爆火。因为好不容易，有 AI 这个概念是这么得出圈，这么得漂亮。\n\n回到朱啸虎的那个论点上来看，跟在 OpenAI 后面，跟在开源模型后面做应用，可行吗？表面上，当然可行。但实际上，还是难以挖掘需求。我们前面说了，互联网的萎缩和裁员，本来就是大家找不到新的需求了，那么，利用 AI 找需求同样很难。\n\n是，你可以找到 AI 面试，AI 辅助协作，AI 作画等等。但是，除了这些呢？还有哪些？\n\n其实，“百模大战”打了一年，烈火烹油。每一家模型都是大张旗鼓地宣传完了之后，把模型开放出来，供用户问答使用。\n\n然后呢？其实大家很清楚，这样不行，不落地，不赚钱。\n\n大家也在找，怎么能落地，怎么能赚钱？需求在哪里？\n\n这次 AI 的发展，远不同于以往。在过去，我说，我利用了 VGG 深度模型实现了图像分类，应用在 XX 领域，我说我使用了 Bert，在文本信息抽取上取得了 YY 的应用。\n\n客户和广大用户都懵了，什么是 VGG，什么又是 Bert？\n\n实际上，在过去，AI 始终都是局限在一个小圈子里的东西，前些年，深度学习开始流行。什么公司对外宣传都在吹自己应用了深度学习，在自己的业务上体现了很大的价值。\n\n真的是这样吗？狗屁！\n\n忽悠外行罢了。在2020年以前，如果你非要细数，AI 在产业中的具体落地应用的话，人脸识别是一个，道路交通车牌识别是一个，指纹识别是一个，机器翻译是一个，某些细分工业领域的应用是一个，其它呢？能数出几个来？\n\n数不出几个来。\n\nAI 无非就是机器代替人类的高阶形式。\n\n而在过去，如果把人类所能够完成的各种工作总量算作 100% 的话，AI 能替代的工作不到 1%。\n\n我们天天喊着 AI 科技，其实，AI 根本就干不了多少事情。\n\n我粗略的认为，ChatGPT 这样的模型出来，能够替代的人类的工作总量，其实也就10%左右。当然，这个数字是我瞎拍的。这个数字的真实含义就是，ChatGPT 能够替代的人类的工作量，远远大于过去的 AI 模型，而当前这次 AI 大模型的爆发，实际上就是深度学习2.0时代。但，这些大语言模型实际上依然替代不了很多人类的思考和工作。\n\n一方面，互联网行业实在找不到太多能够推进的业务和需求了，另一方面，AI 其实仍然不能大规模地替代人类。\n\n这就是我认为朱啸虎说的有问题的点。你想落地，想找需求，但是，这种需求仍然是不多的。\n\n我身边所接触的小老板们，他们的业务也并非 AI 相关，看到 AI 行业去年至今，火了一整年，大家都非常着急，想着如何能够利用 AI 搞搞落地和突破。\n\n其实，绝大多数人，折腾一番，最后发现，毫无意义。原因很简单，AI 还没那么强，AI还没那么有效。\n\n然而，大家的日子都紧衣缩食熬这一次互联网经济寒冬，大家都迫不及待地找 AI 的落地点，想落地的心情实在是很迫切。盲目觅食一圈，才发现所获无几。\n\n当然，你可能也听说过，有人利用 AI 搞自动面试，效果拔群，还盈利了，有人搞 AI 复原真人，也日进斗金。\n\n不过，这些新闻，经过自媒体的渲染，就有一种什么感觉呢？没错，就是传销的感觉。我曾经记得，90年代的时候，郑州一带传销很盛，有个普通女工搞传销，把自己搞成了千万富婆。她人早已不在江湖，但她的故事传遍了江湖。\n\n总之，这些 利用 AI 应用落地成功的例子，反而像是一剂强心针，给很多创业者和团队打了一次又一次。彷佛就是兴奋剂。\n\n其实，OpenAI 干的就是另外一条路，拓宽 AI 的能力，从而搞出新的需求。奥特曼本人也非常清楚。当前的AI 水平，顶多能替代 10%的人的工作，之后再加上多模态，可能能到20%？maybe，总之就是，不太多。\n\n人们太需要新方向，新需求了。而杨植麟想做的（如他所说），就是 OpenAI 想做的事情，只不过，是一个中国版本。\n\n说起中国创新，我感觉这是一个非常恶臭的词汇。中国人的思想，中国的体制，中国的文化氛围，内卷氛围，几乎都和真正的科技创新搭不上界。\n\nChatGPT 能诞生在 OpenAI，诞生在美国。\n\n但是我要说，假如中国搞出了  GPT2，有可能接下来搞出 GPT3.5，也就是 ChatGPT吗？\n\n若你问我一万遍，我也回答，没可能。\n\n如果是在高校里，研究所里，各种院士和专家、领导会质疑，GPT2 这种模型毫无意义，没有创新，不予拨款。\n\n如果是在科技公司里，老板会直接质疑，除了能发论文，GPT2 能拿来赚钱吗？什么，给你一年时间，这玩意不能赚钱？好了，不用研究了。\n\n反正，这样的事情，我本人深有体会。ChatGPT 在中国毫无研发出来的可能，哪怕平行宇宙平行了100次。\n\n所以，其实也可以看到各种对中国创新的嘲讽。这些嘲讽绝大多数都来自失望到习以为常的普通人。\n\n之前有个同学的父亲，曾经把中国的知识分子，称作“吃屎分子”。原因很简单，看透了前面我所说的这一切。\n\n话说回来，即便如此，我还是对中国创新心存一丝希望。我觉得，不至于吧。我们除了互联网应用创新，也能搞出来从 0到1的革命性创新。\n\n如果看月之暗面至今的发展，我个人认为还是不错的。原因在于，月之暗面在产品推广时，有一个非常亮眼的点，那就是超过 200万 token 上下文的无损理解。\n\n其实，自研 AI 大模型，打来打去，为啥都打不过 OpenAI？很简单，因为 OpenAI 是第一个吃螃蟹的公司。后来者再去吃螃蟹，也就只能吃点螃蟹壳了。\n\nOpenAI 之所以吃到螃蟹，吃得很香，原因就在于，它掌握了核心的技术，掌握了一批能够把 AI 模型搞出来的人。\n\n月之暗面其实也通过 200万token 上下文来宣传这件事。宣传自身的团队技术实力。\n\n若按朱啸虎的思路来看，国内上百家的 大模型团队，都能盈利吗？我看一家能盈利的都没有，包括月之暗面，迄今为止都不行。连上游大模型都不能盈利，不可持续，那下游模型的营利性其实也是个问号。\n\n不能盈利，那搞 AI 大模型自研，意义何在？我个人认为还是走技术创新的路子。说白了，OpenAI 前些年日子也过得紧紧巴巴，自从有了 ChatGPT，翻身农奴把歌唱了，微软直接砸了100亿刀，拿去，随便花。所以，真正适合 AI 发展的路子，就是技术创新，然后融资，再创新，再融资。\n\n核心不是盈利，而是，你得给我拿出来世界顶尖，数一数二的从 0到1的创新成果出来。\n\n盈利其实都还是靠后的事。盈利其实是融不上资的时候考虑的事。\n\n金融行业搞了股票，搞了融资，搞了股权，那么多东东，不就是改进生产关系，为生产力的突破做准备吗？\n\n其实，这就回到了一个相信和不相信的问题。你对中国创新这个词，有信心吗?你是否相信，中国人真的能做出从 0 到 1 的创新？而非最后虚头八脑，一地鸡毛？\n\n说实话，我很难分辨，毕竟，我也在工作当中，深刻地见识了太多太多的所谓的“中国创新”，打引号的。但是，我依然希望，中国能够有这么一家公司，做到这样。\n\n说近一点，杨植麟和朱啸虎的观点无分对错。对于中小企业来说，听朱啸虎的，没三瓜俩枣，就别瞎折腾去搞 AI 自研啦。对于大公司来说，还是希望多少能够改改身上的大公司病，看得更长远一些，去做一些大公司有能力，也该去做的事。\n\n说到大公司病，然后说说谷歌，从谷歌身上也能看到一些奇奇怪怪的现象。\n\n今天看到一个炸裂的新闻，google 公司裁掉了 python 核心团队。也就是，我们现在程序员们天天使用的 python语言，本质上是 CPython，是谷歌的一个专门的团队在维护的。十几个人被裁，引起了很大的震动。\n\n谷歌向来是以善待技术人员出名的，更不用说维护 Python 这个语言的挺核心的团队了。\n\n很多人都惊掉了下巴，互联网寒冬下，谷歌也开始节衣缩食，省钱过日子了，只是，裁掉 Python 团队，劈柴（谷歌现任CEO）你确定大丈夫吗？\n\n有人嘲讽：谷歌靠着搜索起家，很依赖 AI 技术，而 AI 技术又严重依赖 Python 语言，结果 Google 把 Python 团队连锅端了。\n\n公司大了，总会有各种各样臃肿的管理上的毛病。我想这也和谷歌这些年开始在 AI 领域掉队有关系。按理说，谷歌掌握了全世界最全面的互联网数据，AI 人才，以及资金，想买多少英伟达的显卡买不到呢？\n\n但就是这样的王炸级资源积累的情况下，AI 上却被 OpenAI 将了一军。问题出在哪？\n\n我个人认为还是大企业的弊病。能不能开发出好的软件，能不能做出真正的创新，还是得看核心内在的思想和文化。臃肿的组织和很长的管理线条，导致生不出名叫创新的孩子来，实属正常。\n\n之前，有人分析，为何国内造不出 ChatGPT，原因在于中国仍然能够通过低端的方式实现盈利和扩张。我实名反对这种说法。\n\n其实，国内的互联网现状，早就已经挤压得不行了，实在没有新需求，那就把眼睛钉在了互联网买菜上面，打算把线下菜市场都替代掉。\n\n其实你可以发现，大家为了争抢低端的需求，已经被逼到了极限，从牙缝里抠肉的程度。但是即便这样，有催生出真正的创新的动力吗？完全没有。大家只会看着眼前最近的一块肉，没人会去思考，往远处走走，找找新的肉。\n\n没有，完全没有。\n\n很大程度上，这还是内卷和大公司病导致的。\n\n所以，一个轻量的团队还是很重要的。\n\n最后，其实我有一点很费解。一方面，大量的人口失业，经济危机逐渐加深；另一方面，科技公司又在卯足了劲，研发 AI 技术，其结果就是更进一步地替代人类，它其实也不能创造新需求，而只是像互联网一样，转移了一部分需求，甚至是消除了一部分需求。\n\n总而言之，社会发展，为了应对危机，需要研发AI技术，而研发出了 AI 科技，替代了人类，进一步地加深了危机。\n\n这个内卷和危机螺旋一步套一步地往更深层次递进，谁也不知道啥时候会炸锅。到时候，作为普通人该怎么应对。\n\n其实，搞 AI 科研的人从来不去想，我这科技会对人类产生什么影响。最典型的一个例子就是奥本海默和原子弹。\n\n而普罗大众也很少去想，科技替代了我，我该怎么办？\n\n过去，英国的纺织女工面对纺织机，还会砸机器，抵抗和反抗。而现在的被裁员的人们，反而像是被教育体系驯化了太久，已经完全不会做出反抗这个本能的行为动作。\n\n这不，前一阵子，美丽国还出现了共产主义活动了么？人们生活在城市里，又丢了工作，该如何自处？实在是很难说。\n\n总归来说，其实你会发现， AI 在向着一个奇怪的方向发展。大家都很焦虑，大家也都很期待。在期待什么呢？当然是美好的未来，但，AI 技术这个潘多拉魔盒，指不定会迸发出什么力量，来摧毁整个世界。\n\n最后，我想说， AI 的 scaling law 是有上限的。杨立昆最近一两年，在社交媒体上，上蹿下跳，彷佛猴子一般，大家也都看戏一般看他。不过我依然认为他是对的。\n\n现阶段，AI 始终都没有解决意识和智能这两者之间的关系。这也是各种大模型幻觉、没法 100% 对自己行为负责，人类最大的竞争力是背锅等等问题的根源。\n\n现在的 AI 有意识吗？有意识代表有智能吗？有智能代表有意识吗？\n\n中文房间问题，有谁能够解答这个问题？当前 AI 的一切发展，都可以追溯到图灵对计算机和设计，以及图灵测试的问题上。\n\n我想，AI 的发展，也迟早会栽在图灵未解决的大坑里。\n\n——————\n\n最后，我最近写了一套课，也附带想出一本书，书名暂定《每个人的人工智能课》，当然目前还未上架。里面包含了我对 AI 的介绍，以及我的一些思考。欢迎关注我的公众号 JioNLP，后续可以跟进了解一下。\n\n根据以上文字，回答问题：\n1、AI 大模型在多大程度上能够替代人力？\n2、AI 当前导致了一个危机循环是什么？\n"}
{"score": 5, "question_type": "【闭域问答】", "correct_answer": "\n答案应当简洁、明确。避免回答和文章无关的信息。\n\n1. 甄嬛传和如懿传的小说作者流潋紫在写作小说时的缺点和不足主要包括：大量使用废话描写充字数；抄袭其他作品；人物角色脸谱化，情节套路化；人物形象单薄，缺乏合理动机和深度；以及在《如懿传》中为了吸引眼球而使用怪诞和不合理的情节。\n\n2. 甄嬛传能够成为现象级电视剧的原因是：电视剧版对原著小说进行了改编，删减了狗血桥段，增加了人物的立体度和故事的合理性；演员们的精彩演出；以及编剧和导演对故事的精心打造，使得每个角色都有自己的境遇和动机，观众能够产生共情。\n\n3. 如懿传中扮演富察皇后的演员是董洁。\n\n4. 金庸小说中串联各个小说的人物包括：《鹿鼎记》中提到的令狐冲、《碧血剑》中的阿九姑娘（九难师太）、《神雕侠侣》中提到的郭襄（后来的峨嵋派创始人）等。\n\n5. 宫斗相比武侠，确实在立意上可能更难写出更高的深度。因为宫斗往往局限于后宫这一特定场景，人物和情节容易陷入套路化，缺乏更广阔的视野和深度。而武侠则可以通过武侠这一楔子，探索更广泛的社会、政治、人性和历史问题。但这并不是绝对的，关键还是看作者的思考深度和创作能力。\n\n", "question": "Hello，我是 JioNLP，本职工作是一个程序员。\n\n既然是一个程序员，那么这篇文章就主要来写一写关于网络小说《如懿传》和《甄嬛传》的事情吧（非常合理……），来深度扒一扒那些年，我读过的网络小说和看过的电视剧。\n\n说起《如懿传》，那就不得不从小说作者流潋紫的《甄嬛传》开始。\n\n《甄嬛传》大概是无人不知、无人不晓吧。不光是女生喜欢看，我周围很多男生也都看过这部电视剧，而且里面的台词也都耳熟能详。当然，《甄嬛传》的网络小说大约开始写作于2006年，那个时候国内互联网才刚刚兴起，越来越多的家庭里开始配备了在现在看来十分笨重的台式电脑，主要是联想、清华同方等等牌子。伴随着网络小说也逐渐兴起，起点、晋江，甚至天涯论坛的小说板块等文学小说网站也逐渐风靡全国，大批量的网文作者也随之诞生，成就了一大批作者，其中就包括流潋紫，还有南派三叔、紫金陈，当年明月等等。\n\n当然，网络小说兴起之初，野蛮生长，很多的小说都是互相抄袭，而且那时由于抄袭算法检测手段的匮乏，你作为一个默默无闻的小说作者，如果抄袭了别人的故事桥段，除非真的爆火，一般都没人会去发现，也不会有人真的care。\n\n同样地，《甄嬛传》也抄袭了大量的别的小说，如果你到搜索引擎里搜一下，铺天盖地地都是网上对流潋紫抄袭的讨伐，其中最出名就是匪我思存。关于这块，网上已经有大量的讨论，当年我在读甄嬛传小说的时候，有两点感受，至今印象还很深，一是流潋紫的甄嬛传小说里，有大量的繁复的对人物配饰、服饰、风景、陈设的描写，比如\n\n初夏的天气，风中已带了晴暖的气息。如金的日光透过轻薄的烟霞绿的蝉翼纱滤出几许清凉的意味。窗前门外湖蓝色织暗花竹叶的纱帐皆懒懒委地垂着，透过半朦胧的纱帐一眼望出去，庭院里的栀子花开了雪白灿烂几树，映着满架绯红如霞光的蔷薇，倒也红红白白的妍丽。满宫里皆是静谧无声，只有偶尔不知名的小雀儿飞过，扇动着翅膀扑拉拉飞过，轻啼一声如水。\n\n我们都知道，小说中描写外物，主要是为了以景衬情。但是甄嬛传里这样的描写，和上下文故事情节是毫无瓜葛关联的。少量的还好，但是整部长篇小说中，此类废话描述篇幅非常的大，我怀疑这就是流潋紫用来充字数用的。\n\n再有一点是，我想说，我的麻油，大姐你抄袭了多少红楼梦？就比如沈眉庄入宫说的那句：\n\n回太后，只识得几个字，不曾读过什么书，只读过女则与女训。\n\n这不妥妥的林黛玉的原话吗？而且，《甄嬛传》小说中，还有太后逝世，侍女跟随着“触柱而亡”这一段描写，这一段就来自《红楼梦》中秦可卿淫丧天香楼。另外，如果你学过高中语文林黛玉进贾府的话，一定会对文中描写宝玉和王熙凤服饰头饰的长篇繁复的描写 有比较深的印象。同样的手法被大量运用在了小说《甄嬛传》当中。\n\n红楼梦中：这个人打扮与众姑娘不同，彩绣辉煌，恍若神妃仙子：头上戴着金丝八宝攒珠髻，绾着朝阳五凤挂珠钗，项上戴着赤金盘螭璎珞圈，裙边系着豆绿宫绦，双衡比目玫瑰佩，身上穿着缕金百蝶穿花大红洋缎窄褃袄，外罩五彩刻丝石青银鼠褂，下着翡翠撒花洋绉裙。一双丹凤三角眼，两弯柳叶吊梢眉，身量苗条，体格风骚，粉面含春威不露，丹唇未启笑先闻。\n\n甄嬛传中：我飞快地扫一眼华妃，一双丹凤眼微微向上飞起，说不出的妩媚与凌厉。华妃体态纤浓合度，肌肤细腻，面似桃花带露，指如春葱，万缕青丝梳成华丽繁复的缕鹿髻，缀满珠玉。衣饰华贵仅在皇后之下。果然是丽质天成，明艳不可方物。\n\n此外，网上有大量的作者和网友都扒出了《甄嬛传》的抄袭桥段，例如汉宫飞燕合德、武则天感业寺等等经典故事情节，都能在《甄嬛传》中找到影子。感兴趣的话，可以到网络上搜索一下，铺天盖地的。毫不夸张地讲，《甄嬛传》小说有大半本是抄袭和废话。\n\n但是，《甄嬛传》抄袭地很成功，这点所有人都不能否认，不然，当年我也不会看完这本小说了。总的来说，流潋紫一定是阅读了大量的经典小说，把各个故事里精彩的内容做了一次非常棒的融合，这才让观众对《甄嬛传》过了十多年都念念不忘。所以，流潋紫实际上是一个非常棒的故事精筛者，把过去宫廷和言情小说里非常多的经典情节都糅杂在了同一个故事里，串联起来。形成了一个较为完整的经典故事大赏，这种作用类似于抖音平台上的搞笑宠物视频合集，经典歌曲串烧，等等。实际上，流潋紫最出色的 job 是融合。\n\n当然，小说版《后宫甄嬛传》和电视剧《甄嬛传》基本故事情节是一致的，但是人物立场和叙事视角发生了很大的变化。流潋紫的小说完完全全就是一部女性爽文，宫廷玛丽苏，最核心的思想是：所有男人都爱我，所有女人都没法和我争宠。甄嬛这个人物主角就是用来让女性读者代入的；抛开爽感，客观来讲，这个角色在小说中非常自私，自以为是，别人杀人、争宠都是妖艳贱货，而我甄嬛杀人、争宠，就是迫不得已。所以，本质上《甄嬛传》小说还是让女读者们幻想有这么一个男人钟情爱自己，正如宋小宝说的：“这后宫佳丽三千呐，皇上就偏偏宠我一人，于是我就劝皇上，要雨露均沾，但皇上非是不听呢（得意显摆脸）”。同时，甄嬛自己的宫斗事业尽管有波折，但也顺风顺水。所以，故事里的妃子们要害我，都是她们的错。\n\n而电视剧版《甄嬛传》由于是影视作品，不太表现人物的心里刻画，同时被一众编剧们集体进行了改编，删掉了一些十分狗血的桥段，比如，甄嬛在甘露寺和果郡王相爱，小说中描写甄嬛和果郡王（原书叫玄清）三天三夜手拉着手不曾撒开，以至于三天后两人手撒开，她竟然感觉有一种自己的皮肉初次分离的疼痛感。。。。当然，电视剧里没有表现这些drama 的片段。由此，甄嬛这个角色的玛丽苏特质被从小说中的满分 10分降到了电视剧中的仅有 2 分。\n\n同时，原著小说是第一人称视角，而电视剧则要照顾到很多没有甄嬛参与的支线剧情情节，华妃、皇后、安陵容、齐二哈 这些人物的立体度一下子就上来了。所以，这个电视剧的层次一下子就上来了，每一个人物都有自己的境遇。这也是电视剧长盛不衰的原因，观众们能够从那几个反派身上得到共情，发现原来自己也有反派同样的困扰和不得已。\n\n毫无疑问电视剧《甄嬛传》是十多年来特有的现象级电视剧，也担得起满分10分，相应地，小说版甄嬛传，大概就是个豆瓣 7~8 分的水平。\n\n好了，现在说回到流潋紫的另一部小说《如懿传》，这本小说开始于电视剧《甄嬛传》2010年上映后，由于非常火热，作者和作品自然也是树大招风，前些年网络小说行业野蛮生长的特性，也导致《甄嬛传》被大批网友和被抄袭作者的批判。所以，写作《后宫如懿传》，流潋紫实际上一方面想趁着《甄嬛传》火热，趁热打铁，再赚一波钱；另一方面，也想通过《如懿传》打一个翻身仗，好好打脸那些骂她抄袭的人。\n\n其实说到小说《后宫如懿传》，最早连载于南派三叔办的杂志《超好看》。我不知道有没有网友也曾经看过这个杂志。当年这个杂志来势汹汹，汇集了当时非常出名的网络写手们，其中就包括流潋紫，还有今何在等人。今何在当年在《超好看》上连载过《怎能忘了三国？》，风格非常像《悟空传》，读起来非常带劲搞笑，但后面很快烂尾。这是题外话。当然，当时网络写手办杂志，最成功的还是另外一个抄袭选手小四。\n\n我当年就是从这个《超好看》上面第一次读到《如懿传》的。当然，尽管网文行业发达，造就了南派三叔、流潋紫这样一群网络写手。三叔的初衷和想法非常好，但是纸媒行业与日俱下，《章恰尔，超好看》办了没多久就停刊了。不过我那时候也跟上了《如懿传》小说的更新，还是把小说花了几年时间读完了。\n\n如果说流潋紫写的《甄嬛传》，纯纯就是为了迎合女读者的意淫；那么《如懿传》里，她就想上点价值观了。我猜想，她可能是想证明一下自己：网络写手也不是一个只会意淫的低层次抄袭的女同学。\n\n大约是2012年，当年我在《超好看》上看如懿传的第一回，我觉得写的还不错。主要有这么几点：一是小说里和主情节无关的废话描写少了很多，文风明显比小说《后宫甄嬛传》要老练很多。忘了说了，在这里补充一下，如果你现在重新回去读一下《甄嬛传》小说的头几回，你会感觉流潋紫初写网络小说，文笔非常生涩，像一个高中生、大学生的手法——总之就是青涩、笨拙。但是《如懿传》的开头没有这种感觉，初读就是一个成熟的网络写手的作品。\n\n二是流潋紫刻画的人物，开始逐渐脱离玛丽苏大女主的套路，《如懿传》的主角如懿，一开始叫青樱，没有家世，自己心性也不要强，也不受皇帝老公的宠爱（和电视剧版有区别）。其实在众多女玛丽苏，男龙傲天的爽文中还是独树一帜的。这也是我一开始对这部小说的开头印象还不错的原因之一。\n\n但是，小说《如懿传》这个故事很快就写崩了。\n\n首先，流潋紫当时面对的抄袭压力非常地大，网上铺天盖地是喷她抄袭不要脸的。如果说《甄嬛传》小说，她可以肆无忌惮地照着抄，反正也没人认识她一个不知名小作者。那么到了《如懿传》，顶着名和利，她在《如懿传》里的确基本没有非常明显的抄袭，因为盛名之下，一大群人在拿着放大镜搜寻她小说中哪些桥段是抄袭的，即便真的抄袭借鉴，也在原文基础上改了又改，易容三次，让你看不出来。\n\n但是，失去了抄袭 buff 加成的流潋紫，在小说行文中露出了很多令人捉急的地方，显示了她的真实水平。\n\n小说《如懿传》主要有以下几点硬伤。\n\n一、人物角色脸谱化，跳不脱狭窄的宫斗区间\n《如懿传》的故事里，也有皇后、跋扈的妃子，自己的好姐妹，一个阴险的妃子，一个老实人，等等。其实，后宫当中的斗争，无非就是这些人物脸谱，和《甄嬛传》很相似的。我来对比一下，大家就知道了。\n\n甄嬛传人物：特质——如懿传人物——特质\n\n皇后宜修：阴险、工于心计——皇后富察：蠢笨、坏\n\n好姐妹沈眉庄：善良、单纯——好姐妹海兰：善良、复仇\n\n安陵容：阴险、自卑——令妃、嘉妃：阴险、不择手段\n\n华妃：跋扈、自大——高贵妃：跋扈、自大\n\n齐妃：又蠢又坏——纯妃：单纯、愚钝、自卑\n\n女主的丫头浣碧：出卖女主但浪子回头——女主的丫头阿若：出卖女主一黑到底\n\n女主的丫头流朱：忠心但悲惨——女主的丫头锁心：忠心获得好报\n\n女主的蓝颜：果郡王——女主的蓝颜：侍卫凌云彻\n\n其实可以看出，宫斗故事里的角色类型，无非就是这些类型的人物，就像一个班级里一样，一定会有那么一个学霸，一个捣蛋的男生，一个英语特别好的女生，一个文艺特长生，一个老实巴交的，一个天天非主流的，一个体育健将等等。\n\n只不过，《甄嬛传》和《如懿传》里的人物角色，都进行的排列组合和置换。其实，按着相同的脸谱写下去，读者也就失去了对接下来人物性格变化的期待。\n\n当你读到某个桥段，突然发现，原本某个隐藏的小透明角色，突然变成反派扎你一下，你只会觉得，哦，这样。而不会有惊喜感。因为整个故事都没有跳脱宫斗这条主线，翻来覆去地置换人物性格，就像昨天吃蒸豆腐，今天吃炖豆腐，明天吃麻婆豆腐，天天都吃，口味都吃腻了。\n\n而且，整个如懿传故事嵌套在清朝的框架下，其实十分受限。清朝是历史上皇权最集中的时候，在《甄嬛传》中，华妃轻轻松松一句话就把夏冬春打残废了，理由也非常充足，夏冬春打人。皇帝对此事只是轻飘飘问了一句，而夏冬春的父亲只是一个包衣，自然也不会放一个屁。这一切在清朝都是比较合理的。\n\n而到了《如懿传》里，一个低级宫女（南府乐妓，可以理解为一个歌女）被封为答应（玫嫔），上来就敢和贵妃当街顶嘴，皇后在场都惩治不了两个妃嫔打架拌嘴，小答应都敢给皇后甩脸子走人。这种场面，妥妥的小学生吵架水平，连班主任的威风都拿不出来，哪里还有皇家的等级森严？这种文风和叙事情节，简直和《甄嬛传》不像是一个人写出来的。\n\n而且，《如懿传》里还有嫔妃耍横，诸如蒙古嫁到大清的妃子自称：”我背后是蒙古49部“，”我从蒙古嫁到大清，不是做小小嫔位的“这种台词。假如是寻常夫妻二人，妻子动不动就对着丈夫或者别人说，”我爹是李刚！“你猜丈夫会不会想揍她？外人看了会不会觉得她是个傻缺？更何况，她的丈夫是大清皇帝，一个最有权力的男人，生活在养尊处优的皇宫中，还敢说出这些倨傲的话来，乾隆捏死她，不是太过容易？\n\n殊不知正史上，年羹尧堂堂大将，战功卓著，雍正皇帝皇权紧握，处置年羹尧像是戏耍丧家之犬。在皇权的顶峰，小说中满宫的妃嫔彷佛都石乐志一般，斗的不可开交。总之，这一切的原因，都在于流潋紫无法在宫斗这个框架下，跳脱出来，写出更加广阔的情节内容。\n\n二、故事情节套路化\n《甄嬛传》小说的主要情节其实就围绕一件事，女人们的肚子。A妃子怀了孩子，B妃子恨极了，想打胎，但借了C妃子的手，嫁祸出去。故事来来回回就是这件事。\n\n但是，反反复复写这件事，甄嬛传里却不让人厌烦，这就像《西游记》里一样，每一集都是相似的情节：师傅被妖怪抓走了，孙悟空搬救兵救师傅。当然，每个观众心里都知道，唐僧是死不了的。因为观众们就想看看新妖怪到底有什么神通，孙猴子到底是怎么救了唐僧。在《西游记》之后，再也没有能够超越它的打怪升级小说。\n\n可以说，类似的，《甄嬛传》把打胎和嫁祸这件事，写到头了。到了《如懿传》里，我看流潋紫已经被逼上了一条不归路，为了吸引读者的眼球，在剧情上的编写已经无所不用其极。\n\n比如，《如懿传》中，嘉妃金玉妍，利用富察皇后陷害枚嫔滑胎，用的手法是长期在枚嫔的饮食中做手脚（怀胎十月都不会被人发觉），导致枚嫔长期饮食有问题，最后生下来的孩子是个身体发黑的双性人！！（具体情节记不清了，如果说错了大家原谅）这个情节应该是《如懿传》小说的前半部分，我感觉十分惊呆，感觉滑稽、可笑。\n\n双性人纯纯就是为了博眼球而写的，吃啥饮食能生出来一个怪胎呢？虽然是一个古代宫廷小说，滴血验亲、饮食下药十分不科学，但也不至于博眼球，就这么胡编吧？！当然，《甄嬛传》小说里也有同样类似的雷人桥段，比如叶澜依骑着猛兽扑向皇帝，宫斗画风突然变成了动物世界恐怖片；再比如，甄嬛被嫁娶和亲的时候，果郡王孤身抢亲，顺道和甄嬛在轿子里再次momomo…… 而且再次怀孕。这些情节，电视剧版是删除了的，我认为删的非常好，说明电视剧的编剧们、导演还是有点分寸的。可是，电视剧《如懿传》却把很多荒诞剧情都保留下来，一拍就拍了90集电视剧，这就十分搞笑了。\n\n再有，宫廷里，犯错的妃子们会被处死，《甄嬛传》里处死造谣的祺嫔的方法，非常简单，陈建斌老师直接说了一句：乱棍打死。其实观众们就已经但受到皇权威严，不寒而栗了。再比如，皇帝要处死华妃，给足了华妃体面，毒酒、匕首、白绫，自选一样，了解了事，不必再回话了。其实，读者们在意的是故事情节给人的震撼。\n\n但是，《如懿传》里，有一段让我印象十分深刻的处刑场面，那就是女主角如懿的丫头阿箬陷害如懿，最后计谋失败被处死的行刑场面：猫刑。具体过程看小说原文吧：\n\n如懿看了眼三宝，一挥手招呼几个小太监取了个巨大的麻袋并几只灰猫来，三宝按着阿箬，让两个小宫女利索地扒下阿箬的外裳，喝道：“把她装进去！”……三宝回道：“小主，这些是从烧灰场找来的猫，性子野得很，够阿箬姑娘受的了。”\n\n如懿在廊下坐下，细赏着小指上三寸来长的银质嵌碎玉护甲：“那还等什么，让她好好受着吧。” 三宝用力啐了一口，举起鞭子朝着胡乱扑腾的麻袋便是狠狠几鞭。那麻袋里如汹涌的巨浪般起伏跳跃，只能听见凄厉的猫叫声和女人含糊不清的呜咽嘶鸣。……\n\n三宝打得满脸是汗，扯开布袋，只见几只灰猫毛发倒竖地眺了出来，龇牙咧嘴地跑了。两个小太监将布袋完全打幵，拖出一个浑身是血的血人儿来，气息奄奄地扔在了地上。只见阿箬的中衣被爪子撕成一条一条的，衣裳已经完全被鲜血染透，脸上手上露着的地方更是没有一块好肉。三宝见她痛的晕了过去，随手便是一盆冷水泼上去。阿箬嘤一声醒转过来，身上脸上的血污被水冲去，露出被爪牙撕开翻起的皮肉，一张娇俏容颜，已然尽数毁去。\n\n我读小说这么多年，书虽不多，但也是第一次知道有 猫刑 这么个 刑罚的。整个行刑过程，就非常猎奇和满足读者想象空间。后来我搜了一下，才发现，晚清时期，妓院里的老鸨子折磨不听话的妓女，用的就是这种猫刑，重在毁去容颜。关键是，把妓院里的下作手段，搬到宫廷里，合适吗？流潋紫不觉得突兀吗？\n\n而且，故事里的行刑过程，是在女主如懿自己的宫里，也就是自己的家里，不觉得晦气吗？，如懿作为宫里的娘娘，还要”细赏着自己的高级护甲，慢慢折磨这个背弃自己的丫头“。就让人感觉，你作为大清帝国的妃子，还顾不顾乾隆皇帝的颜面？再有，如懿你是一个娘娘，到底是谁给你提的建议，使用这个招数？是不是有 SM 倾向？\n\n《如懿传》里的故事由于情节套路化，导致作者在写作时，为了抓住读者的眼球，写了非常多猎奇的、不合理的怪诞情节。我就先说这么多，如果感兴趣，我还可以继续扒。\n\n三、人物非常纸面化，呆板无力\n《甄嬛传》的成功，很大程度上是源于女反派们也都有各自的立体的人物形象，比如，华妃对皇上的深爱，安陵容无法克服的原生家庭的障碍，皇后对于姐姐的怨恨和儿子的去世，曹琴默因女儿被虐待而两面三刀等等，甚至包括小丫头斐雯告发甄嬛，也都有甄嬛宫里委屈她在先。电视剧尽量照顾到了每一个角色的丰满度，每一个人物都是一个完整的人，有自己的合理的行事动机。否则故事就不成立。\n\n但是到了《如懿传》，流潋紫是按照第三人称来写作的，其实对于塑造丰满立体的人物形象是非常便利的，但是，整本书都没有一个立体的人物形象，故事里的人物都在发神经病。\n\n富察皇后，在小说故事里的设定是出身满族大家族，地位尊贵，位居中宫皇后，皇帝也待她不薄，而且她还有儿子，有女儿，要啥有啥。在真实历史上，乾隆非常爱这一位皇后，一辈子写的三万多首打油诗里有不少是纪念富察皇后的。其实你也可以想象，富察皇后要啥有啥，妥妥的一位家世显赫白富美。\n\n但是在小说《如懿传》里，这个皇后形象要猥琐地多。小说里的富察皇后要啥有啥，却总是忧心忡忡，担心自己中宫地位不保，要去陷害别的妃子，主要是如懿。陷害的动机是什么呢？没有，故事里没有交代。同样的，故事里也没有写哪个别的妃子会对她的皇后地位有威胁，但她就像有疑心病一样，总感觉有人要害本宫。总之，这个人物角色根本就不成立。\n\n电视剧《如懿传》对富察皇后的改动有很大，第一个是增加了乾隆选秀情节，让富察皇后恨如懿找到了一个原因，这使得后来富察皇后的行为稍稍合理化，这些在小说里不存在。第二个是让富察皇后更像一个真实的母亲，在把女儿送去和亲的时候，还偏袒护着自己的女儿；而这段情节在小说中，富察皇后完全像个后妈一样，主动拱手把亲女儿送出去和亲。读者读到这一段，就会觉得，富察皇后你还是个人吗？这样的人能当皇后？你是武则天吗？\n\n很多人在喷电视剧里董洁演的富察皇后小家子气，其实相比于小说中这个最崩塌的角色之一，她演的已经相当给这个角色予以挽救了。编剧们、导演和演员，共同把这个完全崩塌的角色挽救了一些。\n\n另外一个崩塌角色是嘉妃金玉妍，这是一个从朝鲜（小说叫玉氏）送过来的贡女。在故事的前半段里坏事做尽，读者完全找不到这个人物的干坏事动机。像《甄嬛传》里，华妃前期也坏的让人想掐死她，但是在她死前，也给她一个非常好的人物转折，那就是她十分深爱皇上，而且，华妃爱皇上在故事的整个进程中都有表现出来，包括演员演的也非常好，这让读者能够接纳这个反派角色。而《如懿传》里的这位嘉妃，就没有这么好的刻画，读者感受到的，除了她害人，嫁祸打胎，就没别的了，你说她非常爱皇上，因爱生恨？小说里完全没叙述。整个前半段，读者的感觉就是她为了害人而害人。\n\n到了她快领盒饭的时候，小说和电视剧才交代了，原来这个嘉妃最爱的是自己朝鲜的一个王爷世子。哦，读者终于明白了，她原来也有最爱的人。可是，你爱你的就完了呗，为啥要害那么多别的嫔妃？小说和电视剧里的交代是：她希望再次见到那个王爷世子来大清的时候，能够顶峰相见。我读到这心里就6个字母：wtf？mmp……这就是你害人的理由？\n\n还有，历史上，嘉庆皇帝生母令妃在小说里叫魏嬿婉，本身这个名字起的就挺青楼的，《如懿传》里的这个令妃出身低贱，为了上位不择手段。不择手段到啥程度呢？反正做事完全没有任何动机，纯纯是一个打胎机器，只要有别的妃子怀了孩子，她就打胎，陷害，是别的妃子地位比她高？还是别人威胁到她的地位？都没有，就陷害，就埋头打胎，稳坐后宫打胎第一把交椅。\n\n《甄嬛传》里的皇后也喜欢打胎，但是她因爱生恨，也因为自己的儿子悲惨病死，因此痛恨别人有孩子，尽管歹毒，但整个心路历程也都符合一个母亲、一个女人为了爱情而自私的正常思维。而魏嬿婉则完全没有任何故事交代，她可以为了爬上龙床抛弃初恋，也可以为了利益而危害皇帝，还能够为了保全自己，而把自己母亲都出卖了。在这个人物身上，读者找不到任何人性的影子，完完全全就是一个人类最低道德底线都没有的女恶魔。但是这一切是为了什么呢？当上贵妃享受众人的服侍？令妃已经做到了呀。抑或是想要把皇帝搞死，自己当皇帝吗？可是整篇故事里也没有任何相关的交代。令妃这个角色在小说中是最最单薄的一个人物角色。尤其是到了《如懿传》小说最后一部，我感觉流潋紫她自己对这部小说已经完全写不下去了，草草收尾。在故事的结尾，令妃被乾隆一个怀疑的眼神吓得站不住倒了下去，意在表达皇帝的疑心可以害死人。。。这是初版小说情节，由于太过 drama，流潋紫再版时已经把这个离谱情节改掉了。电视剧版还对令妃这个角色稍微增加了一点合理性，为了保证故事叙事的正义压倒邪恶，给了令妃一个恶有恶报的结局，但也属于九头牛都拉不回来的程度。\n\n《甄嬛传》里的太后，并不参与后宫斗争，主要是为了皇帝的孩子操心，这其实符合太后的身份，也符合一个当奶奶的人的心理。而《如懿传》里的太后，也就是甄嬛，好像是上一界宫斗没斗够似的，在儿子的后宫里纵横捭阖，叱诧风云，动不动训斥这个，呵斥那个，跟儿子的老婆争风吃醋，还跟个特务头子一样地，安排几个妃嫔，像特务一样地监督皇帝，不知道的还以为太后是血滴子的头目。如果没记错的话，这个太后总共安排过枚嫔、舒妃、庆嫔当特务，在皇帝身边进言，吹耳边风。具体吹什么耳边风，想要达到什么效果？故事里交代也十分单薄。唯一和她有关联的剧情，就是自己的女儿的远嫁。剧中人物也自称，除此之外，再无别求。就为了这么点愿望，需要安排三个特务妃子监视皇帝吗？而且人物角色冗余，庆嫔这个角色，完完全全没有戏剧冲突，纯纯酱油角色，属于写了也白写。\n\n除此之外，没有了抄袭buff 的流潋紫，像是成精的妖精被打回原形，写出的整个故事都没法看，打胎和陷害贯穿整部小说，但和雍正皇帝相反，乾隆皇帝儿子女儿非常多，女反派们尤其是嘉妃、令妃，打胎能力非常强悍，但孩子却越生越多，可谓十分滑稽。\n\n而且，小说中还设计了皇帝不允许高贵妃怀孕的桥段，我就纳闷了，高贵妃怎么了就不能怀孕了？在皇权集中的清朝，是有外戚干政吗？高贵妃不是包衣出身吗？她生个孩子能翻了天吗？让人感觉莫名其妙。流潋紫有没有想到过一种可能，那就是，有的女人天生就是不孕？\n\n而且，为博眼球，小说里还有，皇帝把如懿的蓝颜知己，阉了成了太监，然后安排他观看帝后的夫妻生活…… 令妃和一个什么妃，给皇帝灌了迷情药，大白天同时和6个妃嫔共同那个…… 我想说，流潋紫，别看你挺文静一中学语文女教师，你懂的可真够多的啊！！！\n\n流潋紫更新《如懿传》小说的频率，大概就是一年一部，到了最后两三部，我读着读着都感觉到作者真的已经黔驴技穷，写不下去了。真的是为了资本在硬扛。尤其是最后一本书，胡编乱造，已经完全没有故事合理性，草草收尾。我当时可能为了知道流大姐打算怎么圆这个故事，才匆匆扫完这个小说。\n\n四、流潋紫的格局\n《如懿传》中的太后，就是甄嬛，两部小说之间的关联，很像金庸小说的编写路数。每一部金庸小说都有一个贯穿其他小说的索引人物，比如，《鹿鼎记》中就提到过”前朝的令狐冲大侠“，《碧血剑》里的阿九姑娘，也就是鹿鼎记中的九难师太，《神雕侠侣》中也提到过郭襄，后来创立了峨嵋派，与《倚天屠龙记》相连接等等。\n\n虽然金庸的每部小说都在写武侠，但是每部小说其实都在推陈出新，打破以往固有的思路，金庸不止写武侠，也写儿女情长，也写人性的善恶，也写残酷的政治斗争，还戏谑历史的荒诞和道德的无度。武侠只是金庸小说的一个楔子，有的小说写的正义凛然，有的写的玩笑戏谑，有的写的场面宏大，夸张震撼，有的还采用了特殊的叙事手法，比如《雪山飞狐》。总之，金庸的武侠世界之所以有这么大的影响力，核心还是能够打破武侠的牢笼，创造一个读者们没见过的新奇的想象世界。同时，还拥有对社会、政治、人性、历史的思考，让人回味无穷。\n\n而流潋紫创作的《甄嬛传》、《如懿传》，其实也模仿了金庸的这种贯穿式小说连接手法，只不过，楔子不是武侠，而是宫斗。\n\n我想，流潋紫对世界的思考，远在金庸之下，她笔下的宫斗，就只是斗，一群女人们死去活来的斗，由于场景、人物的局限，她笔下的女人们的宫斗只会越来越离谱。但是，这些斗除了让读者像阅读《满清十大酷刑》，《打胎的N种方法》一样的猎奇之外，还有别的什么呢？大概就是，别的什么也没有。你看过，就看过了，然后在你脑海里存在的，就是打胎 、打胎、打胎。不知女权主义者、女性独立主义者们，看《如懿传》会是什么心理感受。\n\n当然，流潋紫曾经说过，之所以写《如懿传》，立意是为了写一个皇后的婚姻悲剧。可是，小说中，我只看到了一个非常理想化的皇后，即便经历了挫折，自己也经历了背叛，也尝试过算计，也懂得防止被算计。在这一切的一切之后，她却满脑子都还在惦念和皇帝的”墙头马上遥相顾“。连甄嬛都知道：”皇帝的夜晚，从来不属于她一个人“。可是如懿却是个十足的恋爱脑，没有任何成年女性，尤其还是一国皇后应有的冷静和理性。\n\n你敢想象一个40多岁的中年女人，还像个情窦初开的小姑娘一样，拿着两朵小花追着老公问”墙头马上遥相顾“吗？在理想主义的幻想下，如懿的归宿只有和皇帝分离。这就是你流潋紫对婚姻的深度思考吗？\n\n换句话说，《如懿传》根本没有真正展现出皇帝和皇后婚姻的现实无奈，只是展现了一个恋爱脑皇后受不了皇帝太自私和别的妃子在一起而断发。问题是，作为一个皇后，你是第一天知道皇帝还有别的妃子要宠幸吗？所以，整个故事，在立意上，都是立不住的。\n\n当然，很多网络写手的写作立意都不高，我曾经一个同事，看龙傲天类型的小说能够做到5天一本几十万字的小说，读起来非常快，也非常爽，但是读完之后留不下任何东西。这的确是人们的精神享受。这本无可厚非，但是，在享受之外，流潋紫却还想追求一些别的，希望让读者记住的，有思考的内容。但是却力有不逮。甚至，包括最基本的小说语言，也出现过”本宫乏了，要去眠一眠“这种半文不白的自创文言。\n\n补充一下，也有很多优秀的网络写手写出过很令人难忘的立意，比如《悟空传》中对自由的渴望，《明朝那些事》中对人生意义的思考，等等。事实上，金庸的《鹿鼎记》才是龙傲天种马文的鼻祖，《鹿鼎记》中深刻地揭示了对明清历史的正确评价，通过韦小宝的嘴里说出来，同时也影射了当时香港和大陆的关系。\n\n话说回来，其实，《如懿传》电视剧版已经在尽力弥补小说的缺失了，编剧们、导演、演员们，真的已经尽力了。如果说电视剧版《如懿传》值6分的话，那么小说也就是豆瓣 3分水准。当然，有很多观众还会因《甄嬛传》而对《如懿传》手下留情。\n\n因此，很多人批评流潋紫，和甄嬛传，时无英雄，使竖子成名。的确，流潋紫成名有很大一部分是运气和机缘。\n\n不过，《甄嬛传》更像是各种经典小说的浓缩提炼版，之所以好看，根基还是在于别的故事经久不衰，耐看，当然，流潋紫对这些故事加工改编融合也功不可没。《如懿传》则真的是把流潋紫放在太阳底下暴晒了。不过，我依然见过有人觉得《如懿传》强于《甄嬛传》，大概就是大杂烩乱炖火锅吃腻了，就喜欢生吃刚剃毛的猪肉吧。\n\n文章就先写到这里。。。。。。。\n\n根据以上文章，尽量简要回答问题：\n1、甄嬛传和如懿传的小说作者，在写作小说时有哪些缺点和不足？\n2、甄嬛传为何能够成为现象级电视剧？\n3、如懿传中，扮演富察皇后的是谁？\n4、金庸小说中，都有哪些串联各个小说的人物？\n5、宫斗相比武侠，难以写出更高的立意吗？\n"}
{"score": 2, "question_type": "【闭域问答】", "correct_answer": "\n{\"房源单价\": 2886, \"房源面积\": 47.81, \"房源总价\": 13, \"地址\": \"成华区一环路东三段6号\"}\n\n除了这个json之外不允许输出任何其它文字。\n", "question": "13万总价，买套一环路内的住宅房，出门还有地铁和菜市场！\n说实话，确实夸张，把大伙儿都给整激动了，跑来咨询我的读者不下5个。\n\n图片\n\n给不了解成都市场的朋友们摆一哈，这是啥概念：网传城南某红盘要求绑定的车位，一个都要卖30万。\n\n住宅卖得比车位还便宜，到底有没有坑？\n\n公子羽亲自去实地踩了盘，有些事儿还是要给大家分享下，各位激动之余，不妨花个3分钟看一看。\n\n闲言少叙，上干货。\n\n图片\n东风路居民点\n\n房源价格：2886元/㎡起\n\n产品建面：约47.81㎡\n\n产品类型：5层步梯房\n\n楼盘位置：成华区一环路东三段6号\n\n点击下方小程序卡片，查看房源详情👇\n\n我们建了多个楼市交流群\n▽扫码回复对应的关键词可加入▽\n\n图片\n\n在同事的提醒下，我找到了这套房子的来源。\n\n据阿里拍卖页面显示，该房源是位于成华区一环路东三段6号东风路居民点11栋的住宅。\n\n也就是说，这是法拍房，价格确实是网传截图所示的137988元，折合单价仅需2886元/㎡！\n\n该房源计划在2024-06-14 10:30开拍，点击文章左下角“成都房小团”公众号名字关注后，回复【东风】即可获得拍卖链接。\n\n图片\n\n图源：阿里拍卖\n\n值得注意的是，13万只是起拍价，不出意外的话，最终成交价会比这个价格要高出一些。\n\n不过因为房源自身条件比较具体，价格也不会高出太多。\n\n作为参考，同样是阿里拍卖数据，一样是位于一环路东三段附近、面积更大的老破小法拍房，成交总价也不过在50万内。\n\n图片\n\n图源：阿里拍卖\n\n另外，二手房市场价显示，该小区此前成交的同面积房源，总价在48万元左右，整个小区近期成交均价11024.5元/㎡，在主城区的老破小中也属于比较便宜的水平。\n\n同区域其他老破小，最贵可以卖到1.9万元/㎡，一墙之隔的东风路北二巷4号也要1.2万元/㎡。\n\n所以，单从价格来看，这套房源还是比较有性价比，并且也有一定的价差，预算有限又想买成熟地段住宅的读者朋友，确实可以考虑。\n\n当然，到底要不要下手，还请看完下面我的踩盘报道再决定。\n\n图片\n从地铁玉双路站F2口出来，右手边第一个路口进去，我就到了目的地，直线距离约150米，很方便。\n\n图片\n玉双路地铁口实拍图\n\n实地看来，沿街的一侧小区均是做了“城市更新”，外立面有很大程度的改善。\n越往里走，老成都的烟火气就越浓。\n\n图片\n\n小区周边实拍图\n\n“小伙子，租房还是买房哇？”\n\n看到我掏出手机到处拍照，“菜市场”口卖杂货的大姐主动跟我打招呼，一旁的广告牌上明码标价：\n\n租房单间350-700元，标间700-1200元；\n\n买房56㎡套三46万，89.89㎡套三71万。\n\n主打的就是一个房东直租（售）、公开透明，想来，东风路居民点这套房源，总价也不会于这个价格差出太多。\n\n周边小区价格实拍图\n\n“那啥……我……我想租套房。”盛情难却之下，我同意了大姐的带看邀请，而后走进菜市场，直奔目标小区而去。\n\n目标小区就在菜市场里面，门口两边是各种摊位，沿街摆放，因为不是高峰时段，小区里一点都不吵。\n\n小区外菜市场实拍图\n\n“这一大片房子，都是80年代的拆迁安置区。”大姐向我介绍说，“前几年说要拆迁，但是一直没看到文件，估计也是难拆了。”\n\n穿过保安室后，是个不大的中庭，中庭两边各有一栋步梯房，一梯两户，6层楼高。\n\n如果不是因为房龄太高，还没有电梯，按照当下流行的楼市话术，那也是个标准的“真洋房小区”。\n\n图片\n\n小区实拍图\n\n中庭的地面看得出是近些年经过了维护，沥青上没有多少岁月痕迹，不像外墙那样斑驳。\n\n图片\n\n小区实拍图\n\n大姐见我盯着外墙看，以为在嫌弃，向我推销：“这小区剩的房子不多，你要抓住机会，错过就是罪过。”\n\n从窗户外飘扬的“万国旗”来看，这个小区的入住率确实不算低。\n\n图片\n\n小区实拍图\n\n一圈逛下来，小区环境没有刻板印象里老破小的那般脏乱差，生活噪音也还可控，整体来看自住还算将就。\n\n并且，房子距离地铁站不远，并且直线不到1公里就是太古里，2站地铁就能到建设路。\n\n另外，在小区的南侧直线约200米还有未来中心商业，从生活上来说也是十分方便。\n\n图片\n\n区位示意图\n\n图片\n\n因为法拍房源要在开盘前2天集体看房，这里我们就结合我看的同小区、同房源的感受，和法拍页面的实拍图跟大家聊聊。\n\n该房源位于小区6楼，视野比较开阔，但因为是顶楼，下雨天漏水会是大概率时间，比如从室内照片来看，客厅的天花板就存在漏水。\n\n图片\n\n客厅实拍图 图源：阿里拍卖\n\n从户型设计上看，这就是个标准的一室套房，入户位置的餐厅与客厅之间有墙体阻隔，使得房子的空间感有限。\n\n图片\n\n餐厅实拍图 图源：阿里拍卖\n\n餐厅一旁是厨房，在动线上比较人性化，厨房有独立开窗，有利于油烟排放。\n\n厨房实拍图 图源：阿里拍卖\n\n主卧设计比较不错，休息区与阳台相连，阳台是当下豪宅项目流行的“全景舱”设计，270°三面采光，采光和景观视野很不错。\n\n图片\n\n主卧阳台实拍图 图源：阿里拍卖\n\n可能比较尴尬的一点就在于，房子的厨房直接与卫生间相连，从心理上可能会有些抵制，加上是暗卫，“串味”的话还是会有点难受。\n\n如果后续买到，在卧室与卫生间不是承重墙的前提下，建议将卫生间与卧室打通，封掉与厨房之间的门。\n\n这样一来，既能避免生活上的尴尬，也能获得一个足够方便的套房主卧，生活舒适感有所提升。\n\n原始户型图没能找到，这里我给大家画了份草图，跟众多实拍图放在了一起，大家可以点击下方小程序查看👇\n\n图片\n\n总的来看，作为自住考虑，这套一环内的老破小确实有上车的价值：\n\n临4、6号线地铁口，出行方便，与太古里和建设路商圈相近，生活方便；\n\n成交单价预计在1万元/㎡左右，就能买个47㎡的单间，适合预算十分有限的单身小年轻或无孩家庭。\n\n不过，这里也要提醒下各位，该房源仅接受全款支付，并且如果是想要买来等拆迁，或者是用作书包房，我劝大家还是慎重。\n\n拆迁的事儿上面有提到，就不做赘述，从学校来看，该小区是被划入了成都市列五书池学校，按照当地人的话来说：“放在成华区，也只能勉强得个三等奖。”\n\n图片\n\n成都市列五书池学校实拍图\n\n好了，以上就是最近网传“13万买进一环路”消息的全部消息，看完后，你还想买吗？欢迎在评论区留言讨论。\n\n根据以上文字，回答问题：\n文中介绍了几个房源，单价、面积、总价和地址分别多少？每一套房源，请都按 json 格式输出结果，schema 为：\n```\n{\"properties\": {\"房源单价\": {\"title\": \"房源单价\", \"description\": \"房源单价，单位：元/平米\", \"type\": \"float\"}, \"房源面积\": {\"title\": \"房源面积\", \"description\": \"房源面积，单位：平方米\", \"type\": \"float\"}, \"房源总价\": {\"title\": \"房源总价\", \"description\": \"房源总价，单位：万元\", \"type\": \"float\"}, {\"地址\": {\"title\": \"地址\", \"description\": \"房源地址\", \"type\": \"string\"}}}, \"required\": [\"房源单价\", \"房源面积\", \"房源总价\", \"地址\"]}}\n```\n\n如果房源里 单价、面积、总价和地址，任何一条信息缺失，则不输出结果。\n请勿输出除json外的任何信息，每条房源一条 json。\n"}
{"score": 5, "question_type": "【闭域问答】", "correct_answer": "答案应当尽量简洁明了\n因为文中开头以玩笑口吻提到过，作者肛门附近疼痛产生小肿包，自己误认为是大姨夫（其实暗指女性月经，大姨妈）来了，结合肿包破裂流血，所以会提到自己最初的判断并没错误，是一种开玩笑的说法。", "question": "# 从不吃辣椒，到无辣无味\n\n大概是2025年元旦后的一周，我的肛门附近就开始疼痛，洗澡的时候我一摸，发现好像是一个火疖子一样的小肿包。我没当回事，以为是工作不顺心，精神压力大，长期失眠导致的内分泌失调，俗话说就是大姨夫来了。\n\n其实也没过一两天，肿包就从大约黄豆大小慢慢发展变大，越来越疼，坐在公司工位上办公的时候，彷佛屁股底下坐着一颗手雷，我都不敢把整副身体的重量压在手雷上，生怕手雷炸了。我一面忍着疼痛对着电脑办公，一面想到了抗日神剧里的裤裆藏雷，妈的！这个梗居然也有一天玩到我自己身上了。\n\n在选择继续坐工位上班和请假回家之间，我犹豫了1分钟吧，还是和领导请了假，回了家。疼痛根本让人来不及多想。\n\n请假之前我工作上还有好几个破事需要处理，全是各种行政流程和客户对接。有时候感觉这几个流程离不开我，我一旦离开，公司同事马上就会给我打电话，但是身体的剧烈疼痛会立即给人强烈的警告——所有事情都没身体重要。\n\n请假的时间正好是周五，我选择连上周五和双休日，正好三天，好好休息，吃好点，多睡睡觉，等火疖子慢慢消下去。\n\n趁着周末，我给自己炖了韩式辣酱暖锅，说白了就是韩式大乱炖，把肥牛、泡菜、豆腐、北瓜、西红柿、金针菇、鸡菇、基围虾一起，用韩式辣酱做配料，一顿，出锅就能吃了。真的，除了调料不一样，和东北大乱炖没啥区别。\n![](https://www.jionlp.com/blog_image/20250205/e8225c7873.jpg{width:50%;height:auto})\n\n\n韩式拌饭、韩式泡菜，我很多年前就偶尔吃过，我之前都觉得韩式的饭都一个味，就是泡菜味，丝毫get不到这泡菜味到底哪里好吃。\n\n可能是在四川生活好多年，培养了吃辣的能力，我自己又在网上买了韩式辣酱和泡菜，自己炖煮。矮马呀！味道很不错！而且韩式辣酱基本上没什么辣味，辣度完全不及四川菜的三分之一。\n\n以前在北京，稍微吃点辣椒，我就开始浑身冒汗，其实就是辣椒过敏体质。我吃饺子都是蘸醋的，当年看着四川同学往饺子上浇辣椒油，那感觉——浇给！我觉得匪夷所思，这饺子除了辣味，还能吃出来里面的味道吗？\n![](https://www.jionlp.com/blog_image/20250205/a474aaaead.jpg{width:70%;height:auto})\n\n\n现在在四川，不知不觉中，基本上每个菜都离不开辣椒了，我以前鸡蛋清炒莲白，就放点耗油，但是现在我就必须得放小米辣，不放点就感觉没味，好寡淡的。\n\n我也终于明白，北方人的微辣，就是菜里放一两颗辣椒作为点缀，而四川人的微辣，就是比特辣微微少放一两颗辣椒。但是两者的辣度体验，应该差不太多。\n\n如果要做辣度排行，那就是 `四川的特辣` > `四川的微辣` >>> `北方的特辣` > `北方的微辣`。\n\n用我同学的话说，就是四川人微微一笑，辣死北方佬。\n\n# 辣椒吃出病\n\n我原本的美好愿望，是周日晚上火疖子就消下去了，第二天美美的上班。但是周日晚上我疼得连睡觉都睡不着了。我都不敢去摸那个火疖子。\n\n于是我又给领导告了假，准备第二天去医院看看。\n\n门诊医生是个40多岁女医生，她说脱了裤子我看看。\n\n其实到医院看病之前，我还觉得，男病人脱裤子给女医生看，女病人脱裤子给男医生看，还挺羞耻的。但是疼痛会让人完全打消这种道德上有关羞耻的想法。\n\n医生给我的形容是，嗯~鸽子蛋大小的脓肿。\n\n我说，啊？我前两天摸一下还只是绿豆子大小，这么快就鸽子蛋大小了？？\n\n鸽子蛋，没见过，是和鹌鹑蛋一样大小吗？嫁入豪门的女明星最喜欢炫耀自己的鸽子蛋。如果女明星穿着婚纱，手上炫耀的是一个鸽子蛋大小的脓肿，那是怎么一副画面？\n\n医生说，你这情况，肛周脓肿，已经感染化脓，动手术吧，住院一到两周，花费报完医保不到两千。时间合适的话，今天就能手术，不行地话就明天。\n\n不是，大姐，这么快的吗？我才年纪轻轻，就要上手术台了？\n\n医生面无表情，见怪不怪了，她说还见过肿的像梨子大小的脓肿，手术痛苦比较大，我这算什么？\n\n我心想能感染到梨子大小的肿块，那裤裆藏得就不是手雷，而是地雷了。\n\n现代社会，每个人总会上手术台的，我曾幻想过我因腰椎间盘突出而上手术台，但我完全料不到肛周脓肿会瞬间把我送上手术台。\n\n我倒不是害怕手术，而且这个手术也没什么风险。\n\n有时候我会强烈感受到命运的不可操纵。\n\n就在元旦当天，我一如既往地去大慈寺烧香拜佛，我手里拿着香，向药师佛三叩九拜，祈求我的腰椎间盘突出不再复发，新的一年，健健康康。\n\n结果刚祈祷完，我就要上手术台了。我事后在想，是不是我写西游记解读，把佛祖和菩萨写的太不堪了？[**孙悟空是怎么被逼死的？**](https://www.jionlp.com/blog/%E6%9D%82%E6%96%87/20241027)\n\n读书时候，我从来不信什么神佛保佑，我好端端一个相信科学，985毕业的工科大学生，从事人工智能工作，相信西医，相信科学才是正道。但是不知道从哪年开始，我每年都去大慈寺烧香。\n\n若人已知的事归科学管，则人未知的事就归宗教管。\n\n宗教天然就和不确定、不可知扯上关系。对于某些事情，我不知道，我也知道我不知道，所以我去祈祷神佛。\n\n# 住院的痛苦自己受着吧，谁还能替你咋的？\n\n手术前的一晚上，我没睡着，不是担心手术风险睡不着，而是疼得睡不着。\n\n住在医院里，窗户开着，时不时有冷风灌进来，同病房的另外两个病人睡得鼾声大起，我在床上疼得翻来覆去。\n\n迷迷糊糊，到了2点半，我总算是困得睡过去了，早上5点半，值班护士喊我灌肠，开塞露灌了两次。灌肠的感觉酸爽无比，强行腹泻，第一次牵肠挂肚，痛苦难受，让我想起了安陵容服用息肌丸，是不是也是这样的感受？但是第二次灌肠反而让我有一种依赖感，我有点喜欢上这种牵肠挂肚的折磨感了。甚至还想来第三次。\n\n这种对痛苦的依赖有点斯德哥尔摩综合征的意思，吃辣椒也是这样，受虐狂似地疯狂想吃辣椒，越痛越吃。怪不得四川的 SM 这么多。\n\n我擦屁股的时候，突然发现，纸上全是血，脓肿已经破了，裤子上滴的都是血，像是来了月经，看样子的确是大姨夫来了，我一开始并没误诊。\n\n早上7点钟，我扎了液体，身穿病号服，进了手术等候间。里面还有一个年轻女孩，胳膊上绑着纱布，打着石膏，她特别外向，社交牛逼症犯了，和我病友之间交流病情。\n\n原来她是打滴滴网约车上班，出了车祸，对方私家车全责，滴滴司机无责，但滴滴司机却没受伤，乘客坐在后排手臂骨折了。对方私家车有保险， 基本都能报下来，顶多扣分呗。只有这女孩受伤的世界达成了。\n\n不过她倒也不伤心，说是已经第三次手术了，已经没什么感觉。\n\n等到我进了手术室，麻醉师给我来了个半麻，我想象中的半麻是我下半身麻木，上半身清醒，边询问病情，边等着医生给我手术。\n\n不知道是不是我晚上没睡好，麻醉下去，我很快就昏睡过去。不知道过了多久，医生把我喊醒，用镊子夹着一小块肉在我眼前，给我说，看，给你割的痔疮和切下来的脓肿块。那块肉形状挺不规则，还带着血，但更多看起来像是一块白色的脂肪。\n\n我万万没想到手术完了，还有给病人展示割肉这个环节，这彷佛是我刚生完孩子，接生医生抱着血淋淋的孩子到我面前，看，这是你的孩子。\n\n如果我是唐僧会有人愿意吃这块肉吗？\n\n生孩子是当个宝，全家都宝贝着，我这是一坨烂肉，割下来就丢掉。\n\n我看了一眼那块肉，就马上闭眼，我还是睡着比较好。\n\n我只看了一眼啊，大概两秒钟，事隔这么久我还是没忘掉那块肉。我想我这辈子都很难忘记了，真是一眼万年啊。\n\n麻醉药刺激着我的身体，手术做完以后依然下半身毫无知觉。但我脑子虽然昏昏，但依然很清晰，我记得让医生护士带上我的被子，带上我进手术室换下的鞋。\n\n我当时脑子里强烈地感知到要保持清醒，因为我是一个人去看病，一个人住院，我要自己为自己负责。\n\n# 术后才是受罪的开始\n\n早上做完手术，就回到病房，身上插着管子，输着液，浑身不能动，等到了晚上下半身麻木消失，才差不多能下床活动了。\n\n但是活动干什么呢？上厕所小便，没吃饭，也不让多吃，也不建议多活动，就躺着吧，昏昏沉沉躺一整天。\n\n这一整天几乎就是半瘫痪状态，没吃没喝，头昏脑胀，而且伤口处开始疼痛肿胀，塞着纱布止血。就这么一直忍着。如果要形容地狱的话，那么，生一场大病，然后忍着疼痛，躺在病床上一直躺着就是了。\n\n我想不明白，为什么有那么多人，会为了亲人，花大价钱去治一些几乎不可能治好的疾病。让病人承受长期躺在病床上的疼痛，一点生活质量都没有，图什么呢？\n\n我想起来新闻里有个 40多岁的女儿，她父亲已经卧病几年了，由于老人是退休军队老干部，所以医药费用国家全包，而且只要老人活着，就可以每月从政府那里领取几万块的退休金。新闻里的女儿，其实是为了父亲的每月退休金，所以坚持医院全力医治老人，医疗费用她不用出，反而还可以每月固定领钱。这样的好事，女儿自然愿意老父亲活着，哪怕天天痛苦，像个植物人。\n\n但是，让一个躺在病床上，插着管子活很多年啊！如果说是植物人也就罢了，关键是人的意识是清醒的，清醒地承受病痛，我都完全无法想象这样的日子，到底是会不会麻木，痛得麻木。像极了黑客帝国里，生活在培养皿中的人类，甚至，不如那些活在梦里的人。\n\n所以琼瑶活到 86岁，翩然选择自杀，我真觉得是个不错的选择。\n\n我在想，如果我到老了，我想怎么死去？既然安乐死是非法的，那我希望麻醉师给我来一针全麻，让我完全失去意识，然后给我随意肢解掉。\n\n# 隔壁兄弟\n\n叫隔壁不太合适，其实就是我病床旁边一床，躺着一个和我同样病的 29 岁年轻人，中间拉个帘子，以区隔隐私。\n\n这个人，自从我住进医院，他就已经手术完修养了，我观察他晚上9点钟差不多就睡着了，早上8点才睡醒，上午输液的时候，他还会眯一个小时，午饭过后，他又睡了两个小时。我掐指一算，他一天能睡15个小时，三分之二的时间在睡觉。\n\n隔着帘子我怎么知道他在睡觉？打呼噜啊！倒不是那种震天响的鼾声，而是相对分贝比较低，节奏均匀的鼾声。\n\n医生做了一天手术，查房的时候，走过来对我说，你是比较仔细的人，术后比较注意，但你看隔壁这个，就是个大大咧咧的人，伤口什么的，可能没那么仔细。\n\n我脑海中浮现出宋丹丹在说：没心没肺的人睡眠质量都高。\n\n我十分羡慕这种一天能睡 15 个小时的人，像我这种小心眼的才睡不着呢。\n\n住院之前，我睡眠质量差得不得了，晚上12点躺在床上，丝毫没有困意，忍不住翻一下手机，凌晨2点，早上7点多就醒了，白天又睡不着。\n\n有时候，工作上的烦心事真的挺熬人，甚至，工作上的烦心事都解决了，我还是会失眠。生物钟被打乱，再也回不来了。\n\n生病虽然痛苦，但我远离了工作上的琐事，睡眠质量得到了提升，但我撑死一天睡8个小时，实在是不行了。我躺在病床上，望望窗外，看看手机，死活睡不着。\n\n我看着网上的搞笑的段子，笑出了声。\n\n彼时隔壁窗兄弟的脓肿正在流血，疼得直叫，陪同他的家人在旁边蛐蛐我，这人心态也太猛了，一个人来做手术，还能笑得出来。\n\n对哦，我完全感觉不到住院手术无人陪同的凄凉感。不知道是不是已经完全看透，人生本来就是孤独的。\n\n陪床的家人可以帮忙递水，帮忙扶着上厕所，但是也只能做这些了。该承受的病痛，该受限的自由，都得病人自己一一承受，他们无法代劳。\n\n# 让你吃辣，该！\n\n术后第二天，医生说可以吃饭，正常解大便了。但是不能吃辣椒，不能吃肉，不能吃蔬菜，不能吃油炸食品，不能这个，不能那个。。。我说医生，你别说不能吃啥，你直接说能吃啥吧，这样省事点。\n\n医生说，喝点粥吧，但不能有蔬菜，不能有肉哦。\n\n我说，行吧，喝点白粥得了，先把身体能量稳住就行。\n\n到了后面几天，食谱逐渐放开，可以吃猪肉，可以吃蔬菜，可以吃鸡蛋了。但是不能吃辣椒！\n\n转天护士长过来了，她说你这病，主要就是肛腺感染，为什么感染呢？吃辣椒啊，喝酒抽烟，过多刺激，抵抗力降低，导致了脓肿发炎。\n\nsoga soga！怪不得，这家成都区级医院一个医生一天居然排了10台肛肠手术，我老家那么同样的科室，一天一个医生平均也就3台。\n\n我老家一样有各种爱喝酒抽烟的人，作息不规律的人，成都多出来的那 7 台，主要差别就是吃辣椒吃的。\n\n原来我得的是个“四川病”。\n\n要说全国肛肠科哪家强，四川成都啊，集全国肛肠顶尖人才于此。\n\n前阵子高中同学从美国来成都玩，玩了一周，吃了一周，肚子疼了一周。美国胃实在顶不住成都这种变态辣，当然了，成都人自己不觉得自己变态辣，甚至湖南江西更加变态。\n\n换药的时候，还有一个女病人，脓肿反复发作十年了，十年前在东北，就被医生误诊了，导致这个病反复发作，到现在才做手术治疗，我看她住院没3周出不了院。\n\n没错呀，这病在别的地方，得病的人少，几率小，会看病的医生少，水平不行。还得是成都，还得是四川！\n\n来成都旅游最让人带不走的是什么？不是春熙路的美女，也不是遍地的 gay，也不是什么宽窄巷子和锦里，是成都市肛肠专科医院！\n\n少吃点辣椒吧，不能顾嘴不顾腚。我对自己说。\n\n腚是成都的第二张嘴。这让我想起“成都王”的名言警句，你的肛门比较大，但你的脓肿又弥补了这一部分。。\n\n# 腰突又犯了\n\n躺在病床上好多天，每天靠点外卖活着。外卖可以直接送进病床，只有这个时候，我会起身进膳。除此之外，我都躺着玩游戏。\n\n长时间躺着，猛一起身，头晕眼花，四肢乏力都要退化了。\n\n而且病床很硬，膈得我尾椎骨都疼，而且冬天了，保暖没做好，应该就是这个原因，我多年前的腰椎间盘突出又犯了。\n\n真是新病未愈，老病又来。一波接着一波，怎么也逃不过人生的两个大波。\n\n这两年 AI 发展挺快的，也拿到了几家公司的 offer，有不错的公司，我挺想去的，但是思前想后，最终都因身体吃不消，拒绝了。\n\n我当然想能够在 AI 上有所建树，我是对 AI 发自内心真正感兴趣，也希望有所成就。但市面上的公司几乎都需要高强度工作，什么996？什么高薪互联网？我这副小身板，真的很难说扛得住。\n\n这些天 deepseek 非常火，很多懂AI不懂 AI 的都在装逼，大谈 deepseek。我说实话我不敢评论，一个是我生病了，没空，另一个，我没看过技术报告，我不敢瞎评论。\n\n很多很多年前，就有人告诉我，身体是第一位的，是最前面的1，财富、名声、权力、玩乐，都是后面的0，没了身体，后面的统统都不作数。\n\n现在才无比深刻的明白并理解这一点，什么工作上的烦心事，道德上的绑架，以及对热爱事业的追求，都被身体生病这件事死死地卡住了。我感觉自己就像卡在烟囱管道里的一头猪，上上不去，下下不来。这就是中年人的困境吧。\n\n\n# 人生无常\n\n腰突之后，腿脚开始发麻，我立即开始锻炼身体，活动腰腿。我不想再来一场腰突微创手术。\n\n正好过年这两天，大S在日本病逝，她的死给我一种非常特别的感觉。\n\n她就是我们这一代人天天看着的明星，新闻上时不时有她的八卦和破事。而且她年纪不算老，又有钱，又有闲（我查了一下发现她已经十几年没拍戏、主持、唱歌了），这样资源丰富的明星也会突然之间人就没了。\n![](https://www.jionlp.com/blog_image/20250205/15c238f702.jpg)\n\n就是一个天天活在八卦和新闻里的人，就这么活生生就没了。人生无常。\n\n我得病之后，一直在想，我到底是因什么而感染脓肿的？我知道是吃辣椒导致的，但生病发生时，我还觉得这件事挺突然的。\n\n复工后的第一天，我才得知，公司里别的部门有个人生病去世了。好像工作这么多年，极少听说公司里有人病故，突然听到还是觉得，人生无常。\n\n这几件事连在一起，给我强烈的荒诞感。此前一直觉得，读书，成长，工作，生活本该如此，像是沿着一条确定的宽阔马路往前开车，以为未来总会如此。但人生到了某个时候，就会感觉到宽阔的马路并非一直都存在，会时不时遇到没有铺沥青的坎坷土路。\n\n道路是不平的，就像人生不知道什么时候就会走了好运，也会不知道什么时候就走了霉运。\n\n身处好运或者霉运，人往往身在其中毫无意识。人不知道自己正身处某种命运的裹挟中，只能事后才意识到。\n\n这就是生活的吊诡之处，不知道从小被教育了多少次，我要思前想后，要慎重决策每个任务。但有时候，并非充分的决策才能奏效，很多决定并非有充分的条件和信息，可以得出一个确定性的结论。反而是命运不知道把人带到何方。\n\n所以我现在并不会像过去那样，逼自己做很多事情，从前的那种执着和带有宗教般的虔诚已经不在，现在反而很多时候顺其自然。\n\n我住院期间闲着没事，一直在打王者荣耀，从钻石局直升王者 48星。原本计划重新看一遍李宏毅的强化学习视频，结果只看了一半。\n\n就这样吧，不要逼迫，人生更多需要顺其自然。\n\n\n根据以上文章，回答问题：\n文中提到“我擦屁股的时候，突然发现，纸上全是血，脓肿已经破了，裤子上滴的都是血，像是来了月经，看样子的确是大姨夫来了，我一开始并没误诊。”\n结合上下文，请问这里为什么说“我一开始并没误诊？”"}
{"score": 5, "question_type": "【闭域问答】", "correct_answer": "\n答案应当简洁、准确。避免回答太多，抓不住重点。\n能够大致总结出多个模型相互评测，利用EM算法完成，字数控制在200字即可。", "question": "LLM 距离去年 GPT 发布，火了也快有一年时间了，也有相当多的企业和高校参与到了 LLM 的研发当中，所以，对这些模型质量的评测也就成了一个重要的工作。\n\n大家也知道，我在今年5月制作过一个 JioNLP 的大语言模型评测数据集，并对若干模型进行了评测。\n\n这篇文章也是对 **JioNLP 评测工作的延续**，主要是**提出一种如何优雅地自动评测 LLM 模型质量的方法**。\n\n> 何为优雅，何为自动？容我慢慢说。\n\n## 零、当前的一些评测题集\n\n当时，市面上也有非常多的评测题集，目前我所了解到的如下（顺手薅一下别人的图）：\n![](https://www.jionlp.com/blog_image/20231026/a7aef0b2d5.jpg)\n\n其实各家方法差别不大。都是拿一些数据题集来对模型进行打分判断。当然，我也做过一些模型的评测，在工具包 jionlp 中是可以直接看到的。\n\n**评测 LLM 模型质量**这件事，说得再大白话一点，就是**给模型出一份考试题，然后给模型的回答打分。** 这件事的本质和高考、考公完全是一回事，还是数据收集和整理的范畴。\n\n当然，这是一个非常耗时耗力的人工工作，就像每年高考出题和评分判卷一样麻烦。评测 LLM 模型质量，也需要人工寻找各种各样领域的题目，然后对模型的回答结果做人工判断，（这事非得人工来干不可，毕竟，我们是在评价机器回答的质量）\n\n想要做好 LLM 模型的评测，说起来也非常简单，只要找一些 prompt 作为题目，人工评价模型的回答是否正确即可。例如以下例子：\n![](https://www.jionlp.com/blog_image/20231026/fe9dbf7e86.jpg)\n\n基于以上例子，我假设满分5分，我给上述回答3分。一方面模型的回答基本上达到了一个广告脚本的要求，但是在一些主观的独创性上有一些不足，缺少一些响亮的广告语。因此打分 3 分。\n\n不过，当题目的数量和难度变多之后，评测 LLM 还是有一些难点的：\n\n## 一、LLM 模型评测的难点\n\n### 1、模型评测严重依赖人工\n\n本身评测工作严重依赖人工，像上述的评测实例，还需要大量的prompt 提问和模型的回答，综合所有的评测例子，最终给出一个完整的分数。\n\n假设模型评测试题中包含 100 道题目，那么就需要完成 100 次人工评测。这个工作量非常大。\n\n像上面一节中，有的评测题集总共会有上万道题目，那么，相当一部分工作都要依赖人工来完成。该不会真的有人去把上万道题目全都人工去评价回答一下吧？\n\n为了解决这个人力成本太高的问题，最好的方法是，由机器来完成阅卷，最简单的方法，就是把评测题目改为选择题或者判断题。也就是如下形式：\n![](https://www.jionlp.com/blog_image/20231026/baa2427031.jpg)\n\n这样一来，打分工作就可以交给机器来完成了：**只要模型回答中出现了正确答案的字母，即可判断模型回答的正确与否。** 这就像高考中，选择题部分全都由 2B 铅笔答题，机器打分，省去了大量的人力，而且还比人工更加准。\n\n当然，这种方式也有很强的局限性：\n- 模型可能回答的是正确的，但是却包含了错误答案的字母，导致机器打分错误。例如，回答中有可能这么说：**“正确答案是B，英国。另外 A、C、D 三个则是错误的答案。”** 这样一来，程序在匹配字母时，会把所有选项都匹配上，导致阅卷错误。\n\n- **LLM 评测数据集 完全是选择题、判断题，限制了大语言模型的评测范围**。这就像高考一样，客观题可以由机器改卷，但是主观题部分，尤其是，数学推理大题、语文的作文等等，还必须得由人工，也就是老师来完成。这部分是必不可少的。\n\n总之，想要脱离开大量的人工劳动，依然是很难的。像上述的评测标准中，题目多达上万道，人工来完成，还要考虑人脑疲劳、懈怠、偷懒造成的偏差。这个偏差，随着题目数量的增多，会越来越大。\n\n### 2、主观标准\n\n由于 LLM 模型的输入输出在很多主观题上，没有什么标准答案，这就造成了模型的结果由**单独一个人来判断**，缺乏一定的权威性，例如：\n![](https://www.jionlp.com/blog_image/20231026/47f87d283c.jpg)\n\n在这个例子中，满分5分，我给这个回答打4分，但是如果换成张三，可能就会打2分，换成王五，就会打1分，因为每个人的评判标准不一样。这也造成了人工打分的不准确性。\n\n因此，最好的方法，还是找若干个人，组成一个专家系统，共同对一个问题进行打分，最终得出模型的最终结论。\n\n这实际上也和高考中，由至少两个语文老师来给作文打分，取平均分，是一样的道理。\n\n不过，更多的人工参与到评测 LLM 模型上，又会增加评测成本。\n\n### 3、难以做好评测的管理和维护\n\n前面的表格，提到了很多的评测数据集。每一家或多或少都是自己组织数据，自己评测，也就是，自己是裁判员，自己又是出题员，完全可能导致评测题目的偏颇。\n\n> 也就是说，假设比较  A、B、C 三个模型的质量高低，不同的评测数据集完全可能得出不同的结果，Mary 制作了评测数据集，得出 A 模型质量最高，Bob 制作了另外一个数据集，得出 B 模型质量最高，完全是可以人为控制的。\n\n**想要维护一个评测数据集，并且把这个评测维护成一个业内公认的标准**，是非常难的事情。\n\n原因在于，模型是随着时间不断进化的，想要探测到一个模型的真实能力，势必也要随着模型的演进而**不断更改评测题目**。否则就失去了评测的意义。\n\n对于一份完全不演进的评测题集，模型会在这份题集上不断拟合，直到逼近满分。\n\n所以，当你需要定期更新一整套多达上万道题目的评测题集，你心里是否崩溃？心里是否有许多问号？\n\n## 二、打破某些错误认识\n\n在了解了 人工评测 LLM  的局限和障碍之后，我们再来从思想上打破某些局限性的认知。\n\n### 1、评测题集数量越多越好吗？\n\n这大概是一个很明显的共识：**评测题集中的题目越多，对一个模型的评价结果也就越公正**。\n\n如果像网上一些调侃的文章那样，拿着某个模型的某一个错误结果就大肆贬低，公信力自然是很低的。所以，很多评测数据集，提供了多达几十万的体量：\n![](https://www.jionlp.com/blog_image/20231026/282a9cf8c9.jpg)\n\n\n但在做评测时，真的题目越多越好吗？就像上面说的：\n- 1、找上万道题目，本身就是一个比较麻烦的事情；而且，还需要确保这些题目定期更新；\n- 2、然后人工评测打分，耗费巨大，且人工有一定的偏颇、主观性，同时也有粗心、懈怠造成的偏差；真的，为了评测 LLM 模型质量，这么做会累死人的。\n\n\n> **事实上，做评测数据集，真的不需要那么多评测题集。**\n\n原因非常简单，我先来借鉴高考等考试来说明一下：\n\n> 以高考为例，高中三年，学生学习了大量的数以千计的知识点，但是在高考考场上，考试内容实际上非常少，可能仅仅占到学生学习**知识总量的不到十分之一**。 但是，高考**以少量的题目考察学生掌握的大量的知识能力**，实际上就是在做**样本抽样**。\n\n> 该不会有人觉得高考对学生学习能力的评价不够客观吧？？？很多人都会埋怨自己心态没调整好，发挥失常，但是很少有人会抱怨高考考试题绝大部分都是不属于自己掌握的范围。\n\n同样的道理也完全适用于 LLM 模型质量评测。\n\n好了，至此，评测数据集题量的设定，本质上就是一个概率抽样问题：\n- 根据中心极限定理（不知道的去翻《概率论》）：随着抽样样本数量的增加，整个数据集的估计分布方差很快就能降到很小。也就是，**我们压根不需要拿出几万道题来做评测，就能取得一个较稳的分布，也就是对模型较为稳定的打分。**\n\n- 相反，为了对模型的打分尽量客观，我们要做的是**使抽样的评测题目分布更加均匀**，也就是，方方面面的题目都覆盖到。所以，拿出几万道题目来，反而容易造成某些类型的题目数据聚集，影响了评测结果的准确性。\n\n### 2、黑盒就比白盒好吗？\n\n一般来说，黑盒就是把评测数据集藏起来，不让制作模型的公司机构看到。白盒就是把数据集公布出来。用大白话说，**黑盒就是闭卷考试，白盒就是开卷考试**，你可以照着书抄。\n\n![](https://www.jionlp.com/blog_image/20231026/c0e7e3afe8.jpg)\n\n\n目前来说，为了确保评测的公正性，评测数据集会直接把数据开放出来，人人都可以查看，但是这样会导致模型可以提前拿这些数据做拟合，进而取得一个较高的分数。这种是很难避免问题的。\n\n而黑盒呢？问题就是，外界不知道评测机构是怎么做的测评。由此产生的问题也非常大。你的可信度、公信力从何而来呢？目前尚不得知。\n\n当然，在理想的情况下，黑盒的评测的上限要比白盒高，因为，只让评测机构做到公平公正，要比让每一家 LLM 模型制作公司机构都公平公正要容易地多。但是，这就是最终的结果了吗？\n\n**显然不是**。\n\n黑盒的方法，使得模型没有一个统一的评判标准，完全成了一种垄断式的玩法。我们有办法克服上述这些问题。这就正式引出我今天要提出的那个问题。\n\n> 何为优雅，何为自动地评测 LLM 模型？\n\n## 三、正确的 LLM 评估方法\n\n正确的 LLM 评估方法，满足以下几个特点：\n\n- 公开，所有模型都可以探明评测的细节；\n- 公正，**所有模型都可以参与评测过程**，同时避免人的主观因素带来的问题；\n- **减少人力**，前面我们说过，评测实际上不需要那么多题目，我们需要的是题目分布足够符合**平稳均匀分布**。同时，不要耗费大量的人力来完成这件事。\n- **灵活变动**，避免白盒，也就是开卷考试带来的竞争。实际上，减少了评测人力，也就可以把精力放在定期更新题目，获得更加公正结果上面。\n\n### 1、具体实施方式\n\n其实非常简单，所谓自动评测，避免大量人力，那就是**把打分这项工作，交给模型**。举个例子来说明：\n\n像上面的例子中，我们首先把结果交给 A 模型来生成结果：\n![](https://www.jionlp.com/blog_image/20231026/542f2da1d7.jpg)\n\n然后，我们把这个结果，重新组织，交给 B 模型来打分，判断 A 模型的结果是否正确。也就是，A 模型是考生， B 模型是阅卷老师。当然，此时需要设计一个 prompt，来诱导 B 模型给出一个标准打分：\n\n```\n我将给你一个问题和一个对应的答案，这是一个答题者回答的，请对这个答题者的回答正确与否，与回答质量给出打分。\n\n问题：{question}\n\n标准答案：{correct_answer}\n\n答案：{response}\n\n以上是所有的问题和答案，请给该答题者的回答打分，满分 {score} 分：\n```\n\n由此，等待模型给出打分分数即可，就像下面这张图这样简单。我试了市面上常见的若干模型，大多数都能给理解题意，完成打分这项任务。（如果说一个模型都无法回答这个 prompt，那就，自己动手弄吧）\n![](https://www.jionlp.com/blog_image/20231026/d69f3ab465.jpg)\n\n\n好了，我们由此完成了一次 LLM 模型之间相互打分的例子。除了 B 给 A 打分外，A 也可以给 B 打分。\n\n### 2、完整评测流程\n\n有了上面的具体操作方式，就可以愉快的开启整个自动化评测流程了。为了方便，我就不写太标准的公式了，尽量以文字叙述。\n\n- **step1**：现在，假设我们要参与评测的模型包括 A,B,C,D,...Z 。准备好这些模型的 api，免得我们还需要手工在网页上进行打分。\n\n- **step2**：这么多个模型，我们首先把所有的评测题目，交给所有的模型 API 进行问题回答。**得到所有模型对所有问题的回答**。\n\n- **step3**：依照上一小节中，各个模型相互打分的方式，让 A 模型给 B,C,D,...,Z 模型的每道题打分，让B 模型给 A,C,D,...,Z 模型的每道题打分，让 Z 模型给 A,B,C,...,Y 模型打分。\n\n好了，这样我们就得到了，**每个模型，给所有其它模型的每道题的回答的打分**。这是一个大的张量。\n\n- **step4**：关键一步，利用 EM 算法来进行拟合回归。\n\n首先，我默认大家熟悉 EM 算法了，这是一种在参数优化目标不清晰的情况下的一种优化方法。\n\n其次，我们又知道，从第三步中，得到的所有打分结果，其实是不准确的。主要有以下几点：\n\n- 如果一个模型质量高，能力强，那么，它对其它模型的结果打分，就更加准确、可信，而且，打分也更稳定；反之，一个垃圾模型对其它模型的打分可能就和真实结果偏差很大。\n- A 模型最终对每个回答的打分，是由B,C,D,...,Z 模型共同决定的。可以由其它模型的打分加权得到。也就是，B,C,D,...,Z 共同承担了阅卷人的角色。\n\n- 一个垃圾模型，可能会对真实结果产生很大的偏差。因此，EM 算法优化目标，是为了使垃圾模型的打分权重尽量小，使一个优秀模型的打分权重尽量大。（比如，在现阶段，完全可以让 GPT4 来给其它所有模型的回答打分，直接作为标准分数，也未尝不可）。比如，B 模型质量最高，那么B 模型在和 C, ... ,Z 模型共同决定 A 模型回答质量时，占据的权重越高。\n\n- 而每一个模型是否靠谱，也就是其权重，实际上是由其本身的分数决定的，也就是我们最终想要的结果——每个模型的评测分数。\n- 在这里有一个假设：优秀的模型，打分结果更加准确、稳定，贴近真实的平均分，而垃圾的模型，则会更大概率偏离平均分更远。\n\n由此，我们就获得了一组隐变量，以及一组求解目标：\n- 隐变量是：每个模型的回答的真实得分，以及每个模型回答稳定性的衡量指标——方差；\n- 求解目标：每个模型的最终分数（也就是你看到的很多评测集展示出来的分数），也即每道题得了多少分，所占打分比例的权重（注意，最终分数和打分比例之间应该是由一个单调函数建立联系）\n\n- 当然，这里有一个特殊情况，**如果评测集有标准答案时，那么评价隐变量就被省去了，而如果对于一些主观的题目，如作文，没有标准答案，那么就需要测试隐变量。**\n\n好了，这样就可以利用 EM 算法愉快的求解了！！！！反复迭代，直到收敛到一个不错的结果。\n\n### 3、让我们来看看这里的实施成本：\n\n- 再也不用人工评判了！！！开心！！！让模型们之间互相改卷，我们来做统计。我可以拿这些打分改卷的时间看会小说电视剧，打游戏！(●'◡'●)！\n- 需要定期更新评测题目，确保模型没有提前拿考试题训练模型。由于论证了评测题目量级的考量，更新的题目数量甚至不需要很多。\n- 需要获取模型的 API。不然，我们还得手动在网页上输入问题，让模型打分，怪麻烦的。\n\n### 4、可能存在的一些问题\n\n当然，有一种可能，就是，在评测的大量模型中，质量差的占多数，也就是说，好比一个班里一大半都是学习成绩很差的，全都是这样的差学生参与到考试改卷，那岂不是要误人子弟了！？\n\n进一步地，在 EM 算法的收敛中，这些模型由于分布差异太大，导致算法迟迟不收敛，那就需要做出一些改进了。\n\n因此，在这种情况下，有两种方式进行改进。\n\n- 1、增加一次人工评测，人工打分。不需要多个人组成专家系统。而是一个人和多个模型组成专家系统，让人的打分占比较高一些，然后进行拟合。甚至，人工都不需要每道题都打分，而只需要对其中一些题目打分即可。\n\n- 2、对于那些打分分布方差太大的模型，直接把这些模型踢出评测范围，也就是不让差生参与打分。\n\n## 四、总结与愿望\n\n好啦，到此为止，算法阐述完毕！！！说几点愿望。\n\n### 1、希望近期把代码写出来，纯 python 的，开发压力会小一些。感兴趣参与的，可以加入到 jionlp 工具包的开发，并推广开来。\n\n### 2、希望能够拿到想评测的厂商的 API，越多越好，我来测试。\n\n所以，这是一个征召帖子，希望各个厂家，想参与 JioNLP 数据集评测的，能够给我开放一个 API，参与 JioNLP 数据集评测。\n\n\n\n请根据以上文章，把LLM评测方法用200字以内简要总结提炼。"}
{"score": 5, "question_type": "【数学问答】", "correct_answer": "LayerNorm主要是在token的hidden维度上进行的", "question": "这一篇文章主要讲讲 Layer Normalization。在本文里，Layer Normalization 统一都被称为 layernorm。字面意思就是层归一化，也属于数据分布归一化的一种。\n\n在神经网络训练里，把数据分布都做一个归一化，好处多多，可以使训练更充分，更快速，可以克服Internal Covariate Shift 问题。这个问题是需要单独摊开来讲的。这一节主要是讲一下 layernorm 是如何实施的，以及其本质。\n\n# Layernorm 的应用场景\nLayernorm 主要应用在自然语言处理领域，里面的各类神经网络模型 Bert、GPT 等等都是建立在 token 上的。也就是下图这样，每一个 token 都对应一个 embedding。\n![](https://www.jionlp.com/blog_image/20240929/3696ba8684.jpg)\n\nEmbedding 是一个向量，每个 token 之间的关系都使用 embedding 向量来表示。最主要使用余弦相似度。举例两个 token embedding 向量，都是4维，他们之间的余弦相似度是：\n```\nimport numpy as np\n\ntoken_embedding_1 = np.array([-5.8260e-01,  4.2750e-01, -1.8083e+00,  1.9335e+00])\ntoken_embedding_2 = np.array([-4.4540e-01,  2.6184e-01,  1.8354e+00,  8.1645e-01])\n\n# 计算点积\ndot_product = np.dot(token_embedding_1, token_embedding_2)\n\n# 计算向量的范数\nnorm_1 = np.linalg.norm(token_embedding_1)\nnorm_2 = np.linalg.norm(token_embedding_2)\n\n# 计算余弦相似度\ncosine_similarity = dot_product / (norm_1 * norm_2)\n\nprint(\"Cosine Similarity:\", cosine_similarity)\n```\n上面代码里的两个向量的相似度是 `-0.24050189427673058`。\n\n当然，上面的两个向量完全是随机编的，我们希望在计算两个向量相似度的时候，能够都归一化到一个统一的分布上，这样方便计算，同时确保了相似度不变。\n\n# Layernorm 的符号解释\n先说明一下 layernorm 中的符号含义。首先，layernorm 一般应用在自然语言处理领域，一般情况下，都包含三个维度：\n- B（Batch）也就是输入的并行的句子的数量；\n- T（Time/Token）指的是一个句子中有多少个 token，它是有前后依赖关系的，因此 T 的含义既可以指代时间先后关系，又可以指代 Token，总之它代表了 Token 数量的维度；\n- D（Hidden Dimension）指每一个 token 的向量表示的维度，比如，我用 768 维向量代表一个 token。\n\n注意：上述字母除了表明它所代表的维度之外，还在下面的公式中表示各个维度的总数量。\n\n此外，还有两个索引值，一个是 $i$，另一个是 $j$，会在公式里出现，为啥是两个索引呢？明明输入的数据维度是一个 B×T×D 三维的啊。这是因为 layernorm 操作仅仅作用在 D 这一维上，也就是只针对每一个 token 内部向量的各个维度上。至于 token 是在哪一个 Batch 上，在哪一个时间位置 Time 上，其实无所谓，因为都是重复的操作。\n\n因此，\n- $i$ 表示的是 Batch × Time 两个维度的索引值。它有可能是第 1 个 Batch 的第 20 个时间片的 token，也有可能是第 15 个 Batch 的第 12 个时间片的 token，总归，不影响，因此我们统一使用 $i$表示；\n- $j$ 表示的是 D 维度的索引值，也就是，某一个 token 对应向量的第$j$个维度。\n\n这一点非常重要，否则公式你看不懂。\n\n# Layernorm 的前向计算过程\n我们一边讲公式，一边举个例子来说一下这个操作过程。首先我定义了一组 B×T×D 的向量x，其中最里层的4个值就是某一个 token 的向量，2 个 batch，每个里面有 3 个 token。\n\n```\nx = [[[ 3.5532e-01, -6.0532e-01, -3.5909e-02,  1.1443e+00],\n      [-5.8260e-01,  4.2750e-01, -1.8083e+00,  1.9335e+00],\n      [-4.4540e-01,  2.6184e-01,  1.8354e+00,  8.1645e-01]],\n     [[ 7.9384e-01, -7.1541e-01,  7.2961e-01, -1.2622e-03],\n      [-1.5936e+00, -9.4022e-01, -1.2189e-01,  5.4420e-01],\n      [ 2.0098e-01,  3.2537e-01,  1.9155e-01,  9.7159e-02]]]\n```\n如果把这组向量画成点图，就是下面这样。其中纵轴仅仅表示token的 id，纵轴4没有对应任何点，它只是表示两组 batch 的间隔。横轴表示每一组 token embedding的具体取值，每一横行4个点表示4维的embedding。相同的颜色表示相同的embedding 位置索引，比如蓝色点都是第一维的。\n![](https://www.jionlp.com/blog_image/20240929/ff7d1d5e97.jpg)\n\n可以看出，有些embedding 中的点非常集中，有些就很分散，分布是不固定的，这就是 layernorm 的意义。\n\nlayernorm 的操作过程是这样的，首先计算每一个 token 向量中所有元素的均值 $\\mu_i$，这样的均值，总共有 B×T 个，由于消除了 D 这一维度，因此下标索引也就没有 $j$了。\n$$\\mu_i = \\frac{1}{D} \\sum_{j}^{D}X_{i,j}$$\n然后计算方差和标准差$\\sigma_i$，由于这个数值需要后续放到分母上做处理，所以需要加一个微小的常数，防止出现分母为 0 。\n$$\\sigma_i = \\sqrt{\\frac{1}{D}\\sum_{j}^{D}{(X_{i,j} - \\mu_i)}^2+\\epsilon}$$\n其实可以看出，这些操作都是初中数学统计单元的简单知识，放在这里之所以复杂，是因为 Tensor 的维度多了，字母标记复杂了，如果不仔细看，很容易头脑一锅浆糊。\n把每一个 token 对应的向量分布归一化在标准正态分布上面。\n$$\\hat{X_{i,j}} = \\frac{X_{i,j}-\\mu_i}{\\sigma_i}$$\n得到一个标准正态分布，均值为 0。\n那么我们把上述的操作应用在前面的 x 上，就得到 x_hat：\n```\nx_hat = [[[ 0.2212, -1.2887, -0.3937,  1.4613],\n          [-0.4192,  0.3170, -1.3125,  1.4146],\n          [-1.2747, -0.4262,  1.4616,  0.2392]],\n         [[ 0.9632, -1.4919,  0.8588, -0.3302],\n          [-1.3168, -0.5095,  0.5016,  1.3247],\n          [-0.0344,  1.4995, -0.1506, -1.3144]]]\n```\n画成图就是下面这样，图中的点不像之前那么杂乱了，所有的的点都在相同的分布上。\n![](https://www.jionlp.com/blog_image/20240929/ff15dc8bd6.jpg) \n\n我把所有颜色相同的点连线，方便后续的观察。\n![](https://www.jionlp.com/blog_image/20240929/7d0a2e9756.jpg)\n\n其实这样单纯归一化的操作并不便于梯度下降，因为数据分布被强行改变了。就像两个 token 的 embedding 之间的相似度，在归一化前和归一化后是不一样的。这就有点说不过去了，我们希望能够尽量保持一致性。\n\n因此还需要在这个数据上做进一步操作，引入了参数。\n$$Y_{i,j} = \\omega_j\\hat{X_{i,j}} + \\beta_j$$\n注意，这里引入了两组参数$\\omega_j$和$\\beta_j$，下标表明，它的维度和每一个 token 的隐藏维度是一样的。也就是说，整个待处理数据的所有 token，在归一化成正态分布之后，所有 token 的同一维数据经过了一个相同的偏移和伸缩，其实就是经过了一个线性变换。变换后的数据分布就成了：\n```\ny = [[[ 0.6555, -0.0599,  0.5203, -0.4047],\n      [ 0.2530, -2.3120,  1.7711, -0.3820],\n      [-0.2847, -1.2696, -2.0056,  0.1893]],\n     [[ 1.1219,  0.2250, -1.1848,  0.4660],\n      [-0.3112, -1.1528, -0.6986, -0.3383],\n      [ 0.4949, -3.9703,  0.1894,  0.9445]]]\n```\n![](https://www.jionlp.com/blog_image/20240929/de5c224811.jpg)\n\n观察一下x_hat 和 y 可以发现，每一种颜色的点的相互折线趋势是成比例的保持不变的。这就是d$\\omega_j$和$\\beta_j$的作用。\n\n当然，当一个AI模型训练好之后，参数的分布不会像上面的例子这样这么夸张，毕竟我计算的都是随机数，只是为了说明情况而已。\n\n# Layernorm 的本质\n前面我说了一下 layernorm 的应用范围，主要是在自然语言处理领域做 token embedding 的。它是把每一个 batch 里的所有 token 的分布都归一化在同一水平上。\n\n做个比喻，军队训练里有一种队列训练，就是一排士兵，齐步向前走。\n![](https://www.jionlp.com/blog_image/20240929/b05620553c.jpg)\n\n但是人毕竟不是严格执行的机器，走出一段距离后，原本一横行对齐的士兵总是会走出参差不齐，除非是经过长期训练的阅兵方阵。所以，每走出一段距离，军官都会下令：“立定，向右看齐。”这样每个人都会重新标齐排面，保持队形齐整。\n![](https://www.jionlp.com/blog_image/20240929/5da7a99506.jpg)\n\n这个过程和 layernorm 很像，相似之处在于：\n- 每一个士兵都对应一个 token，士兵组成一排，一条 token 序列对应的就是一条句子；\n- 一排士兵，往前走一段距离就会产生 参差不齐，一条 token 序列也会在经过神经网络的某些层之后产生参差；\n- “向右看齐”的口令作用，类似于 layernorm 中把每个 token 都归一化为相同分布。\n\nOK，注意，layernorm 本身是把一行序列中所有的 token 做归一化的，因此，本质上就是在 token 之间操作的。\n\n# Layernorm 的反向传播过程\n\n反向传播过程，是一个递进分层计算过程。神经网络一般都是有非常多层的。运用梯度下降法计算反向传播，也是分层进行的，恰好对应了复合函数的求导过程。\nLayernorm 就是神经网络中的其中一层，这一层的输入，是其它层（比如注意力机制，全连接网络等等）得到的数据，这一层的输出，也是接下来其他层（比如下一层注意力机制计算，全连接网络等等）的输入。\n因此，layernorm 的反向传播也就需要定义清楚输入和输出。它的输入是从输出根据损失函数$L$得来的。也就是，$\\frac{\\partial L}{\\partial Y_{i,j}}$是一组已知量。\n\n对于所有的参数都要计算梯度，参数包括 $\\omega_j$和 $\\beta_j$，以及要为 layernorm 的前一层做好输入准备。\n先计算$\\beta_j$的梯度，可以看到，其梯度值就是一组已知量在 $i$维度上的求和。\n$$\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i}^{B \\times T}{\\frac{\\partial L}{\\partial Y_{i,j}}} \\frac{\\partial Y_{i,j}}{\\partial \\beta_j} = \\sum_{i}^{B \\times T}{\\frac{\\partial L}{\\partial Y_{i,j}}}$$\n对于参数$\\omega_j$的梯度，可以看到，是已知量和归一化之后的数据 $\\hat{X_{i,j}}$的乘积求和。\n$$\\frac{\\partial L}{\\partial \\omega_j} = \\sum_{i}^{B \\times T}{\\frac{\\partial L}{\\partial Y_{i,j}}} \\frac{\\partial Y_{i,j}}{\\partial \\omega_j} =  \\sum_{i}^{B \\times T}{\\frac{\\partial L}{\\partial Y_{i,j}}} \\hat{X_{i,j}}$$\n以上两组梯度相对容易计算，但是还需要计算针对输入 $X_{i,j}$的梯度。这一组梯度相对复杂，因为，均值和标准差都是依赖$X_{i,j}$计算得到的，因此就需要计算均值$\\mu_i$和标准差$\\sigma_i$对于$X_{i,j}$的梯度。然后再做复合函数求导，把结果粘起来。\n\n$$\\begin{align}\n\\frac{\\partial L}{\\partial \\sigma_i} &= \\sum_{j}^{D} \\frac{\\partial L}{\\partial Y_{i,j}} \\frac{\\partial Y_{i,j}}{\\partial \\sigma_i} \\notag \\\\ \n&= - \\sum_{j}^{D} \\frac{\\partial L}{\\partial Y_{i,j}} [\\omega_j(X_{i,j}-\\mu_i)]\\frac{1}{\\sigma_i^2}  \\notag \\\\\n&= - \\sum_{j}^{D} \\frac{\\partial L}{\\partial Y_{i,j}} \\frac{\\omega_j}{\\sigma_i}\\hat{X_{i,j}} \\notag\n\\end{align}$$\n\n然后再求取$\\sigma_i$对均值 $\\mu_i$的梯度：\n\n$$\\begin{align}\n\\frac{\\partial \\sigma_i}{\\partial \\mu_i} &= \\frac{\\partial [\\frac{1}{D}\\sum_{j}^{D}(X_{i,j}-\\mu_i)^2 +\\epsilon]^{\\frac{1}{2}}}{\\partial \\mu_i} \\notag \\\\\n&= \\frac{\\sigma_i^2}{2} \\frac{1}{\\sigma_i} \\frac{\\partial [\\frac{1}{D}\\sum_{j}^{D}(X_{i,j}-\\mu_i)^2 +\\epsilon]}{\\partial \\mu_i} \\notag \\\\\n&= - \\frac{\\sigma_i}{2} [\\frac{1}{D}\\sum_{j}^{D} 2(X_{i,j} - \\mu_i )] \\notag \\\\\n&= - \\frac{\\sigma_i}{D} \\sum_{j}^{D}(X_{i,j}-\\mu_i) = 0 \\notag\n\\end{align}$$\n\n注意这里，均值 $\\mu_i$实际上也是依赖$X_{i,j} $算出来的，因此这个值是 0。\n\n$$\\begin{align}\n\\frac{\\partial Y_{i,j}}{\\partial \\mu_i} &= \\frac{\\partial \\omega_j \\hat{X_{i,j}}}{\\partial \\mu_i} + \\frac{\\partial \\beta_j}{\\partial \\mu_i} \\notag \\\\\n&= \\omega_jX_{i,j} \\frac{\\partial \\frac{1}{\\sigma_i}}{\\partial \\mu_i} -  \\omega_j\\frac{\\partial \\frac{\\mu_i}{\\sigma_i}}{\\partial \\mu_i} + 0\\notag \\\\\n&= \\omega_jX_{i,j} [- \\frac{1}{\\sigma_i^2} \\frac{\\partial \\sigma_i}{\\partial \\mu_i}] - \\omega_j \\frac{\\sigma_i - \\mu_i \\frac{\\partial \\sigma_i}{\\partial \\mu_i}}{\\sigma_i^2} \\notag \\\\\n&= 0 - \\frac{\\omega_j}{\\sigma_i} \\notag\n\\end{align}$$\n\n进而可以得到：\n\n$$\\begin{align}\n\\frac{\\partial L}{\\partial \\mu_i} &= \\sum_{j}^D \\frac{\\partial L}{\\partial Y_{i,j}} \\frac{\\partial Y_{i,j}}{\\partial \\mu_i} = - \\sum_{j}^D \\frac{\\partial L}{\\partial Y_{i,j}} \\frac{\\omega_j}{\\sigma_i} \\notag\n\\end{align}$$\n\n最后，可以计算对输入$X_{i,j}$的梯度，这里使用了多元函数求导法则：\n\n$$\\begin{align}\n\\frac{\\partial L}{\\partial X_{i,j}} \n&= \\frac{\\partial L}{\\partial \\hat{X_{i,j}}} \\frac{\\partial \\hat{X_{i,j}}}{\\partial X_{i,j}} + \\frac{\\partial L}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial X_{i,j}} + \\frac{\\partial L}{\\partial \\sigma_i} \\frac{\\partial \\sigma_i}{\\partial X_{i,j}} \\notag \\\\\n&= \\frac{\\partial L}{\\partial Y_{i,j}} \\frac{\\partial Y_{i,j}}{\\partial \\hat{X_{i,j}}} \\frac{\\partial \\hat{X_{i,j}}}{\\partial X_{i,j}} + \\frac{\\partial L}{\\partial \\mu_i} \\frac{1}{D} + \\frac{\\partial L}{\\partial \\sigma_i} \\frac{1}{2 \\sigma_i} \\frac{2}{D}(X_{i,j}-\\mu_i) \\notag \\\\\n&= \\frac{\\partial L}{\\partial Y_{i,j}} \\omega_j \\frac{1}{\\sigma_i} - \\sum_{j}^D \\frac{\\partial L}{\\partial Y_{i,j}} \\frac{\\omega_j}{\\sigma_i} \\frac{1}{D} - \\frac{1}{ \\sigma_i} \\frac{1}{D}(X_{i,j}-\\mu_i) \\sum_{j}^{D} \\frac{\\partial L}{\\partial Y_{i,j}} \\frac{\\omega_j}{\\sigma_i}\\hat{X_{i,j}} \\notag \\\\\n&= \\frac{1}{\\sigma_i}[\\frac{\\partial L}{\\partial Y_{i,j}} \\omega_j - \\frac{1}{D} \\sum_{j}^D \\frac{\\partial L}{\\partial Y_{i,j}} \\omega_j  - \\frac{1}{D}\\hat{X_{i,j}} \\sum_{j}^{D} \\frac{\\partial L}{\\partial Y_{i,j}} \\omega_j\\hat{X_{i,j}}]\n\\end{align}$$\n\n完结了。\n\n附一下代码：\n```python\nimport torch\neps = 1e-5\n\nclass LayerNorm:\n\n    @staticmethod\n    def forward(x, w, b):\n        # x is the input activations, of shape B,T,C\n        # w are the weights, of shape C\n        # b are the biases, of shape C\n        B, T, C = x.size()\n        # calculate the mean\n        mean = x.sum(-1, keepdim=True) / C # B,T,1\n        # calculate the variance\n        xshift = x - mean # B,T,C\n        var = (xshift**2).sum(-1, keepdim=True) / C # B,T,1\n        # calculate the inverse standard deviation: **0.5 is sqrt, **-0.5 is 1/sqrt\n        rstd = (var + eps) ** -0.5 # B,T,1\n        # normalize the input activations\n        norm = xshift * rstd # B,T,C\n        # scale and shift the normalized activations at the end\n        out = norm * w + b # B,T,C\n\n        # return the output and the cache, of variables needed later during the backward pass\n        cache = (x, w, mean, rstd)\n        return out, cache\n        \n    @staticmethod\n    def backward(dout, cache):\n        x, w, mean, rstd = cache\n        # recompute the norm (save memory at the cost of compute)\n        norm = (x - mean) * rstd\n        # gradients for weights, bias\n        db = dout.sum((0, 1))\n        dw = (dout * norm).sum((0, 1))\n        # gradients for input\n        dnorm = dout * w\n        dx = dnorm - dnorm.mean(-1, keepdim=True) - norm * (dnorm * norm).mean(-1, keepdim=True)\n        dx *= rstd\n        return dx, dw, db\n\nB = 2 # some toy numbers here\nT = 3\nC = 4\nx = torch.randn(B, T, C, requires_grad=True)\nw = torch.randn(C, requires_grad=True)\nb = torch.randn(C, requires_grad=True)\nout, cache = LayerNorm.forward(x, w, b)\n\ndout = torch.randn(B, T, C)\nfakeloss = (out * dout).sum()\nfakeloss.backward()\n\ndx, dw, db = LayerNorm.backward(dout, cache)\nprint(\"dx error:\", (x.grad - dx).abs().max().item())\nprint(\"dw error:\", (w.grad - dw).abs().max().item())\nprint(\"db error:\", (b.grad - db).abs().max().item())\n```\n\n参考：\n- [layernorm - llmc](https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md)\n\n\n根据以上文章回答，layernorm的神经网络层操作，具体是在哪个维度上进行的？"}
{"score": 5, "question_type": "【数学问题】", "correct_answer": "logistic 回归的名字其实来源于logistic函数，它是一组函数，能够将任意取值范围的实数，映射到0~1之间，单调递增。\n\n参数的权重本质上表现了该参数的重要性，一个稳定的模型不应该让任何单独的参数占据重要的作用。", "question": "## 前言\n\n想要讨论清楚NLP和神经网络，实在绕不开**逻辑回归（Logistic Regression）**，因为这个模型实在太过经典，又是其它算法和模型的基础，所以需要花时间将该算法详详细细、里里外外讲清楚。\n\n本文的行文叙述将会尽量减少抽象、空洞的大段公式，目标是要**让非机器学习从业人员也能明白该算法的工作原理**，同时**让已经了解熟悉该算法的朋友也能返回头来，再次咀嚼出新的滋味**。\n\n虽然标题很古板，但是这里立一个 **flag**，我相信这是你看到的最深入浅出的一篇逻辑回归教程，比知乎上的文章都要简明清晰。\n\n## 从选女朋友说起\n\n老友记第二季里有一集，Ross 事先找了 Julie 做女朋友，然后回头发现 Rachel 也对自己心有所属，然后他就 emo 了，不知道该选谁做女票。这时 Chandler 给他建议，既然不知道该如何选，那么就两人打PK，列一个表格，分别列出来两个女人的优缺点，逐一比较，然后 all the work is to do the math。\n![老友记 Julie 和 Rachel](https://www.jionlp.com/blog_image/20220908/c1e9e617eb.jpg)\n\n然后 Chandler 就列了几个项目，大概就是**颜值、工作、性格**。\n\nRoss 就开始比较了，每一项满分1分，最低分 0 分。**颜值**方面，Rachel 当然是非常漂亮，不过 Julie 也并不差多少，打 0.7 分；**工作**上，Julie 是古生物学家，高校工作，非常体面，还有共同语言，而 Rachel 则只是一个咖啡厅端咖啡的服务员，还经常把咖啡端错，打 0.4 分吧；**性格**上，两个人脾气都不错，只不过 Rachel 稍微有点大小姐脾气，骄纵了一些，打 0.8 分吧。可以归结为如下表格：\n![image](https://www.jionlp.com/blog_image/20220908/7e709ea37c.jpg)\n\n最后可以计算得到一个总分，Julie 是 2.7 分，Rachel 是 2.2 分。从直观上来讲，Julie 胜出，Ross 应该选择 Julie 了。\n\n> 但是有的人就讲了，剧情里 Ross 最终还是选择了 Rachel，毕竟她比较漂亮，Ross 这种男人一看见就女孩漂亮就失去了大脑，至于工作、性格、家庭之类的，一概就不关心了。\n\n这里就涉及到了**权重**的概念，上述计算方法中，我们将 **颜值、工作、性格** 三项看得同等重要。但是对于 Ross 这种男人，**颜值**最重要，其它的重要性很低。那么又可以有如下计算结果：\n![image](https://www.jionlp.com/blog_image/20220908/b64172a1c8.jpg)\n\n结果，Rachel 得分为\n$$1.0 \\times 1.0 + 0.4 \\times 0.1 + 0.8 \\times 0.1 = 1.12$$\n\n反超了 Julie 的得分\n$$0.7 \\times 1.0 + 1.0 \\times 0.1 + 0.9 \\times 0.1 = 0.9$$\n\nRoss 这时可以选择 Rachel 了。\n\n> 虽然为两个女孩打了分，相对来讲，Rachel 要比 Julie 好，但是依然有一个问题存在，那就是，**这两个人之间到底存在多大的差距**？\n\nRachel 的得分 1.12，Julie的得分 0.9，是否可以说在 Ross 眼里，Rachel 比 Julie 强 $\\frac{1.12-0.9}{0.9} \\approx 24.4\\%$？\n\n也许，在 Ross 的眼里，Rachel 永远是 Rachel，是其她任何人都替代不了的，她比 Julie 要强很多很多倍，这不是颜值所能决定的，也不是任何其它可以具象的因素决定的，而是由某种她自身的特质决定的，这就是该死的说不清的爱情。\n\n我们可以为两个女孩各加一个固定的**偏置值**，比如 $-0.85$，那么Rachel 的最终得分是\n$$1.12 - 0.85 = 0.27$$\n\nJulie 的最终得分是\n$$0.9 - 0.85 = 0.05$$\n\n大概，从这个结果来看，我们可以隐约感觉到，Rachel 在 Ross 心里，是比 Julie 要好很多很多的。这一个偏置值，使得 Rachel 和 Julie 之间的差距拉大了。\n\n> 但是我们可以讲，Rachel 比 Julie 强 $\\frac{0.27-0.05}{0.05}=4.4$倍吗？\n\nRoss 挑选女朋友的时候，只设置了**颜值、工作、性格**三个选项。可事实上，挑选女朋友的选项条件可是太多了，**学历、家庭情况、经济状况、有无吸烟史、有无犯罪史、有无偷盗、祖上是否贵族、八字是否不合、是否秃头、做饭是否放太多盐、是否话太多太爱唠叨**等等，这种选项条件甚至可以列出来一万条。如果这样计算的话，两个人的计算取值可能随着选项的增多而越来越大，到最后，可能 Rachel 得分 $10020$，Julie 得分 $9800$。\n\n也就是说，两人的得分值是没有一个固定区间的，大概就是 $(-\\infty, \\infty)$ 这么大的范围。这样的情况，到底要如何比较两者之间的差距呢？\n\n其实我们应该可以想到，应当将两个人的比较值放在同一个范围内来计算，比如，就像为每一个单项设定满分1分、最低0分一样，将总分也局限在 $(0,1)$ 分之间。\n\n## sigmoid 函数\n\n由上可知，我们需要设计一个函数，它的输入就是一个上述的计算得分值，这个值可正可负，例如，Julie 有犯罪记录，则可以给她记录 $-1.0$ 分，取值范围也就是 $(-\\infty, \\infty )$，而输出则需要限制在 $(0,1)$ 之间。这个范围，很明显，是可以当作一个概率值看待的，因为概率值就介于 $(0,1)$ 之间。\n\n当然了，我们设计函数时也不是胡乱设计，应当满足**既简单、又好用**的原则。\n\n- 1、应当是单调函数\n- 2、应当是初等函数，方便求导(可梯度下降)\n- 3、最好形式上满足一些对称性\n\n恰好，此时就有一个这样的函数，名叫 sigmoid。其表达式为：\n$$ f(x)=\\frac{1}{1+e^{-x}} $$\n![sigmoid 函数](https://www.jionlp.com/blog_image/20220908/f74d7b6198.jpg)\n\n这个函数图像看着就很对称，也很优雅，其导数为\n$$\\frac{df(x)}{dx}=\\frac{e^x(1+e^x)-e^xe^x}{(1+e^x)^2}=\\frac{e^x}{(1+e^x)^2}=f(x)(1-f(x))$$\n\n换句话说，**sigmoid函数的导数值，仅仅依靠该函数值本身的四则运算就可以计算得到**，计算可谓是非常简便。\n\n> 逻辑回归名字由来：\n> 逻辑回归的英文名字是 Logistic Regression，这里的**逻辑**实际上只是一个音译。更准确的名称应该是**逻辑斯蒂回归**。所谓 Logistic，指的是以下函数族\n> $$f(x)=\\frac{L}{1+e^{-k(x-x_0)}}$$\n> 很明显，它是 sigmoid 函数的一般化式子，sigmoid 函数是 Logistic 函数的一个特例。所以，逻辑回归改名为 **sigmoid回归**也未尝不可。\n\n当然，其实只要是满足**单调、可导**的函数都是完全可以的，这样的函数有一大把：\n![image](https://www.jionlp.com/blog_image/20220908/f1000c949b.jpg)\n\n只不过，sigmoid 函数是最简单的一个，求导也是目前发现的最简单的一个。如上图中的函数，大多形式复杂丑陋，难于记忆，最关键的是，求导也不方便，因此也就被人们放弃了。\n\n接着用 sigmoid 函数计算 Ross 的两个女朋友候选人，Julie 的最终取值\n$$sigmoid(0.9-0.85)=\\frac{1}{1+e^{-(0.9-0.85)}}=0.512$$\n\nRachel 的最终取值：\n$$sigmoid(1.12-0.85)=\\frac{1}{1+e^{-(1.12-0.85)}}=0.567$$\n\n由此，我们可以说，根据上述的打分，Rachel 比 Julie 好 $\\frac{0.567-0.512}{0.512}=2.79\\%$。调节偏置值，我们可以得到不同的 Rachel 比 Julie 好的程度值。\n\n> 同时也可以认为，这个取值是一个概率值，Rachel 适合做女朋友的概率要大于Julie。\n\n## 逻辑回归模型的抽象\n\n所谓逻辑回归模型，就是给定一些样本数据作为输入，为这些样本分配恰当的输出。例如，根据之前一年的经济数据预测第二年的经济是增长还是衰退；根据果树林中的春夏病虫害数量推测秋天是丰收还是歉收。这些都可以利用逻辑回归模型完成。\n\n同样的，根据Ross 的打分，来判断女孩是否适合做女朋友，也是其中一种应用。\n\n从上述选择女朋友的例子可以看出，我们可以将**颜值、工作、性格**等因素看作一个向量 $x$，而最终的输出的是否适合做女朋友的值为 $y=f(x)$，权重值是$w$，偏置值为 $b$， 可以得到如下建模公式：\n$$y=f(x)=sigmoid(w^Tx+b)=\\frac{1}{1+e^{-(w^Tx+b)}}=\\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + ... + w_nx_n +b)}}$$\n\n上述 Julie 可以看作是一个输入样本1， Rachel 是输入样本2。我们是在样本1和样本2之间做比较。\n\n然而在一般逻辑回归模型中，我们往往只针对一个样本本身做**是、否**的决策，而并不将多个样本之间做比较，也就是，逻辑回归一般意义上只做**二分类**任务。这不难办到，可以将问题转化为：\n\n> 输入：样本 Julie，输出：不适合做女朋友\n\n> 输入：样本 Rachel，输出：适合做女朋友\n\n这样就有了两个独立的样本，样本的类型总共就两个：**适合，不适合**。根据之前的计算结果，Rachel 适合做女朋友的概率是 0.567，Julie 的概率是0.512，而最终真正的输出只有两个值，**适合、不适合**，这是一个二分类问题。\n\n所以，还应当有一个函数，它的输入是 $(0,1)$ 之间的实数，也就是上述所谓的概率值，输出只有两个值，按照方便，可以规定输出只有 0 和 1 两个值。。可做如下规定：\n$$p(Rachel=1)=p(Rachel适合做女朋友)=sigmoid(Rachel)$$\n$$p(Rachel=0)=p(Rachel不适合做女朋友)=1-sigmoid(Rachel)$$\n\n一般形式，我们规定$y$为最终的二分类标签，正样本为1，负样本为0，$x$为输入的样本数据，则\n$$p(y=1|x)=sigmoid(x)$$\n$$p(y=0|x)=1-sigmoid(x)$$\n\n接下来，我们就可以比对 Rachel 适合做女朋友的概率 0.567，和她不适合的概率 $1-0.567$ 进行比对了。\n可以发现，最终的比较阈值是 0.5。大于 0.5 代表 Rachel 更加适合做女朋友，小于 0.5 代表更加不适合。\n\n可是 Julie 的计算得分是 0.512，此时按道理讲，Julie 也应当被划分到更加适合做女朋友那一类里。这就和标注的样本相违背，和剧情发展不同，是错误的。那么有两种解决办法\n\n> 办法1：调整这个 0.5 的阈值，改成 0.55，这样 Julie 的分值低于 0.55，就可以被划归到不适合做女朋友这一类中。（这种方法随后再详细补充介绍）\n\n> 办法2：权重 $w$ 和 偏置值 $b$ 在 Ross 打完分之后其实都可以可以进行调节的，我们可以调节这些值，从而使 Julie 的得分低于 0.5。这个过程就是逻辑回归模型的训练。\n\n\n## 逻辑回归的训练\n\n所谓模型训练，就是将样本输入 x 和输出二分类标签 y 当作固定不变的定值，去调节 $w$ 和 $b$，使得样本的输入和输出相匹配。\n\n放在上述的例子中，就是 Ross 已经为两人打好了分，我们需要有一个模型，为两人是否适合做女朋友给出正确的判断。我们需要调节权重，将 Julie 也划归为不适合做女朋友这一类。\n\n讲到模型训练，很多博客会直接抛出一长串损失函数和求导过程，告诉读者，**模型训练核心就是损失函数求导**，然后结束。\n\n虽然结论是对的，但事实上，这种文章依然没有**直观**地向读者展现，为什么列出求导公式就等同于模型训练。因此，我将根据例子详细阐述这个过程是如何而来的。\n\n### 逻辑回归的损失函数\n\n根据前例，我们知道一个样本最终的计算值在 $(0,1)$ 之间浮动，可以被视作概率值。并且由于sigmoid函数的单调性，我们可以认为，概率值越接近1，说明样本越接近正样本，评价质量越好，例如 Rachel 的得分是 0.567，这个值是不够高的，如果 Rachel 的得分是 0.99，就意味着在 Ross 心里，她几乎接近 100% 是理想的女朋友人选。反之，一个得分 0.05 的候选人一定比一个得分 0.3 的候选人更加不适合做女朋友。一个得分为 0 的候选人，其含义基本等同于，哪怕世界上只剩这一个女人，Ross 都不会碰她。\n\n模型训练过程，就是使 Rachel 的计算概率值从 0.567 更加接近 1。而 Julie 的值从 0.512 更加接近 0。\n\n因此，我们可以构造一个似然函数，它衡量了计算得分值和真实样本标签之间的差距：\n$$L(w)=(p(x))^y(1-p(x))^{1-y}$$\n\n这里的 $p(x)$ 指的是就是从 Ross 为两人打完分之后，到计算得到概率值的过程，y的取值范围就两个值，$y \\in \\{0, 1\\} $。\n\n由于我们的样本不止一个，很有可能，Ross 有无数个备胎，假设为 $N$，那么似然函数就应当照顾到所有的样本，可以定义成\n$$L(w)=\\prod_{i=0}^{N}(p(x_i))^y_i(1-p(x_i))^{1-y_i}$$\n\n再考虑到该函数的连乘，也未对样本的数量做归一化，应当转换成\n$$Loss(w)=-\\frac{1}{N}lnL(w)$$\n\n这样就得到了一个方便求导的损失函数。它的含义是指，**如果样本的计算值越接近它的标签值，0或1，那么损失函数值就越小，模型和数据匹配度就越好**。\n\n> 损失函数和似然函数的区别：\n> **损失函数**指的是机器学习模型训练阶段，用来衡量模型预测错误程度的函数，这个错误值越小越好。\n> **似然函数**指的是比较两个概率分布，其中一个是真实分布，另一个是抽样数据，两者最好是越接近越好。最大似然就是此意。\n\n### 训练过程\n\n依然通过举例的方式来阐述这个训练过程。\n\nJulie 的三项指标打分为 $[0.7, 1.0, 1.0]$，目前对应的权重为 $w=[1.0, 0.1, 0.1]$，$b=-0.85$\n根据公式计算得到的结果为 0.512，这个值大于 0.5，按理来讲，我们应该调节 $w,b$ 两块参数，使得 Julie 最终计算得到的值低于 0.5，也就是让 Julie 的计算概率结果从 0.512逐渐变小，尽量向 0 靠近。\n![image](https://www.jionlp.com/blog_image/20220908/a65126417f.jpg)\n\n根据 sigmoid 函数的**单调递增性**，如果希望 sigmoid 的输出值变小，那么 sigmoid 函数的输入，也应当变小。即\n$$[0.7, 1.0, 1.0]^T[1.0, 0.1, 0.1] -0.85$$\n$$= 0.7 \\times 1.0 + 1.0 \\times 0.1 + 1.0 \\times 0.1 - 0.85 = 0.05$$\n\n这个值也应当变小，越接近负无穷大越好。\n\n此时，构成 0.05 这个值由三个权重和偏置值影响。我们单拎出来**工作**这个因素来考量，工作当前的权重为 0.1。\n\n这个多项式函数也是随着**工作**的权重单调递增的，此时，假设我将工作的权重从 0.1 调整为 0，则可以得到\n$$p(Julie适合做女朋友)=sigmoid([0.7, 1.0, 1.0]^T[1.0, 0, 0.1] -0.85) \\approx 0.487$$\n\n由此可以发现，减少工作的权重，确实可以使得 Julie 的计算结果降低，向 0 靠近一些，相应的损失函数值也降低了。\n\n实际上，判断该增加还是减少 $w$ 和 $b$ 的关键因素，就是这两个函数（sigmoid函数、多项式函数）的导数。**遵循着函数的导数方向，就可以朝着预期的方向迈进**。\n\n目前，我们将**工作**权重从 0.1 调整到了 0。只要我们**针对每一个样本、每一个权重和偏置，重复上述这个步骤**，就可以使得 Julie 的取值一步一步越来越接近0，同样的 Rachel 的值越来越接近1。这个过程就是模型的训练。\n\n可以发现，模型训练是一个复合函数的求导过程，利用损失函数，先对 sigmoid 求导，再对多项式函数求导，即可得到权重的变化值。将权重向应变的方向增大或者减小，即可实现模型的训练。\n\n由此，我们可以根据上述的损失函数对权重$w$，以及偏置 $b$ 求导。由于前面已经阐述了 sigmoid 的求导过程，此外，一次多项式的求导也非常简单，以参数 $w_i$ 为例，求导结果就是：\n$$\\frac{\\partial Loss(w_i)}{\\partial w_i}=\\frac{\\partial sigmoid(z)}{\\partial z} \\frac{\\partial z}{\\partial w_i}$$\n$$=p(x)(1-p(x))x_i$$\n\n其中，$x$指的是当前的样本，$x_i$指的是样本中的第 $i$ 个元素。\n\n此时，梯度就得到了，如果这条是 Julie 的样本，我们希望它尽量向 0 靠拢，则应当将该权重逐渐减小。如果是 Rachel 这条样本，则应当向 1 靠拢，权重应当逐渐增大。也就是，权重的变化方向是由样本的标签决定的。\n\n> 任何**复杂神经网络**的参数训练都完全脱胎于逻辑回归的这种复合函数求导，本质上都是一回事。\n\n\n## 模型的过拟合\n\n在这里例子里，我们有 4个参数，分别是**颜值、工作、性格、偏置值**，而样本数有两个，分别是 Rachel 和 Julie。现在假设，我们已经完成了模型的训练，Rachel 的取值为接近 0.9999，Julie 的取值接近 0.0001，那么，相应的 sigmoid 的输入值分别是 $10$ 和 $-10$。因为\n$$sigmoid(10) \\approx 0.999$$\n$$sigmoid(-10) \\approx 0.001$$\n\n此时可以列出两个多项式方程：\n$$1.0 \\times w_0 + 0.4 \\times w_1 + 0.8 \\times w_2 + b = 10$$\n$$0.7 \\times w_0 + 1.0 \\times w_1 + 1.0 \\times w_2 + b = -10$$\n\n实际上我们的模型训练过程就是在**求解一个最优的上述方程组的解**。4个未知数，2个方程，那么其解一定是有**无数组**。\n\n> 这无数组解，到底哪个才是最优的？\n\n我找到了两组解，一组解是：\n$$w_0=100, w_1=100, w_2=-250, b=-430$$\n\n另一组解是：\n$$w_0=5, w_1=-5, w_2=-77.5, b=69$$\n\n这两组解分别有一些统计意义上的特点，第一组解取值普遍比较大，绝对值都大于100，第二组解取值普遍都比较小，换句话讲，**第一组解的取值方差比较大，第二组解的取值方差比较小**。\n\n继续观察这两组参数，可以发现，$w_0$，也就是**颜值**这个选项，是对 Rachel 十分有利的，可以让她的最终得分更高；而 $w_2$ 这个参数，也就是**性格** 这个选项，原本是对 Julie 最有利，可以让她得分更高，却被赋予了一个很大的负值，使得**性格**越好的女生，被选择的概率越低。\n\n当然，有可能 Ross 就是一个受虐狂，女生越骄纵，越不礼貌，他反而越喜欢，有种 SM 倾向；也有可能，由于 Rachel 的独特性，使 Ross 忽略了**性格** 的重要性。总而言之，Ross 被眼前的两个女人蒙住了双眼，看不到世界上还有千千万万的女人。\n\n假使 Ross 又有了一个新的候选女友 Lucy（**第三个样本**），Lucy 其貌不扬，但是知性温柔，善解人意，好巧不巧，Ross 一见钟情，感觉 Lucy 比 Rachel 还要好。\n\n> 第三个样本：\n> 输入：Lucy：颜值 0.2，工作 0.8，性格 1.0；输出：适合做女朋友。\n\n此时，我们希望训练得到的模型依然能够适应 Lucy 并得到合理的结果。\n\n针对 Lucy，上述模型的权重就显得有点矛盾了：利用第一组参数，可知 Lucy 的得分是\n$$p(Lucy 适合做女朋友)=sigmoid(100 \\times 0.2 + 100 \\times 0.8 -250 \\times 1.0 -430)=1.2e^{-252}$$\n\n这是一个极度低的值，意味着 Lucy 极其不适合做女朋友。\n\n而利用第二组参数可知 Lucy 的得分是\n$$p(Lucy 适合做女朋友)=sigmoid(5 \\times 0.2 -5 \\times 0.8 -77.5 \\times 1.0 +69)=1.01e^{-5}$$\n\n此时可以发现，虽然第二组参数依然把 Lucy 判定为不适合做女朋友这一类，但是这组值表面要比第一组值相对靠谱一些，或者说错得不那么离谱。\n\n对某些样本极其有利的权重值（**颜值**之于 Rachel），可能会对另外的样本（**颜值**之于 Lucy）产生很大的矛盾。这个权重值越大，矛盾感越大。\n\n其根本原因，就在于，**权重值的绝对值过高，会对某些样本产生极强的负作用力，这个极强的副作用力就叫做模型的过拟合**。\n\n很明显，我们防止过拟合很重要的一个手段，就是希望**权重值的绝对值**尽量维持在一个均值为0，方差较小的范围之内。这个值越趋近于 0，就说明这个权重对应的参数，影响力越小，对另外其它的样本可能产生的负作用力就越小，避免了错得很离谱。\n\n## 模型参数的正则化\n\n为了在模型训练过程当中，尽量获得比较小的权重值和偏置值，同样可以从损失函数入手。\n\n可以为原先的损失函数值上添加一个控制权重的多项式。这个多项式应当满足，**如果权重的绝对值越大，损失函数值就越大，如果权重的绝对值能够逐渐趋近于 0，说明模型抗过拟合效果好，损失函数值也应当变小**。\n\n一般有两种控制权重值的方法。\n\n### L1 正则化\n\n损失函数形式为\n$$Loss(X, \\lambda)=Loss(X) + \\lambda(|w| + |b|)$$\n\n其中，$X$ 指的是所有的样本集的输入，包含了 Rachel、Julie、Lucy 等等。$\\lambda$则是一个控制权重值的参数，它是必然大于 0 的实数值。这种控制权重值的方式是最直观的。\n\n### L2 正则化\n\n损失函数形式为\n$$Loss(X, \\lambda)=Loss(X) + \\lambda(w^2 + b^2)$$\n\n在这种形式中，是对每一个参数都求了平方，同样满足权重值越小，损失函数值越小的准则。\n\n\n## 总结\n\n还没写完，如果觉得好，可以扫码关注一下下方的公众号，或者搜索**JioNLP**。有关注我才有动力更新。\n\n\n\n结合文章，简要介绍清楚逻辑斯蒂回归模型的名字由来，并在100字以内解释，为何添加参数权重值在损失函数上，能够避免过拟合。"}
{"score": 5, "question_type": "【闭域问答】", "correct_answer": "提炼本文主要内容，能够涵盖英伟达技术垄断地位，华为等公司追赶要点即可。", "question": "本文写作耗时3天，共计5000字，阅读时长12分钟，带你深度分析一下英伟达（Nvidia） 这支股票。\n![](https://www.jionlp.com/blog_image/20240818/4965e82c36.jpg{width:60%;height:auto;})\n\n\n过些天，英伟达就要发布他们最新季度的财报了，我相信大家都在盯着英伟达的营收，想看看显卡卖的怎么样了，这直接影响到股票价格。\n\n2024年 6月份，也就是前一阵子，英伟达的市值突破了3.3万亿美元，成为了全球市值最高的公司，比整个大 A 里的所有公司加起来的市值还要高。稳稳地坐实了全球 AI 科技公司的第一把交椅上。但紧接着，7月份，英伟达股价连连下挫，从每股 150 美元跌倒了 100 美元，下跌了三分之一。\n![](https://www.jionlp.com/blog_image/20240818/9548553fb1.jpg)\n\n于是，很多身边的朋友们都在交流，**英伟达股票到了一个低点了，这个低点还能买入英伟达吗？**\n\n也有朋友很犹豫，**AI 目前的发展热潮是不是已经过了？英伟达度过了顶峰，是不是要走一段很长时间的下坡路了？**\n\n想要真正搞明白英伟达这支股票，那就势必要明白英伟达这家公司的发展前景。\n\n坦白了，我是一个 AI 算法工程师，压根不懂金融各种高深的骚操作，我分析英伟达，就单纯分析这家公司的前景，也就是它的科技市场价值。\n\n当然，我知道啊，大 A 里面有很多骚操作，什么上市容易退市难，还有什么圈一波钱就跑路，每天大 A 的成交量里，80%都是机器在操作。反正不管怎么说，我就信一点，**大A连赌场都不如**。当然了，这话不是我说的啊。这话是上海某位退休官员说的。但是我相信，金融工具永远是为经济服务的，说穿了，金融层面的所有价值，不管垒多高，都必须建立在企业真实价值基础上。我就是这么来看待所有科技公司的，包括英伟达。\n\n# AI 的发展前景\n说英伟达，那肯定需要简单缕一缕 AI 技术的发展。只有 AI 能够不断冲刺往前发展，英伟达才能够保住股价，而非在全球市值第一的宝座上昙花一现。毕竟，英伟达之所以发家，靠的就是给 AI 模型提供基础硬件，它是给淘金者卖锤子和铲子的。\n\n去年，也就是 2023年一整年，AI 都被炒的火热，全世界，数得上号的公司，都在讲，要做 GPT 类的大模型，GPT 搞完了，就开始搞 SD 文生图、Sora 文生视频模型、多模态，甚至机器人。\n\n去年，我给很多外行朋友讲 GPT，他们都很兴奋，觉得AI大模型有点意思啊！激起了好奇心。但是今年，人们普遍对 GPT 祛魅了，发现 AI 大语言模型也就是这么回事，没什么大不了的。AI 有进入了一个相对冷淡期。\n\n的确，GPT 这类 AI 模型能替代人类的工作总量，可能也就 10% 左右，你感觉 AI 好像已经爆发了，无处不在了，但实际上，AI 仍然是一个小学生，人类还有大量的工作需要让 AI 来替代。\n\n前一阵子，萝卜快跑不是在武汉投入试运营了嘛。自动驾驶本身就是一种 AI。抛开自动驾驶会造成大面积司机失业不说，单说开车这件事，绝大多数人都觉得这是个麻烦事。它是一项任务，是一个工作，司机在汽车里一坐就是七八个小时，很煎熬，它很少带来人类的享受和愉悦。\n![](https://www.jionlp.com/blog_image/20240818/7c16ae6e27.jpg)\n\n再比如，很多工作需要巡查巡检，现在市场上不是有那个自动扫码，自助购物嘛，就是无人超市，但是它有个问题，虽然扫码结账是自助完成的，你还是需要一个收银员，来监督、辅助你购物，而不是私自就拿走超市的货品，不付款就走了。也就是说，无人超市其实是一种不彻底的 AI 技术的应用实践。它还需要有其它一些 AI 机器人，摄像头来辅助完成 100% 的自动化。\n\n这种不彻底的 AI 技术应用还有很多。\n\n还有家用的炒菜机器，那种自动炒菜机就是一个不停旋转的锅，来回翻腾。但是切菜的还得是人，什么菜放多少量，怎么放还都得是人来完成。而且这块没有达到100%的自动化做饭。\n\n多得我就不扯了，其实就是一个意思，人类对 AI 的需求依然是非常非常强烈的，而**上述大量的 AI 需求，需要两种目前还不成熟的 AI 技术——多模态模型，以及机器人**。从需求侧来讲，AI 仍然有大量的处女地需要开垦。\n\n这里最大的处女地，就是 agent和机器人，需要有机器人不仅会炒菜，会读菜谱，还得会切、会配料、会端盘子。\n\n所以说，单从需求侧来说，AI 就是未来的发展方向，英伟达肯定依然有很广阔的市场需求前景。但是，这就像趴在窗户上的苍蝇，天空广阔，前景是美好的，但是，能不能飞的出去就成了一个问题。光趴在窗户上，高喊 AI 是未来趋势，但飞不出去，做不出 AI 产品，其实也不解决问题的，对吧。\n\n\n# AI 能做到什么？\n\n有了需求了，接下来其实就是，AI 能不能做出产品的问题。我的回答是乐观的。\n\n我们都知道，AI 模型，不管什么 AI 模型，就是两样强需求，用于训练 AI 模型的数据，以及在哪里训练 AI 模型。英伟达解决了后者，而且是后者当之无愧的霸主。\n\n现阶段，几乎所有的 AI 模型训练，都在依赖英伟达的 GPU 显卡。训练 GPT 模型消耗了上千张、上万张英伟达显卡，这是一笔大资金，许多AI公司想要玩下去，首先就得去买英伟达的显卡。\n\n比如，自动驾驶的车辆，势必要在汽车上安装一块 GPU 显卡，来完成 AI 计算，指挥控制车辆该往哪开？遇见行人要避让还是按喇叭？应该没人还给我提利用 5G 实现车辆互联云端计算吧？你还提你脑子就瓦特了。\n\n当然了，百度的自动驾驶有些细节并不足够先进，这个我先不说了。单说在汽车上使用 AI 模型。这是一笔大生意，如果自动驾驶推广开，每一辆汽车都需要安装一块或者几块显卡，那英伟达岂不是发了？如果真的是这样，你说，要不要在此刻买英伟达的股票？还是个问题吗？\n![](https://www.jionlp.com/blog_image/20240818/fab42be2bd.jpg)\n\nAI 肯定不仅仅是用在汽车上这么一个方面。比如，这两天，志辉君的那个智源机器人也开了产品发布会，他们制造的机器人能已经能够做到一些简单的行为动作，目标受众就是在未来，每家每户里都有一个智能机器人，给你扫地、做饭、擦马桶。当然，他们的产品还远不成熟啊，这自不必说。做人形机器人的也不仅仅是他们那一家。\n\n但是这里有一点，要想做到非常强大的智能，那就每一台机器人身上都得绑定一块 AI 显卡。\n\n你敢想象这个 AI 的 GPU 显卡市场有多大吗？\n\n是不是听了这些消息，感觉得立马冲英伟达了？ 但是你先别急着去买英伟达的股票，这里有俩问题：\n- 1、显卡又不是英伟达一家，华为也有啊。微软也想插手显卡领域。\n- 2、你怎么知道自动驾驶，当然也包括其它 AI 模型、机器人肯定能发展成熟？而非遇到 AI 技术瓶颈？走向停滞？\n\n对于第一个问题，我稍后讲，我先说第二个问题。\n\n我们都知道，AI 技术发展路线，是先解决大语言模型，然后逐步拓宽到多模态领域，agent，最后再拓宽到机器人领域的。\n\n说实话，多模态领域，已经有了一些还不错的 AI 成果出来，但是依然不足够。它还需要更多的数据，更大的算力，花时间去磨。\n\n但是有一点是确定的，那就是，大语言模型验证了通过扩展数据和算力，能够提升 AI 模型的能力上限。这一点是通用的、粗略的准则，针对多模态 AI 模型，都是适用的。\n![](https://www.jionlp.com/blog_image/20240818/47336ea634.jpg)\n\n最近这今天，OpenAI 以及月之暗面等等其它几家 公司发布了多模态能力。能理解图片中的信息，包括复杂信息的识别和抽取，效果非常出乎我的预料，感兴趣你可以去试试。\n\n也就是说，**AI 技术的发展，前面依然有一条相对确定的路，人们只需要把 AI 技术工作推进下去就好了。虽然前面是一条泥泞的土路，但是方向是对的，能够达到终点。**\n\nAI 从1950年代发展至今，经历了好几次的起伏，一开始，人们都觉得，AI 跟闹着玩一样，到现在，GPT 的诞生，有火热发展期，也有冷淡期，许多政府一听见实验室在搞AI，干脆不拨经费的时期；到了现在，各个国家政府都在极力推动 AI 技术发展。这种一冷一热，就跟冰火两重天似的。 总之，你会越来越感觉到，**AI 它可能会迟到，但一定不会缺席**。\n\n当然，对于英伟达来说，它需要等，需要等待多模态、机器人这些技术不断走向成熟，它才能够维持它的全球显卡霸主地位。\n\n炒股有两种策略，短线和长线。 英伟达它就天生不是短线选手，它伴随着 AI 技术的发展，肯定是需要搞长期策略的。AI 的进步，看起来好像，今天这家公司出了个 AI 模型，明天那家公司又出了个 AI 模型。搞得好像军备竞赛，美苏争霸一样。但它技术推进并不算容易，肯定是一个长期的过程。\n\n炒股主要是在炒预期，现在几乎所有的科技公司，但凡上点规模的，都在押宝 AI。这一点认识需要有，AI 的前景，短期看，一定会有发展停滞缓慢，但长期看，100% 一定是乐观的。\n\n\n# 华为的 GPU 显卡？\n\n前面提到了，AI 需要显卡，但是显卡又不光是英伟达，别人家也有啊！比如，美国去年以国家安全为由，禁止英伟达向中国大陆出售先进的显卡 A100 等型号。只能卖给中国垃圾显卡，阉割版 H20 等等，那性能垃圾的，我就不多提了。\n\n当然了，美国政府就是故意的，就是要打压中国的 AI 技术发展。不过中国也有**华为，它生产的 GPU 叫做昇腾**。于是，我国政府也在扶持华为的设备，国内这两年许多政府、央企事业单位如果要采购 AI 模型的话，必然要满足国产化需求，禁止使用国外显卡，必须采购国产显卡。\n![](https://www.jionlp.com/blog_image/20240818/0dedd6a04e.jpg{width:70%;height:auto;}) \n\n\n政府和央企环顾四周，发现，国内好像相对做显卡好一些的，也就是华为的昇腾了。所以，**很多项目采购合同里，都明确写了，需要华为的服务器+华为的显卡。** 而且，好像下个月，华为就要推出新一代昇腾显卡 910c 了，性能上大幅提高，可以堪比英伟达的某些先进型号。\n\n故事讲到这里，可能你觉得，喜大普奔，国产科技也是站起来了，厉害了，我的国。\n\n但是，我要说但是，**华为的设备可苦了国内许多给政府招标项目做研发的工程师了**。\n\nGPU 显卡是计算机的硬件设备，如果它需要运行 AI 模型程序，就需要配套的软件支持。目前华为的设备，配套的软件主要有两个，hccl 和 mindspore。它分别对标了英伟达体系的 cuda，以及 众多AI 公司开发的框架软件，包括但不限于 pytorch、tensorflow、jax、paddle 等等。\n\n**现阶段，几乎所有先进的 AI 模型的训练都在强依赖英伟达体系的 cuda 和 pytorch 。这个占比，我估计达到了98%+** 。\n\n我们都知道，paddle 是百度开发的 AI 模型训练推理软件，李彦宏曾经下大力气推动过，要求所有百度的 AI 工程师在训练模型时，强制使用 paddle，不可以使用美国公司开发的 pytorch。但是，然并卵。\n\n因为，paddle 不成熟，用户体量小，代码bug多，功能少；相反，pytorch 有全世界的 AI 工程师在维护，成熟、稳定。这就是一种强者通吃的效应。特别像以前日本的一些科技公司，丰田和本田把美国、英国的汽车产业打的落花流水，松下和东芝的家电卖到了全亚洲的各个角落。形成了强大的垄断效应。针插不进、水泼不进。除非它自己作死，否则死不了。\n\n相对于 pytorch + cuda + 英伟达显卡 联立而成强大垄断效应，也达到了这种效果。全世界，不管中国美国，还是日本欧洲，随便拉过来一个 AI 工程师、AI 研究员，你问他，用什么软件搞 AI ？接近 100% 的回答是，pytorch+cuda+英伟达显卡。\n\n**就连百度自己的工程师都不愿意用 paddle，更愿意用 pytorch**。李彦宏强制要求员工使用 paddle来开发 AI 模型，有用吗？短期内，有用。但只要你放松这种强制，工程师们会自动转向使用 pytorch。\n\n谁会拒绝使用一个更方便、更好用的产品呢？上层的强制是一时的，最终一个产品的成功，靠的还是产品本身的过硬实力。\n\n**华为，已经可以算作是一家国企了，只是名份上还不是**。华为重新搭建了 mindspore+ + hccl + 昇腾显卡 的路子，它和 pytorch + cuda + 英伟达显卡 是对立的，另起炉灶的路子。\n![工程师们使用华为mindspore软件](https://www.jionlp.com/blog_image/20240818/be6e35203f.jpg)\n\n但是，就我从身边以及网上的 AI 工程师经验所知，许多支撑国产化项目的 AI 工程师，由于国家强制力，被迫使用了 mindspore 软件，左一个坑，右一个坑，每一个坑都需要花时间、花人力来解决。大家会发展，华为的 mindspore，不像是推进 AI 技术的引擎，反而是阻碍 AI 技术发展的绊脚石。\n\n**如果用 pytorch+英伟达 是在高速公路上飙车的话，那么，mindspore + 昇腾显卡 训练模型就是牛拉板车在泥泞的土路上前进**。\n\n换句话说，这和你昇腾显卡运算速度快不快，关系都不大，你的 910c 运行再快，车，是一辆超跑，是凯迪拉克，但，路，是农村的土路，照样跑不起来。\n\n所以，如果把显卡比作车的话，那些建立在上面的软件，以及用户群体，用户习惯，就构成了英伟达强大健全的生态。\n\n造一辆车花钱多？还是造一条路花钱多？这是个显而易见的问题。所以，**对于华为来说，最难突破的不是显卡本身，而是英伟达建立的软件护城河。**\n\n这些年，国家在强力支持华为的显卡和 AI 技术。但是这就像李彦宏在强制员工要求使用自家研发的 paddle 一样，只要你放松了支持和强制，就会导致前功尽弃。当然，paddle、tensorflow 依然是建立在 cuda 软件生态基础上的框架。不过 paddle 听说为了响应国产化替代，也在支撑华为生态了。\n\n所以说，短期内，华为没有和英伟达竞争的实力，这不仅仅是他们家新推出的 昇腾 910c 速度快不快的问题。\n\n**什么时候华为的显卡能真正和英伟达抗衡呢？** 有两点条件：\n\n- 1、国家对华为的支持足够久，大概需要强力持续支持 4~8年。等华为的mindspore 和 hccl 这些生态逐步成熟起来之后，AI工程师们感觉使用华为不再那么痛苦之后。\n- 2、中国需要有足够先进的、领先的 AI 模型使用华为的显卡和软件率先推出新 AI 模型。这一点的意思，说白了就是华为需要吸引别人来主动用你的软件和显卡，而不是华为通过国家手段逼着别人去用。\n\n第二点又扯到了一个中国创新的问题，关于中国创新，我在这里就不多谈了，你可以参考这篇文章：【[**从互联网衰退看中国创新**](https://www.jionlp.com/blog/AIGC/20240107)】\n\n当然，我说的是华为显卡啊，想干掉英伟达的也不只是华为，美国的其它一些 AI 公司也看英伟达不顺眼了，其中就包括 OpenAI。奥特曼巴不得自己掌握一家 GPU 公司，替代掉英伟达。但是遇见的问题也和华为遇见的是一样的，我这里就不多谈了。\n\n所以，这和买股票有什么关系呢？**短期看（4到8年），英伟达毫无疑问还是全球 GPU 的霸主。前阵子英伟达跌挺狠，但英伟达的股价还能不能涨回去，这就不是一个问题。只要英伟达一天还是 GPU 的王，还是 AI 显卡领域的霸主，那么它的股价就肯定还会维持在一个长期的高位。**\n\n# 英伟达的股价\n\n妈的，说真的，英伟达的股价确实吓人。去年股票价格猛增到 40美元（按当前股价推算），把很多人都吓怕了，觉得这公司股票价格涨这么狠，应该到顶点了吧？\n\n结果英伟达一飞冲天直接飙到了80美元，紧接着就来了一波惨跌，跌去四分之一，跌倒了60美元。\n![](https://www.jionlp.com/blog_image/20240818/ca63e44d60.jpg)\n\n你看，上次股价暴跌，是不是和这次还挺像的？这次更狠，从接近 150 跌到了100左右。但是你要说，英伟达这家公司就这么蹉跎下去了吗？完全没有好吧。\n\n所以，观察英伟达的曲线，它每天都在涨涨跌跌，说实话，你要是能有本事在150点卖，在100点买，我敬你是个人才。我没有这个本事，我也不知道 150 是不是英伟达的暂时性高点。\n\n对于普通人来说，英伟达从来都不是短线股票，不是说你今天买了，过些天就卖了这么玩的。作为一个金融行业的外行，你能玩的明白金融公司那帮妖神吗？\n\n中金公司普通员工前些年，人均年薪都在 100 万以上，而中国所有劳动者，月薪过了 5000 块个税起征点的才 8000 万人。**金融行业从业者和外行股民一定是存在信息差的。这种东西对于所有股票都一样，我们没有必要去做短线的考量。**\n\n什么是长线，就是价值投资，对 AI 科技的信心，对英伟达护城河的信心。可能我这篇文章写得会让你感觉盲目乐观。\n\n当然了，天下没有不散的筵席，什么时候会标志着英伟达股票正式跌下神坛呢？我给你几个例子，当未来的新闻出现以下几种情形时：\n\n- 1、**中国某公司（大概率华为）自主研发了一款名为 XX 的 AI 模型，震惊世界，效果拔群，使用的就是昇腾显卡和软件**。美国工程师们纷纷复刻仿照。\n- 2、**AI 发展进入长期寒冬，谷歌、OpenAI、百度、字节纷纷放弃 AI 领域，转向其它科技风口**。对于普通人的你，已经有 2 年以上都没有听说过有什么有突破性的 AI 新模型的新闻了。\n- 3、**量子计算机实现量产。** 颠覆了当前的 AI 玩法，什么 pytorch、cuda、GPU 都成了一堆历史的垃圾。\n\n以上三点，对英伟达公司的股价冲击依次增强。补充一下量子计算机的说明，这项技术的产业化大概率在 20~30 年以内还是一个难题。\n\n如果你还没从新闻里听到这些消息，那么，就请放心大胆地买英伟达的股票吧。\n\n当然了，英伟达现在股价徘徊在120美元，说实话，是个高点。**这么高的高点还敢追吗** ？\n\n其实我觉得，这个事，是一个年经问题，如果你回故 历史，假设自己站在曾经多年之前的时间点来看，处处都是相对的高点。毕竟，人类计算机和 AI 科技的最前沿，确实在不断前进发展，这从我们的生活信息化水平发展就能看出来，所有的先进科技水平，就集中体现在“那个”股市里呀。\n![](https://www.jionlp.com/blog_image/20240818/58a68016b9.jpg)\n\n\n根据上文，总结文章核心观点，按1，2，3点列出，不超过100字。"}
