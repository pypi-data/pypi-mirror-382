import abc
from _typeshed import Incomplete
from a2a.types import AgentCard as AgentCard
from abc import abstractmethod
from dataclasses import dataclass
from gllm_agents.agent.base_agent import BaseAgent as BaseAgent
from gllm_agents.agent.system_instruction_context import get_current_date_context as get_current_date_context
from gllm_agents.constants import TEXT_PREVIEW_LENGTH as TEXT_PREVIEW_LENGTH
from gllm_agents.mcp.client import LangchainMCPClient as LangchainMCPClient
from gllm_agents.memory import BaseMemory as BaseMemory, MemoryFactory as MemoryFactory, MemoryMethod as MemoryMethod
from gllm_agents.memory.constants import MemoryDefaults as MemoryDefaults
from gllm_agents.schema.agent import StreamMode as StreamMode
from gllm_agents.tools.tool_config_injector import CONFIG_SCHEMA_ATTR as CONFIG_SCHEMA_ATTR, TOOL_CONFIG_SCHEMA_ATTR as TOOL_CONFIG_SCHEMA_ATTR, inject_config_methods_into_tool as inject_config_methods_into_tool
from gllm_agents.types import A2AEvent as A2AEvent, A2AStreamEventType as A2AStreamEventType
from gllm_agents.utils import validate_references as validate_references
from gllm_agents.utils.langgraph.tool_managers.a2a_tool_manager import A2AToolManager as A2AToolManager
from gllm_agents.utils.langgraph.tool_managers.delegation_tool_manager import DelegationToolManager as DelegationToolManager
from gllm_agents.utils.logger_manager import LoggerManager as LoggerManager
from gllm_agents.utils.metadata_helper import DefaultStepMessages as DefaultStepMessages, FINAL_AGENT_THINKING_FINISHED_CONTENT as FINAL_AGENT_THINKING_FINISHED_CONTENT, Kind as Kind, MetadataFieldKeys as MetadataFieldKeys, Status as Status, end_step_counter_scope as end_step_counter_scope, get_next_step_number as get_next_step_number, start_step_counter_scope as start_step_counter_scope
from gllm_agents.utils.token_usage_helper import STEP_USAGE_KEY as STEP_USAGE_KEY, TOTAL_USAGE_KEY as TOTAL_USAGE_KEY, USAGE_METADATA_KEY as USAGE_METADATA_KEY
from gllm_core.event import EventEmitter
from langchain_core.messages import BaseMessage as BaseMessage
from langchain_core.tools import BaseTool
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages as add_messages
from langgraph.graph.state import CompiledStateGraph as CompiledStateGraph
from langgraph.types import Checkpointer as Checkpointer, StreamWriter as StreamWriter
from typing import Any, AsyncGenerator, Sequence

logger: Incomplete

@dataclass
class _StreamingContext:
    """Context object for managing streaming state and configuration."""
    original_query: str
    graph_input: dict[str, Any]
    config: dict[str, Any]
    memory_user_id: str | None
    current_thread_id: str
    token: Any
    enable_token_streaming: bool
    final_event_yielded: bool = ...
    pending_artifacts: list | None = ...
    seen_artifact_hashes: set | None = ...
    processed_message_count: int = ...
    final_state: dict[str, Any] | None = ...
    last_final_content: str | None = ...
    saved_memory: bool = ...
    is_token_streaming: bool = ...
    def __post_init__(self) -> None:
        """Initialize mutable defaults."""

class BaseLangGraphAgent(BaseAgent, metaclass=abc.ABCMeta):
    """Base class for LangGraph-based agents with unified tool approach.

    Provides core LangGraph functionality including:
    - Graph compilation and execution
    - State schema management
    - I/O mapping between user inputs and graph states
    - Event emission support
    - Tool resolution and handling
    - A2A communication capabilities via tools
    - Agent delegation capabilities via tools
    - MCP server integration via tools
    - Enhanced output extraction from various state formats

    Tool Management:
    - regular_tools: Standard LangChain tools provided during initialization
    - mcp_tools: Tools retrieved from MCP servers
    - resolved_tools: Combined collection of all tools for graph execution

    Subclasses must implement:
    - define_graph(): Define the specific graph structure
    - _prepare_graph_input(): Convert user input to graph state
    - _format_graph_output(): Convert final graph state to user output
    """
    state_schema: Incomplete
    thread_id_key: Incomplete
    enable_a2a_token_streaming: Incomplete
    event_emitter: Incomplete
    checkpointer: Incomplete
    memory: BaseMemory | None
    a2a_tool_manager: Incomplete
    delegation_tool_manager: Incomplete
    regular_tools: list[BaseTool]
    mcp_tools: list[BaseTool]
    resolved_tools: list[BaseTool]
    def __init__(self, name: str, instruction: str, description: str | None = None, model: Any | None = None, tools: Sequence[BaseTool] | None = None, state_schema: type | None = None, thread_id_key: str = 'thread_id', event_emitter: EventEmitter | None = None, checkpointer: Checkpointer | None = None, enable_a2a_token_streaming: bool = False, **kwargs: Any) -> None:
        '''Initialize the BaseLangGraphAgent.

        Args:
            name: The name of the agent.
            instruction: The system instruction for the agent.
            description: Human-readable description of the agent.
            model: The model to use (lm_invoker, LangChain model, string, etc.).
            tools: Sequence of regular LangChain tools (not A2A or delegation tools).
            state_schema: The state schema for the LangGraph. Defaults to basic message state.
            thread_id_key: Key for thread ID in configuration.
            event_emitter: Optional event emitter for streaming updates.
            checkpointer: Optional checkpointer for conversation persistence.
            enable_a2a_token_streaming: Enable token-level streaming for A2A responses.
                - False (default): Stream message-level events only
                - True: Stream individual tokens plus message-level events
            **kwargs: Additional keyword arguments passed to BaseAgent (including tool_configs and memory settings).
                Memory settings include:
                - memory_backend: Memory backend (e.g., "mem0")
                - agent_id: Agent identifier for memory scoping
                - memory_namespace: Memory namespace
                - save_interaction_to_memory: Whether to save interactions (default True)
        '''
    def set_operation_mode(self, mode: str) -> None:
        '''Set the operation mode for dependency tracking.

        Args:
            mode: Operation mode - "parallel" (default) or "sequential"
        '''
    @abstractmethod
    def define_graph(self, graph_builder: StateGraph) -> CompiledStateGraph:
        """Define the specific graph structure for this agent type.

        Subclasses must implement this method to:
        1. Add nodes to the graph_builder
        2. Add edges and conditional edges
        3. Set entry points
        4. Return the compiled graph

        Args:
            graph_builder: The StateGraph builder to define nodes and edges on.

        Returns:
            The compiled graph ready for execution.
        """
    def register_a2a_agents(self, agent_cards: list[AgentCard]) -> None:
        """Register A2A communication capabilities using the A2A tool manager.

        Args:
            agent_cards (list[AgentCard]): List of AgentCard instances for external communication.
        """
    def register_delegation_agents(self, agents: list[BaseAgent]) -> None:
        """Register internal agent delegation capabilities using the delegation tool manager.

        Args:
            agents: List of BaseAgent instances for internal task delegation.
        """
    tools: Incomplete
    def update_regular_tools(self, new_tools: list[BaseTool], rebuild_graph: bool | None = None) -> None:
        """Update regular tools (not capabilities).

        Args:
            new_tools: New list of regular tools to use.
            rebuild_graph: Whether to rebuild graph. If None, uses auto_rebuild_graph setting.
        """
    def run(self, query: str, **kwargs: Any) -> dict[str, Any]:
        """Synchronously run the LangGraph agent.

        Args:
            query: The input query for the agent.
            **kwargs: Additional keyword arguments.

        Returns:
            Dictionary containing the agent's response.
        """
    async def arun(self, query: str, **kwargs: Any) -> dict[str, Any]:
        """Asynchronously run the LangGraph agent with lazy MCP initialization.

        Args:
            query: The input query for the agent.
            **kwargs: Additional keyword arguments including configurable for LangGraph.

        Returns:
            Dictionary containing the agent's response and full final state.
        """
    async def arun_stream(self, query: str, **kwargs: Any) -> AsyncGenerator[str | dict[str, Any], None]:
        """Asynchronously stream the LangGraph agent's response.

        If MCP configuration exists, connects to the MCP server and registers tools before streaming.
        This method properly handles both LM Invoker and LangChain model streaming:
        - For LM Invoker: Uses StreamEventHandler to capture streaming events
        - For LangChain models: Uses LangGraph's native streaming implementation

        Args:
            query: The input query for the agent.
            **kwargs: Additional keyword arguments.

        Yields:
            Chunks of output (strings or dicts) from the streaming response.
        """
    async def cleanup(self) -> None:
        """Cleanup MCP resources including persistent sessions.

        This method performs best-effort cleanup of MCP client resources.
        Errors during cleanup are logged but do not raise exceptions to ensure
        the cleanup process completes gracefully.
        """
    async def arun_a2a_stream(self, query: str, **kwargs: Any) -> AsyncGenerator[dict[str, Any], None]:
        '''Asynchronously streams the agent\'s response in A2A format.

        Args:
            query: The input query for the agent.
            **kwargs: Additional keyword arguments.

        Yields:
            Dictionaries with "status" and "content" keys.
            Possible statuses: "working", "completed", "failed", "canceled".
        '''
