#ä¸èµ°æ³¨å†Œç³»ç»Ÿçš„ä¸€ä¸ªFlowç¨‹åº
from agent_os2 import Flow,BaseAgent
from typing import Any
import os
import re

class ReadFileContentBootstrapAgent(BaseAgent):
    """è¯»å–æ–‡ä»¶å†…å®¹å¹¶åˆå§‹åŒ–æœç´¢ä¸Šä¸‹æ–‡"""
    
    def setup(self):
        self.user_info = f"## ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶å†…å®¹...\n"
        self.prompts = {}  # ä¸è°ƒç”¨æ¨¡å‹
        self.strict_mode = False
        
    async def post_process(self, source_context: Any, llm_result: Any, shared_context: dict[str, Any], 
                          extra_contexts: dict[str, Any], observer: list[tuple[Any, BaseAgent]], 
                          batch_id: int | None = None) -> tuple[Any, dict[str, dict | list]]:
        
        # ç®€åŒ–é”™è¯¯å¤„ç†ï¼šç›´æ¥æŠ›å¼‚å¸¸è®©æ¡†æ¶å¤„ç†
        if not isinstance(source_context, dict):
            raise ValueError(f"æœŸå¾…å­—å…¸è¾“å…¥ï¼Œå¾—åˆ°: {type(source_context)}")
            
        file_path = source_context.get("file_path")
        query = source_context.get("query")
        max_chunks = source_context.get("max_chunks", 10)
        
        if not file_path or not query:
            raise ValueError("file_pathå’Œqueryå‚æ•°å¿…éœ€")
            
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        return {
            "file_path": file_path,
            "query": query,
            "content": content,
            "max_chunks": max_chunks
        }, {
            "memory_modify": {
                "file_path": file_path,
                "query": query,
                "content_length": len(content)
            }
        }
class CalculateChunkSizeAgent(BaseAgent):
    """æ ¹æ®æ–‡ä»¶é•¿åº¦å’Œmax_chunksè®¡ç®—åˆé€‚çš„åˆ†å—å¤§å°"""
    
    def setup(self):
        self.user_info = f"## ğŸ§® æ­£åœ¨è®¡ç®—åˆ†å—å¤§å°...\n"
        self.prompts = {}  # ä¸è°ƒç”¨æ¨¡å‹
        self.strict_mode = False
        
    async def post_process(self, source_context: Any, llm_result: Any, shared_context: dict[str, Any], 
                          extra_contexts: dict[str, Any], observer: list[tuple[Any, BaseAgent]], 
                          batch_id: int | None = None) -> tuple[Any, dict[str, dict | list]]:
        
        content = source_context.get("content", "")
        max_chunks = source_context.get("max_chunks", 10)
        
        if not content:
            raise ValueError("contentå‚æ•°å¿…éœ€")
        
        # ç®€åŒ–åˆ†å—å¤§å°è®¡ç®—
        if len(content) <= 1000:
            size, chunks = len(content), 1
        else:
            size = max(500, min(5000, len(content) // max_chunks))
            chunks = min(max_chunks, (len(content) // size) + 1)
        
        self.debug(f"æ–‡ä»¶é•¿åº¦: {len(content)}, åˆ†å—å¤§å°: {size}, é¢„è®¡åˆ†å—æ•°: {chunks}")
        
        return {
            **source_context,
            "chunk_size": size,
            "estimated_chunks": chunks
        }, {
            "memory_modify": {"chunk_size": size, "estimated_chunks": chunks}
        }
class SplitChunkAgent(BaseAgent):
    """åŸºäºå­—ç¬¦æ•°æŒ‰è¯­ä¹‰è¾¹ç•Œæ™ºèƒ½åˆ†å—æ–‡æœ¬å†…å®¹"""
    
    def setup(self):
        self.user_info = f"## âœ‚ï¸ æ­£åœ¨æ™ºèƒ½åˆ†å—æ–‡æœ¬å†…å®¹...\n"
        self.prompts = {}  # ä¸è°ƒç”¨æ¨¡å‹
        self.strict_mode = False
        
    def split_text_by_semantic_boundaries(self, content: str, chunk_size: int, max_chunks: int) -> list[dict[str, Any]]:
        """åŸºäºå­—ç¬¦æ•°æŒ‰è¯­ä¹‰è¾¹ç•Œæ™ºèƒ½åˆ†å—"""
        if not content:
            return []
        
        # å°æ–‡ä»¶ä¸åˆ†å—
        if len(content) <= chunk_size:
            return [{
                "content": content,
                "start_line": 1,
                "end_line": content.count('\n') + 1,
                "chunk_id": 0
            }]
        
        # ç®€åŒ–è®¡ç®—
        chunks = min(max_chunks, (len(content) + chunk_size - 1) // chunk_size)
        size = len(content) // chunks
        
        # å¤šå±‚æ¬¡è¾¹ç•Œå­—ç¬¦å®šä¹‰ï¼ˆä¼˜å…ˆçº§é€’å‡ï¼‰
        # ç¬¬ä¸€å±‚ï¼šæ¢è¡Œç¬¦ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        newline_chars = '\n'
        # ç¬¬äºŒå±‚ï¼šå¥å­ç»“æŸç¬¦ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰
        sentence_chars = 'ã€‚ï¼ï¼Ÿï½¡!?Â¡Â¿â€¼â‡âˆâ‰ï¸•ï¸–ï¸™'  # åŒ…å«ä¸­æ—¥éŸ©å¥å·ã€é—®å·ã€æ„Ÿå¹å·
        # ç¬¬ä¸‰å±‚ï¼šå­å¥åˆ†éš”ç¬¦ï¼ˆä½ä¼˜å…ˆçº§ï¼‰
        clause_chars = ';ï¼›:ï¼š,ï¼Œã€ï½€ï½¡ï½¥ï¾Ÿ'  # åŒ…å«ä¸­æ—¥éŸ©é€—å·ã€åˆ†å·ã€å†’å·
        
        result = []
        pos = 0
        
        for i in range(chunks):
            start = pos
            if i == chunks - 1:  # æœ€åä¸€å—
                end = len(content)
            else:
                # åˆ†å±‚è¾¹ç•ŒæŸ¥æ‰¾ï¼šä¼˜å…ˆæ‰¾æ¢è¡Œç¬¦ï¼Œå†æ‰¾å¥å­ç¬¦ï¼Œæœ€åæ‰¾å­å¥ç¬¦
                target = pos + size
                end = target
                search_range = min(target + 200, len(content))
                
                # ç¬¬ä¸€å±‚ï¼šåœ¨è¾ƒå¤§èŒƒå›´å†…ä¼˜å…ˆæŸ¥æ‰¾æ¢è¡Œç¬¦
                for j in range(target, search_range):
                    if content[j] in newline_chars:
                        end = j + 1
                        break
                else:
                    # ç¬¬äºŒå±‚ï¼šæŸ¥æ‰¾å¥å­ç»“æŸç¬¦
                    for j in range(target, search_range):
                        if content[j] in sentence_chars:
                            end = j + 1
                            break
                    else:
                        # ç¬¬ä¸‰å±‚ï¼šåœ¨è¾ƒå°èŒƒå›´å†…æŸ¥æ‰¾å­å¥åˆ†éš”ç¬¦
                        small_range = min(target + 50, len(content))
                        for j in range(target, small_range):
                            if content[j] in clause_chars:
                                end = j + 1
                                break
                        
            text = content[pos:end]
            if text.strip():
                start_line = content[:pos].count('\n') + 1
                result.append({
                    "content": text,
                    "start_line": start_line,
                    "end_line": start_line + text.count('\n'),
                    "chunk_id": len(result),
                    "start_char": pos,
                    "end_char": end
                })
            pos = end
            if pos >= len(content):
                break
        
        return result
        
    async def post_process(self, source_context: Any, llm_result: Any, shared_context: dict[str, Any], 
                          extra_contexts: dict[str, Any], observer: list[tuple[Any, BaseAgent]], 
                          batch_id: int | None = None) -> tuple[Any, dict[str, dict | list]]:
        
        content = source_context.get("content", "")
        if not content:
            raise ValueError("contentå‚æ•°å¿…éœ€")
            
        chunks = self.split_text_by_semantic_boundaries(
            content, 
            source_context.get("chunk_size", 1000), 
            source_context.get("max_chunks", 10)
        )
        
        sizes = [len(c['content']) for c in chunks]
        avg = sum(sizes) // len(sizes) if sizes else 0
        self.debug(f"åˆ†å—å®Œæˆ: {len(chunks)}ä¸ª, å¹³å‡{avg}å­—ç¬¦, èŒƒå›´{min(sizes) if sizes else 0}-{max(sizes) if sizes else 0}")
        
        return {
            **source_context,
            "chunks": chunks,
            "actual_chunk_count": len(chunks)
        }, {
            "memory_modify": {
                "actual_chunk_count": len(chunks),
                "chunks_info": [{"start_line": c["start_line"], "end_line": c["end_line"]} for c in chunks]
            }
        }
class ExtractRelatedContentAgent(BaseAgent):
    """ä½¿ç”¨AIè¿›è¡Œè¯­ä¹‰æœç´¢å¹¶æå–ç›¸å…³å†…å®¹"""
    
    def setup(self):
        self.user_info = f"## ğŸ” æ­£åœ¨è¿›è¡Œè¯­ä¹‰æœç´¢...\n"
        self.batch_field = "src.chunks"  # æ‰¹å¤„ç†chunks
        self.strict_mode = True
        self.retry_count = 2
        # model_configä¼šè‡ªåŠ¨ä»çˆ¶çº§Flowç»§æ‰¿ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®
        
        self.prompts = {
            "system": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è¯­ä¹‰æœç´¢åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åœ¨ç»™å®šçš„æ–‡æœ¬ç‰‡æ®µä¸­å¯»æ‰¾ä¸æŸ¥è¯¢ç›¸å…³çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­ç‰‡æ®µã€‚

âš ï¸ ä¸¥æ ¼è§„åˆ™ï¼š
1. åªèƒ½ä»å½“å‰ç»™å®šçš„æ–‡æœ¬å—ä¸­æå–ç‰‡æ®µï¼Œç»å¯¹ä¸èƒ½åŸºäºæ¨æµ‹æˆ–å¸¸è¯†æ·»åŠ ä¸å­˜åœ¨çš„å†…å®¹
2. start_snippetå’Œend_snippetå¿…é¡»éƒ½èƒ½åœ¨å½“å‰æ–‡æœ¬ä¸­æ‰¾åˆ°ï¼Œä¸€å­—ä¸å·®
3. å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„ç›¸å…³ç‰‡æ®µï¼Œå®å¯è¿”å›ç©ºåˆ—è¡¨ä¹Ÿä¸è¦ç¼–é€ 
4. æ¯ä¸ªç‰‡æ®µå¿…é¡»æ˜¯è¿ç»­çš„æ–‡æœ¬ï¼Œä¸èƒ½è·¨è¶Šä¸ç›¸é‚»çš„è¡Œ

æŠ€æœ¯è¦æ±‚ï¼š
1. ä»…åœ¨ä¸æŸ¥è¯¢æœ‰æ˜ç¡®è¯­ä¹‰ç›¸å…³æ€§æ—¶æ‰è¿”å›ç‰‡æ®µ
2. é”šç‚¹è¦æ±‚ï¼šç®€çŸ­ã€å‡†ç¡®ä¸”åœ¨å½“å‰æ–‡æœ¬ä¸­å”¯ä¸€å¯å®šä½
3. å¿…é¡»ä¸å½“å‰æ–‡æœ¬é€å­—ä¸€è‡´ï¼ŒåŒ…æ‹¬å¤§å°å†™ã€æ ‡ç‚¹ç¬¦å·ç­‰
4. ä¸ºæ¯ä¸ªç‰‡æ®µæ‰“åˆ†ï¼ˆrelevance_score: 0-10ï¼‰
5. åŒä¸€å—æœ€å¤šè¿”å›3ä¸ªç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§æ’åº

è¿”å›JSONæ ¼å¼ï¼š
{
  "segments": [
    {
      "relevance_score": <0-10çš„æ•°å­—>,
      "start_snippet": "<åœ¨å½“å‰æ–‡æœ¬ä¸­å­˜åœ¨çš„å¼€å¤´ç‰‡æ®µ>",
      "end_snippet": "<åœ¨å½“å‰æ–‡æœ¬ä¸­å­˜åœ¨çš„ç»“å°¾ç‰‡æ®µ>",
      "reasoning": "<ç®€çŸ­è¯´æ˜>"
    }
  ]
}

ğŸš¨ JSONæ ¼å¼è¦æ±‚ï¼š
1. å­—ç¬¦ä¸²ä¸­å¦‚æœ‰åŒå¼•å·(")ï¼Œå¿…é¡»è½¬ä¹‰ä¸º(\\")
2. å­—ç¬¦ä¸²ä¸­å¦‚æœ‰åæ–œæ (\\)ï¼Œå¿…é¡»è½¬ä¹‰ä¸º(\\\\)
3. ä¸¥æ ¼éµå¾ªJSONè¯­æ³•ï¼Œç¡®ä¿å¯æ­£ç¡®è§£æ
âš ï¸ å†æ¬¡å¼ºè°ƒï¼šç»å¯¹ä¸èƒ½è¿”å›å½“å‰æ–‡æœ¬å—ä¸­ä¸å­˜åœ¨çš„å†…å®¹ï¼""",
            "user": """æŸ¥è¯¢å†…å®¹ï¼š{src.query}

å½“å‰æ–‡æœ¬ç‰‡æ®µï¼ˆç¬¬{src.chunks[%batch_index%].chunk_id}å—ï¼Œè¡Œå·{src.chunks[%batch_index%].start_line}-{src.chunks[%batch_index%].end_line}ï¼‰ï¼š
```
{src.chunks[%batch_index%].content}
```

è¯·åœ¨è¯¥ç‰‡æ®µä¸­é€‰æ‹©ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„è¿ç»­æ–‡æœ¬ï¼Œè¿”å›ä¸è¶…è¿‡3ä¸ªç‰‡æ®µï¼Œä¸¥æ ¼æŒ‰æŒ‡å®šJSONè¿”å›ã€‚è‹¥æ— ç›¸å…³ï¼Œè¿”å› {\"segments\": []}ã€‚"""
        }
    
    async def parse_model_result(self, runtime_contexts: dict[str, Any], model_result: Any, batch_id: int | None = None) -> Any:
        """æ£€æŸ¥æ¨¡å‹è¿”å›çš„ç‰‡æ®µæ˜¯å¦åœ¨åŸæ–‡ä¸­å­˜åœ¨ä¸”é¡ºåºæ­£ç¡®"""
        parsed = await super().parse_model_result(runtime_contexts, model_result, batch_id)
            
        chunks = runtime_contexts.get("src", {}).get("chunks", [])
        if batch_id is None or batch_id >= len(chunks):
            raise ValueError("æ— æ•ˆæ‰¹å¤„ç†ID")
        
        # åªåœ¨å½“å‰chunkä¸­éªŒè¯ï¼Œä¸æ˜¯å…¨æ–‡
        chunk_content = chunks[batch_id].get("content", "")
        
        # è°ƒè¯•ä¿¡æ¯
        self.debug(f"batch_id: {batch_id}, chunksæ•°é‡: {len(chunks)}")
        self.debug(f"chunk_contenté•¿åº¦: {len(chunk_content)}")
        self.debug(f"chunk_contentå‰100å­—ç¬¦: {chunk_content[:100]}")
        
        valid = []
        
        for seg in parsed.get("segments", []):
            if not isinstance(seg, dict):
                continue
                
            start = str(seg.get("start_snippet", "")).strip()
            end = str(seg.get("end_snippet", "")).strip()
            score = float(seg.get("relevance_score", 0))
            
            if not start or not end or score < 3:
                continue
                
            # å¤§å°å†™ä¸æ•æ„Ÿçš„ç‰‡æ®µæ£€æŸ¥
            start_lower = start.lower()
            end_lower = end.lower()
            content_lower = chunk_content.lower()
            
            if start_lower not in content_lower:
                raise ValueError(f"å¼€å¤´ç‰‡æ®µåœ¨å½“å‰chunkä¸­ä¸å­˜åœ¨: '{start}'")
            if end_lower not in content_lower:
                raise ValueError(f"ç»“å°¾ç‰‡æ®µåœ¨å½“å‰chunkä¸­ä¸å­˜åœ¨: '{end}'")
                
            # ä½¿ç”¨åŸæ–‡è¿›è¡Œç²¾ç¡®å®šä½
            start_idx = chunk_content.lower().find(start_lower)
            end_idx = chunk_content.lower().find(end_lower) + len(end)

            if start_idx > end_idx:
                raise ValueError(f"å¼€å¤´ç‰‡æ®µä½ç½®({start_idx})ä¸èƒ½åœ¨ç»“å°¾ç‰‡æ®µä½ç½®({end_idx})ä¹‹å")
            
            valid.append({
                "relevance_score": score,
                "start_snippet": start,
                "end_snippet": end,
                "start_idx": start_idx,
                "end_idx": end_idx + len(end),
                "reasoning": str(seg.get("reasoning", ""))
            })
        
        return {"segments": valid[:3]}
    
    def adjust_prompt_after_failure(self, prompts, error_text, hint):
        """å¤±è´¥åè°ƒæ•´æç¤ºè¯ï¼Œå‘Šè¯‰æ¨¡å‹å…·ä½“çš„å¤±è´¥åŸå› """
        failure_hint = f"""

ğŸš¨ ä¸Šæ¬¡æå–å¤±è´¥ï¼š{error_text}

è¯·ä¸¥æ ¼æ£€æŸ¥ï¼š
1. ä½ è¿”å›çš„ start_snippet å’Œ end_snippet æ˜¯å¦éƒ½èƒ½åœ¨å½“å‰ç»™å®šçš„æ–‡æœ¬å—ä¸­æ‰¾åˆ°
2. ä¸è¦åŸºäºç¼–ç¨‹å¸¸è¯†æˆ–æ¨æµ‹æ·»åŠ ä»»ä½•ä¸å­˜åœ¨çš„ä»£ç è¡Œ
3. å¦‚æœå½“å‰æ–‡æœ¬å—ä¸­æ²¡æœ‰å®Œæ•´çš„ç›¸å…³ç‰‡æ®µï¼Œè¯·è¿”å›ç©ºåˆ—è¡¨ {{"segments": []}}
4. ä»”ç»†æ ¸å¯¹æ¯ä¸ªå­—ç¬¦ï¼ŒåŒ…æ‹¬å¤§å°å†™ã€æ ‡ç‚¹ç¬¦å·ã€ç©ºæ ¼

âš ï¸ åªæå–å½“å‰æ–‡æœ¬å—ä¸­å®é™…å­˜åœ¨çš„å†…å®¹ï¼"""
        
        return super().adjust_prompt_after_failure(prompts, error_text, failure_hint)
    
    async def post_process(self, source_context: Any, llm_result: Any, shared_context: dict[str, Any], 
                          extra_contexts: dict[str, Any], observer: list[tuple[Any, BaseAgent]], 
                          batch_id: int | None = None) -> tuple[Any, dict[str, dict | list]]:
        
        if batch_id is None or batch_id >= len(source_context.get("chunks", [])):
            return [], {}
        
        chunk = source_context["chunks"][batch_id]
        content = source_context.get("content", "")
        start_char = chunk.get("start_char", 0)
        
        segments = []
        for seg in llm_result.get("segments", []):
            start, end = seg["start_idx"], seg["end_idx"]
            global_start, global_end = start_char + start, start_char + end
            
            segments.append({
                "line_range": (content[:global_start].count('\n') + 1, content[:global_end].count('\n') + 1),
                "char_range": (global_start, global_end),
                "confidence": seg["relevance_score"],
                "text": chunk["content"][start:end],
            })
        
        return segments, {}
class MergeContentAgent(BaseAgent):
    """åˆå¹¶å’Œæ’åºè¯­ä¹‰æœç´¢ç»“æœ"""
    
    def setup(self):
        self.user_info = f"## ğŸ“ æ­£åœ¨åˆå¹¶æœç´¢ç»“æœ...\n"
        self.prompts = {}  # ä¸è°ƒç”¨æ¨¡å‹
        self.strict_mode = False
        
    async def post_process(self, source_context: Any, llm_result: Any, shared_context: dict[str, Any], 
                          extra_contexts: dict[str, Any], observer: list[tuple[Any, BaseAgent]], 
                          batch_id: int | None = None) -> tuple[Any, dict[str, dict | list]]:
        
        if not isinstance(source_context, list):
            return [], {}
            
        # ç®€åŒ–è¿‡æ»¤å’Œæ’åº
        valid = [
            r for r in source_context 
            if isinstance(r, dict) and r.get("text") and r.get("confidence", 0) >= 3
            and isinstance(r.get("line_range"), tuple) and isinstance(r.get("char_range"), tuple)
        ]
        
        valid.sort(key=lambda x: (x["confidence"], -x["char_range"][0]), reverse=True)
        
        self.debug(f"æœç´¢å®Œæˆï¼š{len(valid)}ä¸ªç›¸å…³ç‰‡æ®µ")
        
        return valid, {
            "memory_modify": {
                "search_results_count": len(valid),
                "total_processed_chunks": len(source_context),
                "valid_chunks": len(valid)
            }
        }
class SemanticSearchFlow(Flow):
    """è¯­ä¹‰æœç´¢å®Œæ•´æµç¨‹"""
    
    def __init__(self, name: str = "semantic_search", parent: "BaseAgent|None" = None, default_model: str = "gpt-4o-mini", **settings):
        # è®¾ç½®æ¨¡å‹é…ç½® - åœ¨settingsä¸­æ”¾å…¥é…ç½®å­—å…¸ï¼Œè€Œä¸æ˜¯ModelConfigå¯¹è±¡
        if "model_config" not in settings:
            settings["model_config"] = {
                "model_name": default_model,
                "is_stream": True
            }
        
        # ä¸éœ€è¦ç‰¹å®šçš„shared context keysï¼Œå› ä¸ºæˆ‘ä»¬ä¸»è¦é€šè¿‡argsä¼ é€’æ•°æ®
        super().__init__(name, parent, expected_shared_context_keys=set(), **settings)
        
        # æ·»åŠ è‡ªå®šä¹‰Agentç±»åˆ°Flowä¸­ï¼ˆä¸èµ°æ³¨å†Œç³»ç»Ÿï¼‰
        self.add_custom_agent_class("read_file_bootstrap", ReadFileContentBootstrapAgent)
        self.add_custom_agent_class("calculate_chunk", CalculateChunkSizeAgent)
        self.add_custom_agent_class("split_chunk", SplitChunkAgent)
        self.add_custom_agent_class("extract_content", ExtractRelatedContentAgent)
        self.add_custom_agent_class("merge_content", MergeContentAgent)
        
        # æ„å»ºæµç¨‹å›¾
        self.add_agent("read_file_bootstrap", alias="bootstrap")
        self.add_agent("calculate_chunk", alias="calculate")
        self.add_agent("split_chunk", alias="split")
        self.add_agent("extract_content", alias="extract")  # ä¸å†éœ€è¦ä¼ é€’default_model
        self.add_agent("merge_content", alias="merge")
        
        # æ„å»ºæµç¨‹è¾¹
        self.add_edge("bootstrap", "calculate")
        self.add_edge("calculate", "split") 
        self.add_edge("split", "extract")
        self.add_edge("extract", "merge")
        
        # è®¾ç½®å…¥å£èŠ‚ç‚¹
        self.entrance("bootstrap")