__package__ = "agent_os2.agent_os"
from .utility import modify_graph, update_shared_context, inherit_settings,record_user_info,get_context_value,merge_elements,parse_multi_string_context_embed,log_debug_info,parse_str_to_json_with_preprocess,format_debug_context,generate_log_dir,add_context_to_extra_contexts
from .base_model.builtin_config import ModelConfig
from uuid import uuid4 as random_uuid, UUID
from typing import Any,Callable
import asyncio
import datetime
class BaseAgent:
    alias:str
    uuid:UUID
    settings:dict[str,Any]
    is_debug:bool
    debug_cache:dict[int,str]
    debug_context:bool
    usage:dict[str,Any]
    stdout:Callable[[str,int|None,Any,str|None],None]
    stdin:Callable[[str],Any]
    parent:"BaseAgent|None"
    start_count:int
    source_context_collector:Any
    previous:set["BaseAgent"]
    after:set["BaseAgent"]
    prompts:str|dict[str,str]|None #æ ¹æ®å¤„ç†å™¨ä¸åŒï¼Œä¼ ä¸åŒçš„æç¤ºè¯æ¨¡æ¿
    model_config:ModelConfig
    strict_mode:bool
    retry_count:int
    batch_field:str
    user_info:str
    model_timeout:int
    def __init__(self,alias:str|None=None,parent:"BaseAgent|None"=None,**settings:dict[str,Any]):
        self.alias = alias or self.__class__.__name__[:-5]
        self.uuid = random_uuid()  # å…ˆç”Ÿæˆuuidï¼Œå› ä¸ºlog_dirå¯èƒ½éœ€è¦ç”¨åˆ°
        self.settings = settings
        if parent:
            inherit_settings(parent.settings,self.settings)
        self.settings["log_dir"] = generate_log_dir(self)
        self.is_debug = self.settings.get("is_debug",True)
        self.debug_cache = {}
        self.debug_context = self.settings.get("debug_context",False)
        self.usage = {}
        self.stdout = self.settings.get("stdout",lambda agent_uid,batch_id,info,tag:print(info,end=""))
        self.stdin = self.settings.get("stdin",input)
        self.start_count = 0
        self.parent = parent
        self.source_context_collector = None
        self.previous = set()
        self.after = set()
        self.prompts = {}
        self.model_config = ModelConfig.get_model_config(self.settings.get("model_config",{"model_name":"gemini-2.5-flash","is_stream":True}))
        self.strict_mode = False
        self.retry_count = 3
        self.batch_field = ""
        self.user_info = ""
        self.model_timeout = 60
    def __start__(self,*,src:"BaseAgent|None",source_context:Any,shared_context:dict[str,Any],
    current_queue:asyncio.Queue[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]]|None=None,observer:list[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]]|None=None,concurrent_limit:asyncio.Semaphore|None=None,**extra_contexts)->asyncio.Task[tuple[Any,bool]]|None:
        if observer is None:
            observer = []
        if current_queue is None:
            current_queue = asyncio.Queue()
        if self.start_count == -1:
            raise RuntimeError(f"Agent {self.alias} {self.uuid} å·²ç»è¢«æ‰§è¡Œè¿‡")
        self.source_context_collector = self.merge_source_context(source_context)
        self.start_count += 1
        if self.start_count >= len(self.previous):
            return asyncio.create_task(self._create_execution_task(shared_context,extra_contexts,current_queue,observer,concurrent_limit))
        return None
    async def _create_execution_task(self,shared_context:dict[str,Any],extra_contexts:dict[str,Any],current_queue:asyncio.Queue[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]],observer:list[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]],concurrent_limit:asyncio.Semaphore|None=None)->tuple[dict[str,Any],bool]:
        if concurrent_limit is None:
            return await self.__execute__(self.source_context_collector,shared_context,extra_contexts,current_queue,observer,concurrent_limit),len(self.after) == 0
        else:
            async with concurrent_limit:
                return await self.__execute__(self.source_context_collector,shared_context,extra_contexts,current_queue,observer,concurrent_limit),len(self.after) == 0
    async def _run_body_wrapper(self,source_context:Any,shared_context:dict[str,Any],extra_contexts:dict[str,Any],observer:list[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]],concurrent_limit:asyncio.Semaphore|None=None,batch_id:int|None=None)->tuple[Any,dict[str,dict|list]]:
        if concurrent_limit is None:
            return await self._run_agent_pipeline(source_context,shared_context,extra_contexts,observer,batch_id)
        else:
            async with concurrent_limit:
                return await self._run_agent_pipeline(source_context,shared_context,extra_contexts,observer,batch_id)
    async def __execute__(self,source_context:Any,shared_context:dict[str,Any],extra_contexts:dict[str,Any],current_queue:asyncio.Queue[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]],observer:list[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]],concurrent_limit:asyncio.Semaphore|None=None)->tuple[dict[str,Any],bool]:
        self.start_count = -1
        self.setup()
        result = None
        agent_command = {}
        if self.batch_field:
            actual_batch_content = await self.get_context_value(self.batch_field,{**extra_contexts, "src": source_context, "ctx": shared_context})
            if isinstance(actual_batch_content,list):
                gather_tasks = []
                for batch_id in range(len(actual_batch_content)):
                    gather_tasks.append(self._run_body_wrapper(source_context,shared_context,extra_contexts,observer,concurrent_limit,batch_id))
                results = await asyncio.gather(*gather_tasks)
                for chunk in results:
                    result = merge_elements(result,chunk[0],append_priority=True)
                    agent_command = merge_elements(agent_command,chunk[1],append_priority=True)
            else:
                raise ValueError(f"æ‰¹å¤„ç†å­—æ®µ{self.batch_field}çš„å€¼ä¸æ˜¯åˆ—è¡¨ï¼Œè€Œæ˜¯{type(actual_batch_content)}")
        else:
            result,agent_command = await self._run_agent_pipeline(source_context,shared_context,extra_contexts,observer)     
        #ç»Ÿä¸€æ‰§è¡Œå‰¯ä½œç”¨å‘½ä»¤
        await self.apply_command(agent_command,source_context,shared_context,extra_contexts)
        for agent in self.after:
            if (
                task := agent.__start__(src=self,source_context=result,shared_context=shared_context,current_queue=current_queue,observer=observer,concurrent_limit=concurrent_limit,**extra_contexts)
            ) is not None:
                await current_queue.put((task,agent))
                observer.append((task,agent))
        return result
    async def _run_agent_pipeline(self,source_context:Any,shared_context:dict[str,Any],extra_contexts:dict[str,Any],observer:list[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]],batch_id:int|None = None)->tuple[Any,dict[str,dict|list]]:
        record_user_info(self,f"## ğŸš€ **{self.alias}** å¯åŠ¨\n",batch_id)
        record_user_info(self,f"{self.user_info}\n",batch_id,tag="launch_tips")
        
        if batch_id is not None:
            self.debug(f"\n## Agent {self.alias} {self.uuid} [æ‰¹å¤„ç†id:{batch_id}]è°ƒè¯•ä¿¡æ¯ï¼š",batch_id)
        else:
            self.debug(f"\n## Agent {self.alias} {self.uuid} è°ƒè¯•ä¿¡æ¯ï¼š",batch_id)
            
        try:
            runtime_contexts = {**extra_contexts, "src": source_context, "ctx": shared_context}
            prompts = await parse_multi_string_context_embed(self.prompts, runtime_contexts, self.get_context_value, "", batch_id) if self.prompts else None
            start_time = datetime.datetime.now()
            self.debug(format_debug_context(source_context, not self.debug_context, "è·å¾—çš„ä¸Šæ¸¸å‚æ•°"),batch_id)
            self.debug(format_debug_context(shared_context, not self.debug_context, "å…±äº«å†…å­˜ä¸­çš„ä¸Šä¸‹æ–‡"),batch_id)
            self.debug(format_debug_context(extra_contexts, not self.debug_context, "é¢å¤–ä¸Šä¸‹æ–‡"),batch_id)
            llm_result = None
            if prompts:
                self.debug(f"### è¯¥Agentè°ƒç”¨æ¨¡å‹ï¼Œæ¨¡å‹é…ç½®ä¸ºï¼š\n```\n{self.model_config}\n```",batch_id)
                self.debug(f"### è¯¥Agentè°ƒç”¨æ¨¡å‹ï¼Œæ¨¡å‹åˆå§‹æç¤ºè¯ä¸ºï¼š\n```\n{prompts}\n```",batch_id)
                record_user_info(self,f"âŒ› æ­£åœ¨è°ƒç”¨æ¨¡å‹...\n",batch_id)
                llm_result, usage = await self._call_model(runtime_contexts,prompts,self.model_config,batch_id)
                self.usage = merge_elements(self.usage,usage,append_priority=True)
            else:
                self.debug(f"### è¯¥Agentæ²¡æœ‰è°ƒç”¨æ¨¡å‹ï¼Œæ²¡æœ‰llm_resultã€‚",batch_id)  
            final_result,agent_command = await self.post_process(source_context,llm_result,shared_context,extra_contexts,observer,batch_id)
            self.debug(f"### è¯¥Agentå‘ä¸‹æ¸¸å‘é€ç»“æœï¼š\n```\n{final_result}\n```",batch_id)
            if agent_command:
                self.debug(f"### è¯¥Agentè¿”å›äº†å‘½ä»¤ï¼Œå°†æ‰§è¡Œå‘½ä»¤ï¼š\n```\n{agent_command}\n```",batch_id)
            self.debug(f"### è¯¥Agentæ‰§è¡Œæ—¶é—´ï¼š{datetime.datetime.now() - start_time}",batch_id)
            record_user_info(self,f"âœ… **{self.alias}** æ‰§è¡Œç»“æŸ\n\n",batch_id)
            return final_result,agent_command
        except Exception as e:
            # ç®€åŒ–åçš„é”™è¯¯æç¤º
            record_user_info(self,f"âŒ **{self.alias}** æ‰§è¡Œå¤±è´¥\n\n",batch_id)
            self.debug(f"### è¯¥Agentè°ƒç”¨æ¨¡å‹å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š\n```\n{e}\n```",batch_id)
            raise e
        finally:
            if self.is_debug and "log_dir" in self.settings:
                log_debug_info(self.settings["log_dir"],self.debug_cache.get(batch_id or 0,""))
    async def _call_model(self,runtime_contexts:dict[str,Any],prompts:dict[str,str]|str,model_config:ModelConfig,batch_id:int|None=None)->tuple[Any,dict[str,Any]]:
        from .base_model.base_api import interact_with_model
        from .base_model.model_processor import StreamDataStatus
        
        result = None
        usage = {}
        retry_count = self.retry_count
        
        while retry_count > 0:
            logic_error = False
            error_data = None
            first_chunk_time = datetime.datetime.now()
            try:
                async with asyncio.timeout(self.model_timeout):
                    async for chunk in interact_with_model(prompts,model_config):
                        if first_chunk_time is not None:
                            self.debug(f"### è°ƒç”¨æ¨¡å‹çš„é¦–å­—æ—¶é—´å·®ï¼š{datetime.datetime.now() - first_chunk_time}ï¼Œæ˜¯å¦ä¸ºæµå¼è¾“å‡º:{model_config.is_stream()}",batch_id)
                            first_chunk_time = None
                        if chunk.get_status() == StreamDataStatus.GENERATING:
                            # ç®€åŒ–ååªæ˜¾ç¤ºï¼šæ¨¡å‹è¾“å‡º
                            record_user_info(self, chunk.read_data(), batch_id,tag="model_stream")
                        elif chunk.get_status() == StreamDataStatus.COMPLETED:
                            result = chunk.read_data()
                            if not model_config.is_stream():
                                record_user_info(self,result,batch_id,tag="model_stream")
                            # æ ¼å¼åŒ–è¾“å‡º
                            record_user_info(self,"\n", batch_id,tag="model_stream")
                            usage = merge_elements(usage,chunk.get_usage(),append_priority=True)
                            
                            if self.strict_mode:
                                try:
                                    result = await self.parse_model_result(runtime_contexts,result,batch_id)
                                except Exception as e:
                                    logic_error = True
                                    error_data = {"error":str(e),"original_model_result":result}
                            
                            self.debug(f"### è¯¥æ¨¡å‹æœ¬æ¬¡è°ƒç”¨ç»“æœä¸ºï¼š\n```\n{result}\n```",batch_id)
                            if usage:
                                self.debug(f"### è¯¥æ¨¡å‹æœ¬æ¬¡è°ƒç”¨æ¶ˆè€—tokensï¼š\n```\n{usage}\n```",batch_id)
                        elif chunk.get_status() == StreamDataStatus.ERROR:
                            error_data = chunk.read_data()
                            error_data = {"error":error_data}
            except asyncio.TimeoutError:
                error_data = {"error":f"æ¨¡å‹è°ƒç”¨è¶…æ—¶ï¼Œè¶…æ—¶æ—¶é—´ï¼š{self.model_timeout}ç§’"}
            if error_data:
                retry_count -= 1
                if retry_count > 0:
                    if logic_error:
                        prompts = self.adjust_prompt_after_failure(prompts,error_text=error_data["error"],hint = f"âš ï¸ æ³¨æ„ï¼šä½ åˆšæ‰çš„å›ç­”ä¸ç¬¦åˆæ ¼å¼è¦æ±‚ï¼Œé”™è¯¯ä¿¡æ¯ï¼š\n{error_data['error']}\nï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®š JSON æ ¼å¼ä½œç­”ï¼Œé¿å…è¿”å›æ— å…³å†…å®¹å¯¼è‡´è§£æå¤±è´¥ã€‚")
                        self.debug(f"### è¯¥Agentè°ƒç”¨æ¨¡å‹è¿”å›è¾“å‡ºä¸ç¬¦åˆè¦æ±‚ï¼Œå·²è°ƒæ•´æç¤ºè¯ä¸ºï¼š\n```\n{prompts}\n```\né”™è¯¯ä¿¡æ¯ï¼š\n```\n{error_data}\n```\nï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°ï¼š{retry_count}",batch_id)
                    else:
                        self.debug(f"### è¯¥Agentè°ƒç”¨æ¨¡å‹è¿”å›è¾“å‡ºä¸ç¬¦åˆè¦æ±‚ï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°ï¼š{retry_count}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š\n```\n{error_data}\n```",batch_id)
                    await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                else:
                    self.debug(f"### æ™ºèƒ½ä½“{self.alias}è°ƒç”¨æ¨¡å‹å¤±è´¥ï¼Œé‡è¯•æ¬¡æ•°å·²ç”¨å®Œã€‚\næœ€åä¸€æ¬¡é”™è¯¯ä¿¡æ¯ï¼š\n{error_data}",batch_id)
                    if self.is_debug:
                        raise RuntimeError(f"debug_error:æ¨¡å‹{self.alias} {self.uuid} è°ƒç”¨æ¨¡å‹å¤±è´¥ï¼Œerror_data: {error_data}")
                    return None,usage
            else:
                break
        return result, usage
    async def get_input(self,prompt:str,batch_id:int|None=None)->str:
        """
        è·å–ä½¿ç”¨è€…è¾“å…¥ï¼Œå¹¶è®°å½•åˆ°user_infoä¸­
        """
        record_user_info(self,prompt+"\n",batch_id,tag="input_prompt")
        return await asyncio.get_running_loop().run_in_executor(None, self.stdin, prompt)
    def debug(self,info:str,batch_id:int|None=None):
        self.debug_cache[batch_id or 0] = self.debug_cache.get(batch_id or 0,"") + info + "\n"
    async def parse_model_result(self,runtime_contexts:dict[str,Any],model_result:Any,batch_id:int|None=None)->Any:
        """
        ä»…å½“self.strict_modeä¸ºTrueæ—¶æœ‰æ•ˆ
        é»˜è®¤å°†æ¨¡å‹è¿”å›çš„å­—ç¬¦ä¸²è§£æä¸ºjsonæ ¼å¼ï¼Œå¹¶å¸¦æœ‰é¢„å¤„ç†æœºåˆ¶å’Œè¯¦ç»†çš„é”™è¯¯æç¤º
        å­ç±»å¯é€‰æ‹©æ€§é‡å†™ã€‚
        """
        if not isinstance(model_result,str):
            return model_result
        return parse_str_to_json_with_preprocess(model_result)
    def adjust_prompt_after_failure(self, prompts: dict[str, str]|str, error_text: str, hint: str) -> dict[str, str]|str:
        """
        ä»…å½“self.strict_modeä¸ºTrueæ—¶æœ‰æ•ˆ
        å½“æ¨¡å‹è¾“å‡ºé”™è¯¯æˆ–æ ¼å¼ä¸ç¬¦æ—¶ï¼Œå…è®¸ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯çš„è°ƒæ•´é€»è¾‘ã€‚
        é»˜è®¤å¯¹ OpenAI é£æ ¼æç¤ºè¯è¿›è¡Œæœ€å°è°ƒæ•´ï¼šåŠ å…¥â€œè¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¿”å›â€æç¤ºã€‚
        å­ç±»å¯ä»¥é€‰æ‹©æ€§é‡å†™ã€‚
        """
        if isinstance(prompts,str):
            if prompts.startswith("system:"):
                prompts = prompts[:7] + hint + "\n" + prompts[7:]
            else:
                prompts = prompts + "\n" + hint
        elif isinstance(prompts,dict):
            prompts["system"] = hint + "\n" + prompts.get("system","")
        return prompts
    def merge_source_context(self,source_context:Any)->Any:
        """è¯¥agenté¢å¯¹å¤šä¸ªä¸Šæ¸¸agentè¾“å…¥æ—¶ï¼Œåˆå¹¶å‚æ•°çš„ç­–ç•¥ï¼Œå­ç±»å¯é€‰æ‹©æ€§é‡å†™"""
        return merge_elements(self.source_context_collector,source_context,append_priority=True)
    async def apply_command(self,agent_command:dict[str,Any],source_context:Any,shared_context:dict[str,Any],extra_contexts:dict[str,Any]):
        """
        æ‰§è¡Œagent_commandä¸­çš„å‘½ä»¤ï¼Œå­ç±»å¯é‡å†™æ‰©å±•è‡ªå®šä¹‰å‘½ä»¤å¤„ç†
        é»˜è®¤å®ç°ï¼š
        - update_shared_context(): å¤„ç†memoryå‘½ä»¤
        - modify_graph(): å¤„ç†actionså‘½ä»¤
        """
        update_shared_context(agent_command.pop("memory",{}),shared_context) #å‘åå…¼å®¹ç‰ˆæœ¬,è­¦å‘Šâš å°†åœ¨1.2.6ç‰ˆæœ¬ä¹‹ååˆ é™¤
        update_shared_context(agent_command.pop("memory_modify",{}),shared_context)
        update_shared_context(agent_command.pop("memory_append",{}),shared_context,append_priority=True)
        if self.parent: #ä¸ºäº†å®‰å…¨æ€§å¿…é¡»åœ¨parentå­˜åœ¨çš„å‰æä¸‹ä¸‹åŠ¨æ€ä¿®æ”¹å›¾ç»“æ„
            modify_graph(self,agent_command.pop("actions",[]),self.parent)
        add_context_to_extra_contexts(agent_command.pop("add_context",{}),extra_contexts)
    async def get_context_value(self,key:str,runtime_contexts:dict[str,Any],default:Any="")->Any:
        """
        æ ¹æ®keyï¼Œè·å–ctxä¸­çš„å€¼,é»˜è®¤æ”¯æŒsrcå’Œctxä¸¤ä¸ªä¸Šä¸‹æ–‡çš„è·å–
        å­ç±»å¯é€‰æ‹©æ€§é‡å†™
        """
        if key.startswith("src."):
            return get_context_value(runtime_contexts["src"],key[4:],default) #ç›´æ¥æŠ›å‡ºKeyErroræ˜¯å¯ä»¥çš„ï¼Œparse_smart_string_indexå‘ç°sourceä¸å­˜åœ¨ä¼šå–æ¶ˆæ¨¡æ¿æ›¿æ¢
        elif key.startswith("ctx."):
            return get_context_value(runtime_contexts["ctx"],key[4:],default)
    def __str__(self)->str:
        return f"{self.alias}"
    def setup(self):
        """
        Agentå‚æ•°çš„åˆå§‹åŒ–è®¾ç½®
        å­ç±»åº”è¯¥é‡å†™è¯¥æ–¹æ³•ï¼Œå¯ä»¥è®¾ç½®ï¼š
        - self.user_info: ç”¨æˆ·å¯è§çš„åˆå§‹ä¿¡æ¯
        - self.is_debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        - self.model_config: æ¨¡å‹é…ç½®(é»˜è®¤æ˜¯Gemini25Flashçš„é»˜è®¤å®ç°)
        - self.prompts: æ¨¡å‹æç¤ºè¯æ¨¡æ¿
        - self.retry_count: é‡è¯•æ¬¡æ•°
        - self.batch_field: æ‰¹å¤„ç†å­—æ®µ
        - self.strict_mode: æ˜¯å¦å¯ç”¨ä¸¥æ ¼æ¨¡å¼
        - self.model_timeout: æ¨¡å‹è°ƒç”¨è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’
        - self.context_debug: æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡è°ƒè¯•æ¨¡å¼
        """
        self.prompts = {
        "user":"è¿™æ˜¯ä¸Šæ¸¸agentç»™ä½ å‘é€çš„æ¶ˆæ¯:{src.message}ï¼Œè¿™æ˜¯å…±äº«å†…å­˜ä¸­å¯¹æœ¬æ¬¡ä»»åŠ¡çš„ä»‹ç»{ctx.intro}",
        "system":"ä½ æ˜¯ä¸€ä¸ªåœ¨å·¥ä½œæµä¸­çš„æ™ºèƒ½ä½“ï¼Œè¯·ä½ æ ¹æ®æç¤ºè¦æ±‚ï¼Œæ¥ä¸¥è°¨çš„å®Œæˆä»»åŠ¡ã€‚"
        }
        self.batch_field = "" #æ‰¹å¤„ç†å¼€å…³ï¼ŒæŒ‡å‘ä¸€ä¸ªåˆ—è¡¨å­—æ®µ(å¦‚"src.items")ã€‚
        self.strict_mode = False #å¯ç”¨è§£ææ¡†æ¶ï¼Œè”åŠ¨è‡ªåŠ¨é‡è¯•
    async def post_process(self,source_context:Any,model_result:Any|None,shared_context:dict[str,Any],extra_contexts:dict[str,Any],observer:list[tuple[asyncio.Task[tuple[Any,bool]],"BaseAgent"]],batch_id:int|None=None)->tuple[Any,dict[str,dict|list]]:
        """
        å¯¹Agentçš„æ‰§è¡Œç»“æœè¿›è¡Œåç»­å¤„ç†ï¼Œå­ç±»åº”é‡å†™æ­¤æ–¹æ³•ä»¥å®ç°è‡ªå®šä¹‰ä¸šåŠ¡é€»è¾‘ã€‚
        
        è¿™æ˜¯Agentä¸­æœ€é‡è¦çš„â€œå‰¯ä½œç”¨â€å‡ºå£ï¼Œæ‰€æœ‰çŠ¶æ€å˜æ›´éƒ½åº”åœ¨æ­¤å¤„é›†ä¸­å¤„ç†ã€‚
        
        Returns:
            ä¸€ä¸ªå…ƒç»„ (final_result, agent_command):
            - final_result (Any): å°†ä¼ é€’ç»™ä¸‹æ¸¸Agentçš„æœ€ç»ˆç»“æœã€‚
            - agent_command (dict): å‘é€ç»™çˆ¶Flowçš„å‘½ä»¤ï¼Œç”¨äºåŠ¨æ€ä¿®æ”¹å·¥ä½œæµã€‚
                æ”¯æŒ `memory` (æ›´æ–°å…±äº«ä¸Šä¸‹æ–‡) å’Œ `actions` (ä¿®æ”¹å›¾ç»“æ„) å’Œ `add_context` (æ³¨å…¥æ–°çš„æ¥å£å¯¹è±¡) ç­‰å‘½ä»¤ã€‚
                è¯¦ç»†ç”¨æ³•è¯·å‚é˜… `DEVELOPING_GUIDE.md`ã€‚
        """
        return model_result,{}

if __name__ == "__main__":
    base_agent = BaseAgent(alias="test",parent=None)
    async def main():
        from .flow import execute
        print("\n\nresult:\n",await execute(base_agent,source_context={"message":"ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ ä¸‰ã€‚"},shared_context={"intro":"ä½ çš„ä»»åŠ¡æ˜¯è¦å¤è¿°ä¸€éä½ çš„ä»»åŠ¡å’Œä¸Šæ¸¸æ¶ˆæ¯çš„å†…å®¹ã€‚"}))
    asyncio.run(main())