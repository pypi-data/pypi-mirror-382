#
# coding: utf-8
# Copyright (C) pSeven SAS, 2010-present
#
from __future__ import division

import sys
import numpy as np
import itertools

from ..six import string_types

from .. import shared as _shared
from .. import exceptions as _ex
from .. import loggers as _loggers
from . import utilities as _utilities
from . import outlier_detection as _outlier_detection
from . import statistics as _statistics
from . import tests as _tests

class Analyzer(object):
  """
  Implements various statistical analysis methods.

  """

  def __init__(self):
    self.__logger = None
    self.__watcher = None

  def set_logger(self, logger):
    """Set logger.

    :param logger: logger object
    :return: none

    """
    self.__logger = _shared.wrap_with_exc_handler(logger, _ex.LoggerException)

  def set_watcher(self, watcher):
    """Set watcher.

    :param watcher: watcher object
    :return: none

    """
    # Note we keep this method for backward compatibility only. This watcher is not used any more.
    self.__watcher = _shared.wrap_with_exc_handler(watcher, _ex.WatcherException)


  @staticmethod
  def __process_single_sample(sample):
    try:
      normalizedSample, singleVector = _shared.as_matrix(sample, ret_is_vector=True, name="Data sample ('sample' argument)")
    except ValueError:
      exc_value, exc_tb = sys.exc_info()[1:]
      _shared.reraise(_ex.GTException, ('Invalid data sample: %s' % exc_value), exc_tb)

    # the data sample should not be empty
    if 0 == normalizedSample.size:
      raise _ex.GTException('The data sample is empty!')

    if singleVector:
      # always treat single vector as a single column matrix
      normalizedSample = normalizedSample.reshape(-1, 1)

    # the data sample should not contain NaN or Inf values
    if np.any(np.isnan(normalizedSample)) or np.any(np.isinf(normalizedSample)):
      raise _ex.GTException('The data sample contains at least one NaN or Inf value!')

    return normalizedSample


  def __log_message(self, level, message):
    if self.__logger:
      self.__logger(level, message)


  def check_distribution(self, sample, tests='all', confidence=0.99, budget=1000000000):
    """Check sample points on uniformity and normality.

    :param sample: data sample
    :type sample: :term:`array-like`
    :param tests: tests to be performed
    :type tests: ``'all'`` or ``'uniform|'normal_skewness'|'normal_kurtosis'`` or list of those
    :param confidence: confidence level for check of distribution.
                      Higher confidence means more strict limitations for the sample
                      to be considered as generated by the specific probability distribution.
    :type confidence: ``float``
    :param budget: maximum number points to be processed
    :type budget: ``int``
    :return: boolean results of tests
    :rtype: :class:`~da.p7core.stat.DistributionCheckResult`

    Checks data sample for specific types of distribution.
    See the :ref:`ug_stat_distribution_tests` section for details.

    """
    x = Analyzer.__process_single_sample(sample)

    try:
      confidence = float(confidence)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      _shared.reraise(_ex.GTException, 'Invalid confidence level: %s' % exc_value, exc_tb)
    if (confidence < 0.) or (confidence > 1.):
      raise _ex.GTException('Confidence level should be between 0 and 1 (confidence=%g given)' % confidence)

    try:
      budget = int(budget)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      _shared.reraise(_ex.GTException, 'Invalid budget value: %s' % exc_value, exc_tb)
    if budget <= 0:
      raise _ex.GTException('Maximum budget should be positive (budget=%d given)' % budget)

    uniformity_tests = ['uniform', 'all', 'discrepancy', 'minimum_spanning_tree']
    normality_tests = ['normal_skewness', 'normal_kurtosis', 'all']
    possible_tests = list(set(uniformity_tests + normality_tests))
    results_tests = ['uniform', 'normal_skewness', 'normal_kurtosis']

    try:
      tests = _utilities._get_standard_str_options(tests)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      _shared.reraise(_ex.GTException, 'Invalid list of the statistical tests: %s' % exc_value, exc_tb)

    if isinstance(tests, string_types):
      tests = np.array([tests])

    if not all([test in possible_tests for test in tests]):
      raise _ex.GTException('Tests specified %s are not from the list of possible tests %s', (tests, possible_tests))

    debug = _loggers.LogLevel.DEBUG
    self.__log_message(debug, "Following options were used for distribution tests: ")
    self.__log_message(debug, "tests=" + str(tests))
    self.__log_message(debug, "confidence=" + str(confidence))
    self.__log_message(debug, "budget=" + str(budget))

    if any(test in normality_tests for test in tests):
      self.__log_message(debug, "Test on normality is being performed")
      normality_test_result, are_features_strongly_correlated = _tests._check_normality(x, confidence, budget)
      if are_features_strongly_correlated == True:
        self.__log_message(_loggers.LogLevel.INFO, "Features are strongly correlated. It seems that data lays in the lower dimensional space. Try Dimension Reduction Tool for more information.")
      self.__log_message(debug, "Test on normality finished")
    else:
      normality_test_result = dict()

    uniformity_test_result = dict()
    for test in tests:
      if test in uniformity_tests:
        self.__log_message(debug, "Test on uniformity is being performed")
        uniformity_test_result = _tests._check_uniformity(x, confidence, test, budget)
        self.__log_message(debug, "Test on uniformity finished")
        break

    if ('discrepancy' in tests) or ('minimum_spanning_tree' in tests):
      tests = np.append(tests, 'uniform')

    test_result = dict(itertools.chain(uniformity_test_result.items(), normality_test_result.items()))

    if 'all' not in tests:
      test_result = dict([(test, test_result[test]) for test in np.intersect1d(tests, results_tests)])
      for test in np.intersect1d(tests, results_tests):
        self.__log_message(debug, "The result of test " + str(test) + " is " + str(test_result[test]))
    else:
      for test in results_tests:
        if test != 'all':
          self.__log_message(debug, "The result of test " + str(test) + " is " + str(test_result[test]))

    return _utilities.DistributionCheckResult(test_result)


  def calculate_statistics(self, sample, confidence=0.95, covariance='auto', rank_components=[]):
    """
    Calculates elementary statistics.

    :param sample: data sample
    :type sample: :term:`array-like`
    :param confidence: confidence level for the lower and upper quantiles [0.5, 1]
    :type confidence: ``float``
    :param covariance: type of covariance calculation for non-rank variables
    :type covariance: ``'empirical'|'robust'|'auto'``
    :param rank_components: indices of rank components. For rank variables calculate_statistics() computes the Kendall rank corrleation coefficient,
                            ignoring the calculation type set by covariance.
    :type rank_components: ``list[int]``
    :return: sample statistics
    :rtype: :class:`~da.p7core.stat.ElementaryStatistics`

    Calculates various statistics for the given data sample(s).
    See the :ref:`ug_stat_elementary_statistics` section for details.

    """

    train_data = Analyzer.__process_single_sample(sample)

    number_points = train_data.shape[0]

    if number_points <= 1:
      raise _ex.GTException('Train points must have at least two points')

    try:
      confidence = float(confidence)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      _shared.reraise(_ex.GTException, 'Invalid value of the confidence level: %s' % exc_value, exc_tb)
    if (confidence < 0.5) or (confidence > 1.):
      raise _ex.GTException('Confidence level must be between 0.5 and 1')

    possible_covariance_types = np.array(['auto', 'empirical', 'robust'])
    try:
      covariance = _utilities._get_standard_str_options(covariance)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      raise _ex.GTException('Invalid covariance type: %s' % exc_value, exc_tb)

    if covariance in possible_covariance_types:
      if covariance == 'auto':
        covariance = 'empirical'

      if covariance == 'empirical':
        is_robust = False
      else:
        is_robust = True
    else:
      raise _ex.GTException('Wrong value of covariance=%s , covariance could have one of following values %s', str(covariance), str(possible_covariance_types))

    try:
      rank_components = [int(rank_variable) for rank_variable in rank_components]
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      _shared.reraise(_ex.GTException, 'Invalid value of rank_components=%s: %s' % (str(rank_components), exc_value), exc_tb)

    if len(rank_components) > 0:
      if (np.min(rank_components) < 0) or (max(rank_components) > len(train_data) - 1):
        raise _ex.GTException('Wrong value of rank_components=%s , every component of rank_components should be in range (1, %s)', str(rank_components), str(len(train_data)))

    is_rank = [False] * len(train_data[0])
    for index in rank_components:
      is_rank[index] = True

    debug = _loggers.LogLevel.DEBUG
    self.__log_message(debug, "Following options were used for statistics computation: ")
    self.__log_message(debug, "confidence=" + str(confidence))
    self.__log_message(debug, "covariance=" + covariance)
    self.__log_message(debug, "rank_components=" + str(rank_components))

    self.__log_message(debug, "Calculation of statistics is being performed")
    statistics = _statistics._calculate_elementary_statistics(train_data, confidence, is_robust, is_rank)
    self.__log_message(debug, "Calculation of statistics finished")

    return _utilities.ElementaryStatistics(statistics)


  def detect_outliers(self, sample, covariance='auto', score_type='auto', confidence=0.95):
    """Finds outliers in data

    :param sample: data sample
    :type sample: ``array-like``
    :param covariance: type of covariance calculation
    :type covariance: ``'empirical'|'robust'|'auto'``
    :param score_type: determines which type of scores should be computed
    :type score_type: ``'probability'|'distance'|'auto'``
    :param confidence: real value from [0, 1] which means fraction of objects in the sample which will be considered as inliers (opposite to outliers)
    :type confidence: ``float``
    :return: scores for objects to be an outlier and corresponding decisions
    :rtype: :class:`~da.p7core.stat.OutlierDetectionResult`

    Detects outliers in the given data sample.
    See section :ref:`ug_stat_outlier_detection` for details.

    """
    x = Analyzer.__process_single_sample(sample)

    possible_covariance_types = np.array(['auto', 'empirical', 'robust'])
    try:
      covariance = _utilities._get_standard_str_options(covariance)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      raise _ex.GTException('Invalid covariance type: %s' % exc_value, exc_tb)

    if covariance in possible_covariance_types:
      if covariance == 'auto':
        covariance = 'empirical'

      if covariance == 'empirical':
        is_robust = False
      else:
        is_robust = True
    else:
      raise _ex.GTException('Wrong value of covariance=%s , covariance could have one of following values %s', str(covariance), str(possible_covariance_types))

    possible_score_types = np.array(['auto', 'probability', 'distance'])
    try:
      score_type = _utilities._get_standard_str_options(score_type)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      raise _ex.GTException('Invalid score type: %s' % exc_value, exc_tb)

      raise _ex.GTException('Wrong value of score_type: could not convert it to string')

    if score_type in possible_score_types:
      if score_type == 'auto':
        score_type = 'distance'

      if score_type == 'distance':
        is_outlier_probabilities = False
      else:
        is_outlier_probabilities = True
    else:
      raise _ex.GTException('Wrong value of covariance=%s , covariance could have one of following values %s', str(score_type), str(possible_score_types))

    try:
      confidence = float(confidence)
    except:
      exc_value, exc_tb = sys.exc_info()[1:]
      raise _ex.GTException('Invalid confidence level: %s' % exc_value, exc_tb)

    if (confidence < 0.) or (confidence > 1.):
      raise _ex.GTException('Confidence level should be between 0 and 1')

    debug = _loggers.LogLevel.DEBUG
    self.__log_message(debug, "Following options were used for outlier detection: ")
    self.__log_message(debug, "covariance=" + covariance)
    self.__log_message(debug, "score_type=" + score_type)
    self.__log_message(debug, "confidence=" + str(confidence))

    self.__log_message(debug, "Calculation of outliers is being performed")
    if is_outlier_probabilities:
      scores = _outlier_detection._calculate_outlier_probabilities(x, is_robust)
    else:
      scores = _outlier_detection._calculate_deviation_scores(x, is_robust)

    outliers = _utilities._compute_outliers(scores, confidence)
    self.__log_message(debug, "Calculation of outliers finished")

    return _utilities.OutlierDetectionResult(scores, outliers)
