# Copyright Â© 2025 Oracle and/or its affiliates.
#
# This software is under the Universal Permissive License
# (UPL) 1.0 (LICENSE-UPL or https://oss.oracle.com/licenses/upl) or Apache License
# 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0), at your option.

import logging
from abc import ABC, abstractmethod
from collections import defaultdict
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, AsyncIterable, Dict, Iterable, List, Optional, Union

from wayflowcore._metadata import MetadataType
from wayflowcore._utils.async_helpers import async_to_sync_iterator, run_async_in_sync
from wayflowcore.component import Component
from wayflowcore.executors._events.event import _TokenConsumptionEvent
from wayflowcore.idgeneration import IdGenerator
from wayflowcore.property import Property
from wayflowcore.serialization.context import DeserializationContext, SerializationContext
from wayflowcore.serialization.serializer import SerializableDataclassMixin, SerializableObject
from wayflowcore.tokenusage import TokenUsage
from wayflowcore.tools import Tool

from ._requesthelpers import (
    StreamChunkType,
    TaggedMessageChunkType,
    TaggedMessageChunkTypeWithTokenUsage,
)
from .llmgenerationconfig import LlmGenerationConfig
from .tokenusagehelpers import (
    _get_approximate_num_token_from_wayflowcore_list_of_messages,
    _get_approximate_num_token_from_wayflowcore_message,
)

if TYPE_CHECKING:
    from wayflowcore.conversation import Conversation
    from wayflowcore.messagelist import Message
    from wayflowcore.outputparser import OutputParser
    from wayflowcore.templates import PromptTemplate


logger = logging.getLogger("wayflowcore")


@dataclass
class LlmCompletion(SerializableDataclassMixin, SerializableObject):
    message: "Message"
    """Message generated by the LLM"""
    token_usage: Optional[TokenUsage]
    """Token usage for this completion"""


@dataclass
class Prompt(SerializableDataclassMixin, SerializableObject):
    """Dataclass containing all information needed for LLM completion + potential post-processing"""

    messages: List["Message"]
    """List of messages to use for chat generation."""
    tools: Optional[List[Tool]] = None
    """Optional tools to use for native tool calling."""
    response_format: Optional[Property] = None
    """Optional response format to use for structured generation."""
    output_parser: Optional[Union["OutputParser", List["OutputParser"]]] = None
    """Optional parser to transform the raw output of the LLM."""
    generation_config: Optional[LlmGenerationConfig] = None
    """Optional parameters for the llm generation."""

    def parse_output(self, message: "Message") -> "Message":
        if self.output_parser is None:
            return message
        if isinstance(self.output_parser, list):
            processed_message = message
            for output_parser in self.output_parser:
                processed_message = output_parser.parse_output(processed_message)
            return processed_message
        return self.output_parser.parse_output(message)

    def copy(self, **kwargs: Any) -> "Prompt":
        """Makes a copy of the prompt and changes some given attributes."""
        self_dict = dict(
            messages=[m.copy() for m in self.messages],  # to avoid side-effects
            tools=[*self.tools] if self.tools is not None else None,  # to avoid side-effects
            response_format=self.response_format,
            output_parser=self.output_parser,
            generation_config=self.generation_config,
        )
        self_dict.update(kwargs)
        return Prompt(**self_dict)  # type: ignore


class LlmModel(Component, SerializableObject, ABC):
    def __init__(
        self,
        model_id: str,
        generation_config: Optional[LlmGenerationConfig],
        chat_template: Optional["PromptTemplate"] = None,
        agent_template: Optional["PromptTemplate"] = None,
        supports_structured_generation: Optional[bool] = None,
        supports_tool_calling: Optional[bool] = None,
        __metadata_info__: Optional[MetadataType] = None,
        id: Optional[str] = None,
        name: Optional[str] = None,
        description: Optional[str] = None,
    ):
        """
        Base class for LLM models.

        Parameters
        ----------
        model_id:
            ID of the model.
        generation_config:
            Parameters for LLM generation.
        chat_template:
            Default template for chat completion.
        agent_template:
            Default template for agents using this model.
        supports_structured_generation:
            Whether the model supports structured generation or not. When set to `None`,
            the model will be prompted with a response format and it will check it can use
            structured generation.
        supports_tool_calling:
            Whether the model supports tool calling or not. When set to `None`,
            the model will be prompted with a tool and it will check it can use
            the tool.
        id:
            ID of the component.
        name:
            Name of the component.
        description:
            Description of the component.
        """
        super().__init__(
            name=IdGenerator.get_or_generate_name(name, prefix="llm_", length=8),
            description=description,
            id=id,
            __metadata_info__=__metadata_info__,
        )

        self.model_id = model_id
        self.generation_config = generation_config

        # count tokens per conversation, per step (first key is conversation, second is step)
        self.token_usages_flow: Dict[str, Dict[str, TokenUsage]] = defaultdict(
            lambda: defaultdict(lambda: TokenUsage(exact_count=True)),
        )
        self.token_usages_flexible: Dict[str, TokenUsage] = defaultdict(
            lambda: TokenUsage(exact_count=True)
        )
        self.token_usage_standalone: TokenUsage = TokenUsage(exact_count=True)

        self.agent_template = agent_template or self.default_agent_template
        self.chat_template = chat_template or self.default_chat_template

        from ._modelhelpers import _fetch_structured_generation_support, _fetch_tool_calling_support

        self.supports_structured_generation = (
            supports_structured_generation
            if supports_structured_generation is not None
            else _fetch_structured_generation_support(self)
        )
        self.supports_tool_calling = (
            supports_tool_calling
            if supports_tool_calling is not None
            else _fetch_tool_calling_support(self)
        )

    @property
    def default_chat_template(self) -> "PromptTemplate":
        from wayflowcore.templates import NATIVE_CHAT_TEMPLATE

        return NATIVE_CHAT_TEMPLATE

    @property
    def default_agent_template(self) -> "PromptTemplate":
        from wayflowcore.templates import NATIVE_AGENT_TEMPLATE

        return NATIVE_AGENT_TEMPLATE

    def generate(
        self, prompt: Union[str, Prompt], _conversation: Optional["Conversation"] = None
    ) -> LlmCompletion:
        """
        Generates a new message based on a prompt using a LLM

        Parameters
        ----------
        prompt:
            Prompt that contains the messages and other arguments to send to the LLM

        Examples
        --------
        >>> from wayflowcore.messagelist import Message
        >>> from wayflowcore.models import Prompt
        >>> prompt = Prompt(messages=[Message('What is the capital of Switzerland?')])
        >>> completion = llm.generate(prompt)
        >>> # LlmCompletion(message=Message(content='The capital of Switzerland is Bern'))

        """
        return run_async_in_sync(
            self.generate_async, prompt, _conversation, method_name="generate_async"
        )

    async def generate_async(
        self, prompt: Union[str, Prompt], _conversation: Optional["Conversation"] = None
    ) -> LlmCompletion:
        from wayflowcore.tracing.span import LlmGenerationSpan

        if not isinstance(prompt, Prompt):
            from wayflowcore.templates import PromptTemplate

            prompt = PromptTemplate.from_string(prompt).format()

        # add model default values if no generation config is passed
        if prompt.generation_config is None:
            prompt = prompt.copy(generation_config=self.generation_config)

        self._check_supports_prompt(prompt)

        with LlmGenerationSpan(llm=self, prompt=prompt) as span:
            logger.debug("LLM generating: %s", prompt)
            completion = await self._generate_impl(prompt)
            logger.debug("LLM output: %s", completion.message)
            self._update_token_usage(
                conversation=_conversation, prompt=prompt, completion=completion
            )
            span.record_end_span_event(completion=completion)
        return completion

    def stream_generate(
        self,
        prompt: Union[str, Prompt],
        _conversation: Optional["Conversation"] = None,
    ) -> Iterable[TaggedMessageChunkType]:
        for chunk in async_to_sync_iterator(self.stream_generate_async(prompt, _conversation)):
            yield chunk

    async def stream_generate_async(
        self,
        prompt: Union[str, Prompt],
        _conversation: Optional["Conversation"] = None,
    ) -> AsyncIterable[TaggedMessageChunkType]:
        """
        Returns an async iterator of message chunks

        Parameters
        ----------
        prompt:
            Prompt that contains the messages and other arguments to send to the LLM

        Examples
        --------
        >>> import asyncio
        >>> from wayflowcore.messagelist import Message, MessageType
        >>> from wayflowcore.models import Prompt
        >>> message = Message(content="What is the capital of Switzerland?", message_type=MessageType.USER)
        >>> llm_stream = llm.stream_generate(
        ...     prompt=Prompt(messages=[message])
        ... )
        >>> for chunk_type, chunk in llm_stream:
        ...     print(chunk)   # doctest: +SKIP
        >>> # Bern
        >>> #  is the
        >>> # capital
        >>> #  of
        >>> #  Switzerland
        >>> # Message(content='Bern is the capital of Switzerland', message_type=MessageType.AGENT)
        """
        from wayflowcore.tracing.span import LlmGenerationSpan

        if isinstance(prompt, str):
            from wayflowcore.templates import PromptTemplate

            prompt = PromptTemplate.from_string(prompt).format()

        # add model default values if no generation config is passed
        if prompt.generation_config is None:
            prompt = prompt.copy(generation_config=self.generation_config)

        self._check_supports_prompt(prompt)

        with LlmGenerationSpan(llm=self, prompt=prompt) as span:
            logger.debug("LLM generating: %s", prompt)
            final_chunk: Optional["Message"] = None
            async for chunk_type, chunk, token_usage in self._stream_generate_impl(prompt):
                if chunk_type == StreamChunkType.END_CHUNK:
                    final_chunk = chunk
                yield chunk_type, chunk

            if final_chunk is None:
                raise ValueError("No end chunk was streamed")

            completion_chunk = LlmCompletion(message=final_chunk, token_usage=token_usage)
            self._update_token_usage(
                conversation=_conversation, prompt=prompt, completion=completion_chunk
            )
            span.record_end_span_event(completion=completion_chunk)

    def _update_token_usage(
        self,
        conversation: Optional["Conversation"],
        prompt: Prompt,
        completion: LlmCompletion,
    ) -> None:
        if completion.token_usage is None:
            num_prompt_tokens = _get_approximate_num_token_from_wayflowcore_list_of_messages(
                prompt.messages, prompt.tools
            )
            num_completion_tokens = _get_approximate_num_token_from_wayflowcore_message(
                completion.message
            )
            completion.token_usage = TokenUsage(
                input_tokens=num_prompt_tokens,
                output_tokens=num_completion_tokens,
                total_tokens=num_prompt_tokens + num_completion_tokens,
                exact_count=False,
            )

        token_usage = completion.token_usage

        logger.debug(
            f"LLM generation finished with {token_usage.input_tokens} input tokens and {token_usage.output_tokens} generated tokens"
        )

        if conversation is None:
            # generate was standalone
            self.token_usage_standalone += completion.token_usage
            return

        conversation.token_usage += token_usage
        # for now add to private list of events
        conversation._register_event(_TokenConsumptionEvent(token_usage=token_usage))

        from wayflowcore.executors._flowconversation import FlowConversation

        if isinstance(conversation, FlowConversation):
            # generate with flow
            self.token_usages_flow[conversation.conversation_id][
                conversation.current_step_name
            ] += token_usage
        else:
            self.token_usages_flexible[conversation.conversation_id] += token_usage

    def get_total_token_consumption(self, conversation_id: str) -> TokenUsage:
        """Calculate and return the total token consumption for a given conversation.

        This method computes the aggregate token usage for the specified conversation
        by summing the token usages.

        Parameters
        ----------
        conversation_id:
            The unique identifier for the conversation whose token consumption is to be calculated.

        Returns
        -------
            A TokenUsage object that gathers all token usage information.

        See Also
        --------
        :class:`~wayflowcore.models.tokenusage.TokenUsage` : An object to gather all token usage information.

        """
        total_token_usage = TokenUsage(exact_count=True)
        if conversation_id in self.token_usages_flexible:
            total_token_usage += self.token_usages_flexible[conversation_id]
        if conversation_id in self.token_usages_flow:
            for step_name, token_usage in self.token_usages_flow[conversation_id].items():
                total_token_usage += token_usage
        return total_token_usage

    @abstractmethod
    async def _generate_impl(self, prompt: Prompt) -> LlmCompletion: ...

    @abstractmethod
    async def _stream_generate_impl(
        self, prompt: Prompt
    ) -> AsyncIterable[TaggedMessageChunkTypeWithTokenUsage]:
        # need an implementation otherwise mypy will complain
        yield None  # type: ignore
        raise NotImplementedError()

    @property
    @abstractmethod
    def config(self) -> Dict[str, Any]:
        """Get the configuration dictionary for the {VLlm/OpenAI/...} model"""
        raise NotImplementedError("Abstract methods from abstract classes should not be called")

    def _serialize_to_dict(self, serialization_context: SerializationContext) -> Dict[str, Any]:
        return {
            **self.config,
            "_component_type": LlmModel.__name__,
            "id": self.id,
            "name": self.name,
            "description": self.description,
        }

    @classmethod
    def _deserialize_from_dict(
        cls, input_dict: Dict[str, Any], deserialization_context: DeserializationContext
    ) -> "SerializableObject":
        from wayflowcore.models.llmmodelfactory import LlmModelFactory

        return LlmModelFactory.from_config(input_dict)

    def _check_supports_prompt(self, prompt: Prompt) -> None:
        if prompt.tools is not None and not self.supports_tool_calling:
            raise ValueError(
                f"The LLM model `{self.model_id}` doesn't support tool calling, but got tools: {prompt.tools}"
            )
        if prompt.response_format is not None and not self.supports_structured_generation:
            raise ValueError(
                f"The LLM model `{self.model_id}` doesn't support structured generation, but got response format: {prompt.response_format}"
            )
