# Copyright Â© 2025 Oracle and/or its affiliates.
#
# This software is under the Universal Permissive License
# (UPL) 1.0 (LICENSE-UPL or https://oss.oracle.com/licenses/upl) or Apache License
# 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0), at your option.

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Dict, List, Optional

from wayflowcore._metadata import MetadataType
from wayflowcore.executors.interrupts.executioninterrupt import (
    FlexibleExecutionInterrupt,
    FlowExecutionInterrupt,
    InterruptedExecutionStatus,
    _AllEventsInterruptMixin,
)
from wayflowcore.serialization.context import DeserializationContext, SerializationContext
from wayflowcore.serialization.serializer import SerializableObject

if TYPE_CHECKING:
    from wayflowcore.conversation import Conversation
    from wayflowcore.executors._executionstate import ConversationExecutionState
    from wayflowcore.models.llmmodel import LlmModel


class SoftTokenLimitExecutionInterrupt(
    _AllEventsInterruptMixin,
    FlexibleExecutionInterrupt,
    FlowExecutionInterrupt,
    SerializableObject,
):

    def __init__(
        self,
        tokens_per_model: Optional[Dict[LlmModel, int]] = None,
        all_models: Optional[List[LlmModel]] = None,
        total_tokens: Optional[int] = None,
        __metadata_info__: Optional[MetadataType] = None,
    ):
        """
        Execution interrupt that stops the assistant's execution after a given token limit for an LLM is reached.
        The interrupt counts the number of tokens generated by a given LLM object during the whole conversation.
        It stops the execution if:

        - Summing the tokens generated by the given list of LLMs, the global number of tokens is reached
        - One of the LLMs reached a given token limit

        This is a soft limit, as it does not force the interruption of the execution at any time.
        For example:
        - It does not interrupt the execution of a step (except for ``FlowExecutionStep``)
        - It does not interrupt the execution of a tool
        - It does not interrupt LLM models during generation

        Parameters
        ----------
        tokens_per_model:
            Dictionary containing, for each model that has a limit, the maximum number of tokens
            it is allowed to generate.
        all_models:
            List of all the LLMs that should be counted for the global token limit.
            When a value for this parameter is provided, also `total_tokens` should be specified.
        total_tokens:
            Maximum number of tokens that should be globally generated by the LLMs in ``all_models``.
            When a value for this parameter is provided, also ``all_models`` should be specified.

        Example
        -------
        >>> from wayflowcore.agent import Agent
        >>> from wayflowcore.executors.interrupts.tokenlimitexecutioninterrupt import SoftTokenLimitExecutionInterrupt
        >>> from wayflowcore.executors.executionstatus import ExecutionStatus
        >>> from wayflowcore.executors.interrupts.executioninterrupt import InterruptedExecutionStatus
        >>> from wayflowcore.models.llmmodelfactory import LlmModelFactory
        >>> VLLM_CONFIG = {"model_type": "vllm", "host_port": LLAMA70B_API_ENDPOINT, "model_id": "/storage/models/Llama-3.1-70B-Instruct",}
        >>> llm = LlmModelFactory.from_config(VLLM_CONFIG)
        >>> assistant = Agent(llm=llm, custom_instruction="You are a helpful assistant")
        >>> conversation = assistant.start_conversation()
        >>> conversation.append_user_message("Tell me something interesting")
        >>> # With the following configuration we set a 100 tokens limit on the `llm` model
        >>> token_limit_interrupt = SoftTokenLimitExecutionInterrupt(tokens_per_model={llm: 100})
        >>> status = conversation.execute(execution_interrupts=[token_limit_interrupt])

        The following configuration is equivalent to the above:

        >>> conversation = assistant.start_conversation()
        >>> conversation.append_user_message("Tell me something interesting")
        >>> token_limit_interrupt = SoftTokenLimitExecutionInterrupt(all_models=[llm], total_tokens=100)
        >>> status = conversation.execute(execution_interrupts=[token_limit_interrupt])
        >>> isinstance(status, ExecutionStatus) or isinstance(status, InterruptedExecutionStatus)
        True

        """
        if tokens_per_model is None and total_tokens is None:
            raise ValueError(
                "One limit between per model, or total number of tokens must be provided"
            )
        if (
            all_models is None
            and total_tokens is not None
            or all_models is not None
            and total_tokens is None
        ):
            raise ValueError("Both the list of models and the global token limit must be provided")
        self.total_tokens = total_tokens
        self.tokens_per_model = tokens_per_model or dict()
        self.all_models = all_models or list()

        super().__init__(__metadata_info__=__metadata_info__)

    def _get_token_usage_from_llm(self, llm: LlmModel, conversation_id: str) -> int:
        return (
            sum(
                token_usage.output_tokens
                for token_usage in llm.token_usages_flow[conversation_id].values()
            )
            if conversation_id in llm.token_usages_flow
            else 0
        ) + (
            llm.token_usages_flexible[conversation_id].output_tokens
            if conversation_id in llm.token_usages_flexible
            else 0
        )

    def _return_status_if_condition_is_met(
        self, state: ConversationExecutionState, conversation: "Conversation"
    ) -> Optional[InterruptedExecutionStatus]:

        conversation_id = conversation.conversation_id

        # We first check the global token limit, then we go over the llm-wise limits
        # Note that we must do the checks separately, because the list of all models
        # and the models in the tokens_per_model dictionary might not be equal
        if self.total_tokens is not None:
            total_tokens = 0
            for llm in self.all_models:
                tokens_generated = self._get_token_usage_from_llm(llm, conversation_id)
                total_tokens += tokens_generated
                if 0 <= self.total_tokens < total_tokens:
                    return InterruptedExecutionStatus(self, "Global token limit reached")

        for llm, max_tokens in self.tokens_per_model.items():
            tokens_generated = self._get_token_usage_from_llm(llm, conversation_id)
            if tokens_generated > max_tokens:
                return InterruptedExecutionStatus(
                    self, f"Token limit reached for model {llm.model_id}"
                )

        return None

    def _serialize_to_dict(self, serialization_context: "SerializationContext") -> Dict[str, Any]:
        from wayflowcore.serialization.serializer import serialize_to_dict

        return {
            "total_tokens": self.total_tokens,
            "all_models": [
                serialize_to_dict(llm, serialization_context) for llm in self.all_models
            ],
            "tokens_per_model": [
                [serialize_to_dict(llm, serialization_context), token_per_model]
                for llm, token_per_model in self.tokens_per_model.items()
            ],
        }

    @classmethod
    def _deserialize_from_dict(
        cls, input_dict: Dict[str, Any], deserialization_context: "DeserializationContext"
    ) -> "SerializableObject":
        from wayflowcore.models.llmmodel import LlmModel
        from wayflowcore.serialization.serializer import deserialize_from_dict

        return SoftTokenLimitExecutionInterrupt(
            total_tokens=input_dict["total_tokens"],
            all_models=[
                deserialize_from_dict(LlmModel, llm_dict, deserialization_context)
                for llm_dict in input_dict["all_models"]
            ],
            tokens_per_model={
                deserialize_from_dict(LlmModel, llm_dict, deserialization_context): token_per_model
                for llm_dict, token_per_model in input_dict["tokens_per_model"]
            },
        )
