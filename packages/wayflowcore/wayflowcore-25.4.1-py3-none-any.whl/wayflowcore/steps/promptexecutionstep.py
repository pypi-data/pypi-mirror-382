# Copyright Â© 2025 Oracle and/or its affiliates.
#
# This software is under the Universal Permissive License
# (UPL) 1.0 (LICENSE-UPL or https://oss.oracle.com/licenses/upl) or Apache License
# 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0), at your option.
import json
import logging
from dataclasses import replace
from enum import Enum
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Set, Tuple, Union

from wayflowcore._metadata import MetadataType
from wayflowcore._utils._templating_helpers import MessageAsDictT, check_template_validity
from wayflowcore.messagelist import Message, MessageList, MessageType
from wayflowcore.models.llmgenerationconfig import LlmGenerationConfig
from wayflowcore.models.llmmodel import LlmModel
from wayflowcore.property import (
    ObjectProperty,
    Property,
    StringProperty,
    _format_default_value,
    _output_properties_to_response_format_property,
)
from wayflowcore.steps.step import Step, StepResult
from wayflowcore.steps.templaterenderingstep import TemplateRenderingStep
from wayflowcore.templates import PromptTemplate
from wayflowcore.templates.structuredgeneration import (
    _system_prompt_to_json_generation_prompt_template,
)
from wayflowcore.tools.tools import _convert_list_of_properties_to_tool

if TYPE_CHECKING:
    from wayflowcore.executors._flowconversation import FlowConversation

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from wayflowcore import Tool


class StructuredGenerationMode(Enum):
    """Method to perform constrained generation"""

    CONSTRAINED_GENERATION = "constrained_generation"
    """Mode that uses constrained generation to perform structured generation. The LLM will be forced to answer directly and won't be able to reason first for most models, which might improve the latency but reduce accuracy."""
    TOOL_GENERATION = "tool_generation"
    """Mode that uses a generate tool that has expected outputs as arguments. The llm answers will be a tool request that is parsed into the expected outputs. Depending on the model, it might be able to reason first, increasing the accuracy but worsening the latency."""
    JSON_GENERATION = "json_generation"
    """Mode that adds to the prompt the json_schema and asks to generate a JSON. The raw answer is then parsed using an ``ExtractValueFromJsonStep``. With this mode, the LLM will be able to reason first before giving its final output, increasing the accuracy but worsening the latency."""


class PromptExecutionStep(Step):
    """Step to generate text using an LLM and a prompt template."""

    _output_descriptors_change_step_behavior = (
        True  # we do structured generation when some output descriptors are passed
    )

    OUTPUT = "output"
    """str: Output key for the output generated by the LLM."""

    JSON_CONSTRAINED_GENERATION_PROMPT = "At the end of your answer, finish with <final_answer>$your_answer$</final_answer> with $your_answer$ being a properly formatted json that is valid against this JSON schema:"
    """str: Additional prompt that is appended to the prompt to ask the LLM to generate a json that respects a particular JSON Schema"""

    def __init__(
        self,
        prompt_template: Union[str, PromptTemplate],
        llm: LlmModel,
        generation_config: Optional[LlmGenerationConfig] = None,
        _structured_generation_mode: StructuredGenerationMode = StructuredGenerationMode.JSON_GENERATION,
        send_message: bool = False,
        input_descriptors: Optional[List[Property]] = None,
        output_descriptors: Optional[List[Property]] = None,
        input_mapping: Optional[Dict[str, str]] = None,
        output_mapping: Optional[Dict[str, str]] = None,
        name: Optional[str] = None,
        __metadata_info__: Optional[MetadataType] = None,
    ):
        """
        Note
        ----

        A step has input and output descriptors, describing what values the step requires to run and what values it produces.

        **Input descriptors**

        By default, when ``input_descriptors`` is set to ``None``, the input_descriptors will be automatically inferred
        from the ``prompt_template``, with one input descriptor per variable in the template, trying to detect
        the type of the variable based on how it is used in the template. See
        :ref:`TemplateRenderingStep <TemplateRenderingStep>` for concrete examples on how descriptors are extracted
        from text prompts.

        If you provide a list of input descriptors, each provided descriptor will automatically override the detected one,
        in particular using the new type instead of the detected one.
        If some of them are missing, an error will be thrown at instantiation of the step.

        If you provide input descriptors for non-autodetected variables, a warning will be emitted, and
        they won't be used during the execution of the step.

        **Output descriptors**

        By default, when ``output_descriptors`` is set to ``None``, this step will have a single output descriptor,
        ``PromptExecutionStep.OUTPUT``, type ``StringProperty()``, which will be the raw text generated by the LLM.

        If you provide a list of output descriptors, the ``PromptExecutionStep`` will use structured generation
        to ensure generating all expected outputs with the right types. If the LLM is not able to generate them,
        the values will be filled with their default values if they are specified, or the default values
        of their respective types.

        Parameters
        ----------
        prompt_template:
            Jinja str prompt template to use for generation. See docstring/documentation of the ``TemplateRenderingStep``
            for concrete examples of how to work with jinja prompts in WayFlow.
            Can also be a fully defined PromptTemplate, in which case the arguments of this step will be matched to
            the ones of the prompt to make sure they are compatible.
            If the PromptTemplate includes a chat history placeholder, this will be populated
            automatically by step with the agent, user and tool messages in the conversation.

            .. warning::
                Note that not all LLMs support native structured generation (which is the default
                structured generation mode when simply configuring a ``PromptTemplate`` with a response
                format). For such cases, or to improve the reasoning performed by the model before
                generating the structured output, you can use the helper method
                ``adapt_prompt_template_for_json_structured_generation`` to quickly get started with
                JSON structured generation. For more complex use-cases, you can check out the
                :ref:`PromptTemplate how-to guide <top-howtoprompttemplates>`.

        llm:
            Model that is used when executing the step.
        generation_config:
            Optional generation arguments for the LLM generation in this step. See ``LlmGenerationConfig`` for available parameters.
        send_message:
            Determines whether to send the generated content to the current message list or not. Note that message is still streamed if streaming is not disabled.
            By default, the content is only exposed as an output.
        input_descriptors:
            Input descriptors of the step. ``None`` means the step will resolve the input descriptors automatically using its static configuration in a best effort manner.

        output_descriptors:
            Output descriptors of the step. ``None`` means the step will resolve them automatically using its static
            configuration in a best effort manner.

            .. warning::
                If not ``None`` and doesn't have a single ``StringProperty`` descriptor, the step will perform
                structured generation.

        name:
            Name of the step.

        input_mapping:
            Mapping between the name of the inputs this step expects and the name to get it from in the conversation input/output dictionary.

        output_mapping:
            Mapping between the name of the outputs this step expects and the name to get it from in the conversation input/output dictionary.

        Example
        -------
        >>> from wayflowcore.steps import PromptExecutionStep
        >>> from wayflowcore.flowhelpers import create_single_step_flow
        >>> from wayflowcore.models.llmmodelfactory import LlmModelFactory
        >>> VLLM_CONFIG = {"model_type": "vllm", "host_port": LLAMA70B_API_ENDPOINT, "model_id": "/storage/models/Llama-3.1-70B-Instruct",}
        >>> llm = LlmModelFactory.from_config(VLLM_CONFIG)
        >>> step = PromptExecutionStep(
        ...     prompt_template="What is the capital of {{ country }} The answer is among {% for city in cities %}- {{city}}\\n{% endfor %}",
        ...     llm=llm,
        ... )
        >>> [s.name for s in step.input_descriptors]
        ['country', 'cities']

        >>> assistant = create_single_step_flow(step, 'step')
        >>> conversation = assistant.start_conversation(inputs={'country': 'Switzerland', 'cities': ['bern', 'basel', 'zurich']})
        >>> status = conversation.execute()
        >>> len(status.output_values[PromptExecutionStep.OUTPUT]) > 0
        True

        """
        super().__init__(
            llm=llm,
            input_mapping=input_mapping,
            output_mapping=output_mapping,
            step_static_configuration=dict(
                prompt_template=prompt_template,
                llm=llm,
                generation_config=generation_config,
                send_message=send_message,
            ),
            input_descriptors=input_descriptors,
            output_descriptors=output_descriptors,
            name=name,
            __metadata_info__=__metadata_info__,
        )
        self.llm: LlmModel = llm
        self.prompt_template = prompt_template

        self.generation_config = generation_config
        self.structured_generation_mode = _structured_generation_mode
        self.send_message = send_message
        self.prepared_template, self.use_structured_generation = self._build_template(
            output_descriptors
        )

        self.fill_chat_history_placeholder = isinstance(
            self.prompt_template, PromptTemplate
        ) and PromptTemplate.CHAT_HISTORY_PLACEHOLDER_NAME in set(
            [input_desc.name for input_desc in self.prompt_template.input_descriptors or []]
        )

    def _adapt_messages_for_tool_generation(
        self,
        additional_messages: Sequence[Union[Message, MessageAsDictT]],
        response_format: List[Property],
    ) -> PromptTemplate:
        prompt_template = self.llm.chat_template.copy()
        prompt_template = replace(prompt_template, native_structured_generation=False)
        # we only keep messages with potentially tool placeholders, in case it's a
        # non-native-tool-calling prompt template
        prompt_template.messages = [
            m
            for m in prompt_template.messages
            if isinstance(m, Message) and PromptTemplate.TOOL_PLACEHOLDER_NAME in m.content
        ] + additional_messages  # type: ignore
        prompt_template.__post_init__()  # refresh template properties
        built_template = prompt_template.with_partial(
            inputs={"__PLAN__": None, "custom_instruction": None}
        ).with_tools([_convert_list_of_properties_to_tool(response_format)])
        return built_template

    def _build_template(
        self, output_descriptors: Optional[List[Property]]
    ) -> Tuple[PromptTemplate, bool]:
        if isinstance(self.prompt_template, PromptTemplate):
            built_template = self.prompt_template
            if built_template.response_format is None and output_descriptors is not None:
                # we set the response format automatically on the template
                built_template = built_template.with_response_format(
                    _output_properties_to_response_format_property(
                        self._internal_output_descriptors
                    )
                )
            use_structured_generation = built_template.response_format is not None
            if use_structured_generation and self.structured_generation_mode != None:
                logger.warning(
                    "%s was configured with a prompt template, ignoring structured generation mode "
                    "configuration.",
                    self.name,
                )
        else:
            check_template_validity(self.prompt_template)
            if output_descriptors is None or (
                len(output_descriptors) == 1
                and isinstance(output_descriptors[0], StringProperty)
                and output_descriptors[0].enum is None
            ):
                # simple string template
                built_template = PromptTemplate.from_string(self.prompt_template)
                use_structured_generation = False
            else:
                if self.send_message:
                    raise ValueError(
                        "Cannot generate some output and post it to the conversation. Send `send_message` to False to use structured generation"
                    )
                use_structured_generation = True

                if self.structured_generation_mode == StructuredGenerationMode.TOOL_GENERATION:
                    formatted_messages = [
                        Message(message_type=MessageType.USER, content=self.prompt_template)
                    ]
                    built_template = self._adapt_messages_for_tool_generation(
                        formatted_messages, self._internal_output_descriptors
                    )
                elif self.structured_generation_mode == StructuredGenerationMode.JSON_GENERATION:
                    built_template = _system_prompt_to_json_generation_prompt_template(
                        self.prompt_template, output_descriptors
                    )
                elif (
                    self.structured_generation_mode
                    == StructuredGenerationMode.CONSTRAINED_GENERATION
                ):
                    built_template = PromptTemplate.from_string(
                        self.prompt_template
                    ).with_response_format(
                        _output_properties_to_response_format_property(
                            self._internal_output_descriptors
                        )
                    )
                else:
                    raise ValueError(
                        f"Structured_generation_mode not supported: {self.structured_generation_mode}"
                    )
            built_template = built_template.with_generation_config(self.generation_config)
        return built_template, use_structured_generation

    @classmethod
    def _get_step_specific_static_configuration_descriptors(
        cls,
    ) -> Dict[str, Any]:
        """
        Returns a dictionary in which the keys are the names of the configuration items
        and the values are a descriptor for the expected type
        """
        return {
            "prompt_template": str,
            "llm": LlmModel,
            "generation_config": Optional[LlmGenerationConfig],
            "send_message": bool,
        }

    @classmethod
    def _compute_step_specific_input_descriptors_from_static_config(
        cls,
        prompt_template: Union[str, PromptTemplate],
        llm: LlmModel,
        output_descriptors: Optional[List[Property]],
        generation_config: Optional[LlmGenerationConfig],
        send_message: bool,
    ) -> List[Property]:
        if isinstance(prompt_template, str):
            return (
                TemplateRenderingStep._compute_step_specific_input_descriptors_from_static_config(
                    template=prompt_template
                )
            )
        else:
            return [
                prop
                for prop in (prompt_template.input_descriptors or [])
                # This will be populated automatically by the step
                if prop.name != PromptTemplate.CHAT_HISTORY_PLACEHOLDER_NAME
            ]

    @classmethod
    def _compute_step_specific_output_descriptors_from_static_config(
        cls,
        prompt_template: Union[str, PromptTemplate],
        llm: LlmModel,
        output_descriptors: Optional[List[Property]],
        generation_config: Optional[LlmGenerationConfig],
        send_message: bool,
    ) -> List[Property]:

        # case 1: the template is defined, the output descriptors need to match
        if (
            isinstance(prompt_template, PromptTemplate)
            and prompt_template.response_format is not None
        ):
            template_output_descriptors = [prompt_template.response_format]
            if output_descriptors is not None and output_descriptors != template_output_descriptors:
                raise ValueError(
                    f"The output descriptors of the step and the prompt template do not match. Make sure they are equal:\nStep output descriptors: {output_descriptors}\nTemplate response format: {prompt_template.response_format}"
                )
            return template_output_descriptors

        # case 2: string prompt template, but output_descriptors are set
        if output_descriptors is not None and len(output_descriptors) > 0:
            return output_descriptors

        # case 3: simple string generation
        return [
            StringProperty(
                name=PromptExecutionStep.OUTPUT,
                description="the generated text",
            )
        ]

    async def _invoke_step_async(
        self, inputs: Dict[str, Any], conversation: "FlowConversation"
    ) -> StepResult:
        from wayflowcore.executors._agentexecutor import _can_use_streaming

        if self.fill_chat_history_placeholder:
            filtered_messages = MessageList._filter_messages_by_type(
                messages=conversation.get_messages(),
                types_to_include=[
                    MessageType.TOOL_RESULT,
                    MessageType.USER,
                    MessageType.AGENT,
                    MessageType.TOOL_REQUEST,
                ],
            )
            inputs = {PromptTemplate.CHAT_HISTORY_PLACEHOLDER_NAME: filtered_messages, **inputs}

        prompt = self.prepared_template.format(inputs=inputs)

        if self.send_message and not self.use_structured_generation and _can_use_streaming():
            async_iter = self.llm.stream_generate_async(prompt=prompt, _conversation=conversation)
            new_message = await conversation._append_streaming_message(stream=async_iter)
        else:
            new_message = (
                await self.llm.generate_async(prompt, _conversation=conversation)
            ).message
            if self.send_message:
                conversation.append_message(new_message)

        outputs, next_branch = self._gather_outputs(new_message)
        validated_outputs = self._validate_outputs(outputs, self._internal_output_descriptors)
        return StepResult(outputs=validated_outputs, branch_name=next_branch)

    def _gather_outputs(self, new_message: Message) -> Tuple[Dict[str, Any], str]:
        """Extracts the output from the generated message into the right step outputs"""
        branch_name = Step.BRANCH_NEXT
        outputs = {}
        if self.use_structured_generation:
            if self.structured_generation_mode == StructuredGenerationMode.TOOL_GENERATION:
                # case 1: the model should have generated a tool request, which arguments are the
                # properties of the expected output
                if new_message.tool_requests is not None and len(new_message.tool_requests) >= 1:
                    if len(new_message.tool_requests) > 1:
                        logger.debug(
                            "Model generated %i tool calls for structured generation, using only "
                            "the first one to collect step outputs",
                            len(new_message.tool_requests),
                        )
                    outputs = new_message.tool_requests[0].args
            elif self.structured_generation_mode in {
                StructuredGenerationMode.CONSTRAINED_GENERATION,
                StructuredGenerationMode.JSON_GENERATION,
            }:
                # case 2: the model should have generated a well-formatted JSON
                try:
                    content = json.loads(new_message.content)
                except json.decoder.JSONDecodeError:
                    # might happen that the JSON is wrongly formatted or that it's a simple string
                    content = new_message.content

                if not isinstance(content, dict) or (
                    len(self._internal_output_descriptors) == 1
                    and isinstance(self._internal_output_descriptors[0], ObjectProperty)
                ):
                    # case 2.1:
                    # - the returned content is a simple str/int... (not a dict)
                    # - the expected output is an object and we return it complete
                    # in these cases, we need to wrap the content in a dict and give it the expected name
                    outputs = {self._internal_output_descriptors[0].name: content}
                else:
                    # case 2.2:
                    # the content is already a dict, and we'll use it as output of the step so that the different
                    # properties are returned separately by the step
                    outputs = content
        else:
            # case 3: no structured generation, just putting the raw string under the right output name
            outputs = {self._internal_output_descriptors[0].name: new_message.content}
        return outputs, branch_name

    @staticmethod
    def _validate_outputs(
        outputs: Dict[str, Any],
        expected_outputs: List[Property],
    ) -> Dict[str, type]:
        logger.debug(
            "PromptExecutionStep generated output: %s\nExpected outputs: %s",
            outputs,
            expected_outputs,
        )
        validated_outputs = {}
        for expected_output in expected_outputs:
            output_name = expected_output.name
            if output_name not in outputs:
                default_value = _format_default_value(expected_output)
                logger.warning(
                    f"Constrained generation is missing %s value. Will fill it with default value: %s",
                    output_name,
                    default_value,
                )
                validated_outputs[output_name] = default_value
            else:
                # validate proper types or fill be default values
                # can potentially replace wrong types by the default values of the types
                value = outputs[output_name]
                properly_typed_value = expected_output._validate_or_return_default_value(value)
                if value != properly_typed_value:
                    logger.debug(
                        f'Found value "%s" is not of the expected type: "%s". Will be replaced by: %s',
                        value,
                        expected_output,
                        properly_typed_value,
                    )
                validated_outputs[output_name] = properly_typed_value

        for output_name, output_value in outputs.items():
            if output_name not in validated_outputs:
                logger.warning(
                    "Constrained generation returned an additional value: `%s` with value `%s`. Not returning it.",
                    output_name,
                    output_value,
                )

        logger.debug("PromptExecutionStep validated outputs: %s", validated_outputs)
        return validated_outputs

    def _referenced_tools_dict_inner(
        self, recursive: bool, visited_set: Set[str]
    ) -> Dict[str, "Tool"]:
        all_tools = {}

        if recursive:
            if isinstance(self.prompt_template, PromptTemplate):
                if self.prepared_template.tools is not None:
                    for tool in self.prepared_template.tools:
                        all_tools[tool.id] = tool

        return all_tools
