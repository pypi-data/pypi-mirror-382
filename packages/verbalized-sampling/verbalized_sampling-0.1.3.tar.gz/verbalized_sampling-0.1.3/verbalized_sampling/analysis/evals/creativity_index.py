# Copyright 2025 CHATS-Lab. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import os
import pickle
import signal
import time
from dataclasses import dataclass
from string import punctuation
from typing import Any, Dict, List, Optional

import nltk
import numpy as np
import requests
import torch
from nltk.corpus import stopwords
from sacremoses import MosesDetokenizer
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from unidecode import unidecode

from .base import BaseEvaluator, EvalResult

# Download required NLTK data
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")

md = MosesDetokenizer(lang="en")


@dataclass
class MatchedSpan:
    start_index: int
    end_index: int
    span_text: str
    ref_span_text: str = None  # For semantic matches
    score: float = 1.0  # 1.0 for exact matches, similarity score for semantic matches
    occurrence: int = 0  # For exact matches


@dataclass
class RefDocument:
    """Reference document for semantic matching."""

    token_ids: List[int]
    content_token_ids: List[int]
    content_token_indices: List[int]
    text: str


def timeout_handler(signum, frame):
    """Timeout handler for Earth Mover computation."""
    raise TimeoutError("Earth Mover computation timed out")


class CreativityIndexEvaluator(BaseEvaluator):
    """Evaluator for measuring creativity by analyzing overlap with pretraining data."""

    instance_plot_metrics = [("creativity_index", "violin")]
    aggregate_plot_metrics = ["avg_creativity_index", "avg_coverage", "match_rate"]
    key_plot_metrics = [
        ("avg_creativity_index", "Creativity (Infini-gram)"),
    ]

    def __init__(
        self,
        method: str = "exact",  # "exact" or "semantic"
        corpus: str = "v4_dolma-v1_7_llama",  # Infini-gram corpus
        min_ngram: int = 5,
        min_content_word: int = 2,  # For earth mover
        threshold: float = 0.95,  # For semantic matching
        embed_model: str = "meta-llama/Meta-Llama-3-8B-Instruct",
        api_url: str = "https://api.infini-gram.io/",
        embed_table_path: str = None,
        reference_corpus_path: str = None,  # For semantic matching
        timeout: int = 30,  # Timeout for earth mover computation
        num_workers: int = 24,
    ):
        super().__init__("creativity_index", num_workers)
        self.method = method
        self.corpus = corpus
        self.min_ngram = min_ngram
        self.min_content_word = min_content_word
        self.threshold = threshold
        self.api_url = api_url
        self.embed_model = embed_model
        self.embed_table_path = embed_table_path
        self.reference_corpus_path = reference_corpus_path
        self.timeout = timeout

        # Initialize tokenizer
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                embed_model, add_bos_token=False, add_eos_token=False
            )
        except:
            # Fallback to a public model if the specified one is not accessible
            self.tokenizer = AutoTokenizer.from_pretrained(
                "microsoft/DialoGPT-medium", add_bos_token=False, add_eos_token=False
            )

        # Setup stop words and punctuation for semantic matching
        stop_words = stopwords.words("english") + ["'m", "'d", "'ll", "'o", "'re", "'ve", "'y"]
        self.stop_tokens = set([t for w in stop_words for t in self.tokenizer.tokenize(w)])
        self.punctuations = list(punctuation)

        # Load embedding table for semantic methods
        if method in ["semantic", "earth_mover"]:
            if embed_table_path and os.path.exists(embed_table_path):
                with open(embed_table_path, "rb") as f:
                    self.sim_table = pickle.load(f)
                print(f"Loaded embedding similarity table from {embed_table_path}")
            else:
                print(
                    f"Warning: No embedding table found at {embed_table_path}. Creating new one..."
                )
                self.sim_table = None
                if embed_table_path:
                    self._create_embedding_table(embed_table_path)

        # Load reference corpus for semantic matching
        if method in ["semantic", "earth_mover"] and reference_corpus_path:
            self.reference_docs = self._load_reference_corpus(reference_corpus_path)
        else:
            self.reference_docs = []

    def _create_embedding_table(self, save_path: str):
        """Create embedding similarity table."""
        try:
            print(f"Creating embedding table for {self.embed_model}...")
            model = AutoModelForCausalLM.from_pretrained(self.embed_model)
            embed_table = model.get_input_embeddings().weight.to(
                "cuda" if torch.cuda.is_available() else "cpu"
            )
            num_vocab = embed_table.shape[0]

            cos_sim = torch.nn.CosineSimilarity(dim=1)
            sim_table = torch.zeros((num_vocab, num_vocab))

            with torch.no_grad():
                for i in tqdm(range(num_vocab), desc="Computing similarities"):
                    word_embed = embed_table[i][None, :].expand(num_vocab, -1)
                    sim_score = cos_sim(word_embed, embed_table).cpu()
                    sim_table[i] = sim_score

            sim_table = sim_table.numpy()

            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            with open(save_path, "wb") as f:
                pickle.dump(sim_table, f)

            self.sim_table = sim_table
            print(f"Embedding table saved to {save_path}")

        except Exception as e:
            print(f"Error creating embedding table: {e}")
            self.sim_table = None

    def _load_reference_corpus(self, corpus_path: str) -> List[RefDocument]:
        """Load reference corpus for semantic matching."""
        if not os.path.exists(corpus_path):
            print(f"Warning: Reference corpus not found at {corpus_path}")
            return []

        reference_docs = []
        try:
            with open(corpus_path, "r") as f:
                corpus_data = json.load(f)

            for doc_data in tqdm(
                corpus_data[:100], desc="Loading reference corpus"
            ):  # Limit for demo
                text = doc_data.get("text", "")
                content_token_ids, content_token_indices, token_ids = (
                    self._convert_to_content_tokens(text, return_index=True)
                )

                reference_docs.append(
                    RefDocument(
                        token_ids=token_ids,
                        content_token_ids=content_token_ids,
                        content_token_indices=content_token_indices,
                        text=text,
                    )
                )

            print(f"Loaded {len(reference_docs)} reference documents")

        except Exception as e:
            print(f"Error loading reference corpus: {e}")

        return reference_docs

    def tokenize_text(self, text: str) -> List[str]:
        """Tokenize text using casual tokenization."""
        return nltk.tokenize.casual.casual_tokenize(unidecode(text))

    def detokenize_text(self, tokens: List[str]) -> str:
        """Detokenize tokens back to text."""
        return md.detokenize(tokens)

    def _convert_to_content_tokens(self, text: str, return_index: bool = False):
        """Convert text to content tokens (removing stop words and punctuation)."""
        tokens = self.tokenizer.tokenize(text)
        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        content_tokens, content_indices = [], []

        for i, token in enumerate(tokens):
            clean_token = token.replace("Ä ", "").lower()
            is_stopword = clean_token in self.stop_tokens
            is_punct = all(c in self.punctuations for c in clean_token)

            if not is_stopword and not is_punct:
                content_tokens.append(token_ids[i])
                content_indices.append(i)

        if return_index:
            return content_tokens, content_indices, token_ids
        return content_tokens

    def _compute_similarity(
        self, source_token_ids: List[int], target_token_ids: List[int]
    ) -> float:
        """Compute semantic similarity between token sequences using embedding table."""
        if self.sim_table is None:
            return 0.0

        similarities = []
        for token_id in source_token_ids:
            max_sim = max([self.sim_table[token_id][t] for t in target_token_ids])
            similarities.append(max_sim)

        return sum(similarities) / len(similarities) if similarities else 0.0

    def query_infini_gram(self, query_text: str) -> int:
        """Query the Infini-gram API for exact matches."""
        request_data = {
            "corpus": self.corpus,
            "engine": "c++",
            "query_type": "count",
            "query": query_text,
        }

        try:
            time.sleep(0.1)  # Rate limiting
            response = requests.post(self.api_url, json=request_data, timeout=10)
            result = response.json()
            return result.get("count", 0)
        except Exception as e:
            print(f"Error querying Infini-gram: {e}")
            return 0

    def find_exact_matches(self, tokens: List[str]) -> List[MatchedSpan]:
        """Find exact n-gram matches using Infini-gram API."""
        matched_spans = []
        first_pointer, second_pointer = 0, self.min_ngram

        while second_pointer <= len(tokens):
            span_text = self.detokenize_text(tokens[first_pointer:second_pointer])
            occurrence = self.query_infini_gram(span_text)

            if occurrence > 0:
                matched_span = MatchedSpan(
                    start_index=first_pointer,
                    end_index=second_pointer,
                    span_text=span_text,
                    occurrence=occurrence,
                )

                # Merge overlapping spans or extend existing ones
                if (
                    matched_spans
                    and matched_span.start_index <= matched_spans[-1].start_index
                    and matched_spans[-1].end_index <= matched_span.end_index
                ):
                    matched_spans[-1] = matched_span
                else:
                    matched_spans.append(matched_span)

                second_pointer += 1
            else:
                if second_pointer - first_pointer > self.min_ngram:
                    first_pointer += 1
                elif second_pointer - first_pointer == self.min_ngram:
                    first_pointer += 1
                    second_pointer += 1

        return matched_spans

    def _find_matched_span_earth_mover(
        self, tgt_token_ids: List[int], ref_doc: RefDocument
    ) -> Dict[str, float]:
        """Find matched spans using Earth Mover Distance."""
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(self.timeout)

        try:
            ref_to_tgt_scores = []
            for token_id in ref_doc.content_token_ids:
                ref_to_tgt_scores.append(max([self.sim_table[token_id][t] for t in tgt_token_ids]))

            # Find sub-arrays with mean >= threshold using cumulative sum
            cumulative_sum = [0] * (len(ref_to_tgt_scores) + 1)
            for i in range(1, len(ref_to_tgt_scores) + 1):
                cumulative_sum[i] = cumulative_sum[i - 1] + ref_to_tgt_scores[i - 1]

            matched_spans = {}
            for start in range(len(ref_to_tgt_scores)):
                for end in range(start + self.min_content_word, len(ref_to_tgt_scores) + 1):
                    subarray_sum = cumulative_sum[end] - cumulative_sum[start]
                    subarray_length = end - start
                    subarray_mean = subarray_sum / subarray_length

                    if subarray_mean >= self.threshold:
                        cand_token_ids = ref_doc.content_token_ids[start:end]
                        tgt_to_ref_score = self._compute_similarity(tgt_token_ids, cand_token_ids)

                        if tgt_to_ref_score >= self.threshold:
                            final_score = min(subarray_mean, tgt_to_ref_score)
                            start_idx = ref_doc.content_token_indices[start]
                            end_idx = (
                                ref_doc.content_token_indices[end]
                                if end < len(ref_doc.content_token_ids)
                                else len(ref_doc.token_ids)
                            )
                            ref_span_text = self.tokenizer.decode(
                                ref_doc.token_ids[start_idx:end_idx]
                            )
                            matched_spans[ref_span_text] = final_score

            signal.alarm(0)
            return matched_spans

        except TimeoutError:
            signal.alarm(0)
            return {}
        except Exception as e:
            signal.alarm(0)
            print(f"Error in Earth Mover computation: {e}")
            return {}

    def find_earth_mover_matches(self, tokens: List[str]) -> List[MatchedSpan]:
        """Find semantic matches using Earth Mover Distance."""
        if self.sim_table is None or not self.reference_docs:
            return []

        matched_spans = []
        first_pointer, second_pointer = 0, self.min_ngram

        while second_pointer <= len(tokens):
            span_text = self.detokenize_text(tokens[first_pointer:second_pointer])
            span_token_ids = self._convert_to_content_tokens(span_text)

            if len(span_token_ids) < self.min_content_word:
                second_pointer += 1
                continue

            # Search across all reference documents
            all_matched_spans = {}
            for ref_doc in self.reference_docs:
                matched_spans_dict = self._find_matched_span_earth_mover(span_token_ids, ref_doc)
                all_matched_spans.update(matched_spans_dict)

            if all_matched_spans:
                # Take the best match
                best_ref_span = max(all_matched_spans.keys(), key=lambda k: all_matched_spans[k])
                best_score = all_matched_spans[best_ref_span]

                matched_span = MatchedSpan(
                    start_index=first_pointer,
                    end_index=second_pointer,
                    span_text=span_text,
                    ref_span_text=best_ref_span,
                    score=best_score,
                )
                matched_spans.append(matched_span)
                second_pointer += 1
            else:
                if second_pointer - first_pointer > self.min_ngram:
                    first_pointer += 1
                elif second_pointer - first_pointer == self.min_ngram:
                    first_pointer += 1
                    second_pointer += 1

        return matched_spans

    def compute_coverage(self, tokens: List[str], matched_spans: List[MatchedSpan]) -> float:
        """Compute coverage score (percentage of tokens covered by matches)."""
        if not matched_spans:
            return 0.0

        covered_flags = [False] * len(tokens)
        for span in matched_spans:
            for i in range(span.start_index, min(span.end_index, len(tokens))):
                covered_flags[i] = True

        coverage = sum(covered_flags) / len(covered_flags)
        return coverage

    def compute_instance_metric(self, prompt: str, response: Dict) -> Dict[str, Any]:
        """Compute creativity index for a single response."""
        tokens = self.tokenize_text(response["text"])

        if len(tokens) < self.min_ngram:
            return {
                "response": response["text"],
                "creativity_index": 1.0,  # High creativity for very short responses
                "coverage": 0.0,
                "matched_spans": [],
                "num_tokens": len(tokens),
                "avg_span_length": 0.0,
                "error": "Response too short for analysis",
            }

        # Choose matching method
        if self.method == "exact":
            matched_spans = self.find_exact_matches(tokens)
        elif self.method == "earth_mover":
            matched_spans = self.find_earth_mover_matches(tokens)
        else:  # semantic fallback
            matched_spans = []  # Could implement basic semantic matching here

        coverage = self.compute_coverage(tokens, matched_spans)
        creativity_index = 1.0 - coverage  # Creativity is inverse of coverage

        avg_span_length = (
            np.mean([span.end_index - span.start_index for span in matched_spans])
            if matched_spans
            else 0.0
        )

        return {
            "response": response["text"],
            "creativity_index": float(creativity_index),
            "coverage": float(coverage),
            "matched_spans": [
                {
                    "start_index": span.start_index,
                    "end_index": span.end_index,
                    "span_text": span.span_text,
                    "ref_span_text": span.ref_span_text,
                    "score": float(span.score),
                    "occurrence": span.occurrence,
                }
                for span in matched_spans
            ],
            "num_tokens": len(tokens),
            "avg_span_length": float(avg_span_length),
            "method": self.method,
        }

    def aggregate_metrics(self, instance_metrics: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Aggregate creativity metrics across all responses."""
        if not instance_metrics:
            return {
                "avg_creativity_index": 0.0,
                "std_creativity_index": 0.0,
                "avg_coverage": 0.0,
                "std_coverage": 0.0,
                "total_responses": 0,
                "responses_with_matches": 0,
                "average_span_length": 0.0,
            }

        creativity_scores = [m["creativity_index"] for m in instance_metrics]
        coverage_scores = [m["coverage"] for m in instance_metrics]
        span_lengths = [m["avg_span_length"] for m in instance_metrics if m["avg_span_length"] > 0]
        responses_with_matches = sum(1 for m in instance_metrics if m["matched_spans"])

        return {
            "avg_creativity_index": float(np.mean(creativity_scores)),
            "std_creativity_index": float(np.std(creativity_scores)),
            "avg_coverage": float(np.mean(coverage_scores)),
            "std_coverage": float(np.std(coverage_scores)),
            "min_creativity_index": float(np.min(creativity_scores)),
            "max_creativity_index": float(np.max(creativity_scores)),
            "total_responses": len(instance_metrics),
            "responses_with_matches": responses_with_matches,
            "match_rate": float(responses_with_matches / len(instance_metrics)),
            "avg_span_length": float(np.mean(span_lengths)) if span_lengths else 0.0,
            "method": self.method,
            "corpus": self.corpus,
            "min_ngram": self.min_ngram,
            "threshold": self.threshold if self.method != "exact" else None,
        }

    def evaluate(
        self, prompts: List[str], responses: List[str], metadata: Optional[Dict[str, Any]] = None
    ) -> EvalResult:
        """Evaluate creativity index for responses."""
        if metadata is None:
            metadata = {}

        metadata.update(
            {
                "evaluation_method": "creativity_index",
                "matching_method": self.method,
                "corpus": self.corpus,
                "min_ngram": self.min_ngram,
                "num_responses": len(responses),
                "threshold": self.threshold if self.method != "exact" else None,
            }
        )

        return super().evaluate(prompts, responses, metadata)


# Utility function to create embedding table
def create_embedding_table(
    model_name: str = "meta-llama/Meta-Llama-3-8B-Instruct",
    save_path: str = "data/embed_distance/creativity_index_embeddings.pkl",
):
    """Create and save embedding similarity table for semantic matching."""
    evaluator = CreativityIndexEvaluator(embed_model=model_name)
    evaluator._create_embedding_table(save_path)
    return save_path
