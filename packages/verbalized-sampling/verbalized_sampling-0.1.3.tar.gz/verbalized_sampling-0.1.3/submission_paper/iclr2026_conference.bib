@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@article{Casper2023OpenPA,
  title={Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
  author={Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and J'er'emy Scheurer and Javier Rando and Rachel Freedman and Tomasz Korbak and David Lindner and Pedro J Freire and Tony Wang and Samuel Marks and Charbel-Rapha{\"e}l S{\'e}gerie and Micah Carroll and Andi Peng and Phillip J. K. Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J. Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco di Langosco and Peter Hase and Erdem Biyik and Anca D. Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.15217},
  url={https://api.semanticscholar.org/CorpusID:260316010}
}
@article{yang2024verbalized,
  title={On verbalized confidence scores for llms},
  author={Yang, Daniel and Tsai, Yao-Hung Hubert and Yamada, Makoto},
  journal={arXiv preprint arXiv:2412.14737},
  year={2024}
}
@article{xiao2025flipping,
  title={Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling},
  author={Xiao, Tim Z and Zenn, Johannes and Liu, Zhen and Liu, Weiyang and Bamler, Robert and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2506.09998},
  year={2025}
}
@article{tao2024trust,
  title={When to trust llms: Aligning confidence with response quality},
  author={Tao, Shuchang and Yao, Liuyi and Ding, Hanxing and Xie, Yuexiang and Cao, Qi and Sun, Fei and Gao, Jinyang and Shen, Huawei and Ding, Bolin},
  journal={arXiv preprint arXiv:2404.17287},
  year={2024}
}
@article{Wen2024LanguageML,
  title={Language Models Learn to Mislead Humans via RLHF},
  author={Jiaxin Wen and Ruiqi Zhong and Akbir Khan and Ethan Perez and Jacob Steinhardt and Minlie Huang and Samuel R. Bowman and He He and Shi Feng},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.12822},
  url={https://api.semanticscholar.org/CorpusID:272753153}
}
@inproceedings{Sun2025RethinkingRM,
  title={Rethinking Reward Modeling in Preference-based Large Language Model Alignment},
  author={Hao Sun and Yunyi Shen and Jean-Franccois Ton},
  booktitle={International Conference on Learning Representations},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:278499047}
}
@misc{wong2024simplestratdiversifyinglanguagemodel,
      title={SimpleStrat: Diversifying Language Model Generation with Stratification}, 
      author={Justin Wong and Yury Orlovskiy and Michael Luo and Sanjit A. Seshia and Joseph E. Gonzalez},
      year={2024},
      eprint={2410.09038},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09038}, 
}

@misc{lu2025aihumanityssalieriquantifying,
      title={AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text}, 
      author={Ximing Lu and Melanie Sclar and Skyler Hallinan and Niloofar Mireshghallah and Jiacheng Liu and Seungju Han and Allyson Ettinger and Liwei Jiang and Khyathi Chandu and Nouha Dziri and Yejin Choi},
      year={2025},
      eprint={2410.04265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04265}, 
}

@inproceedings{wang-etal-2019-persuasion,
    title = "Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good",
    author = "Wang, Xuewei  and
      Shi, Weiyan  and
      Kim, Richard  and
      Oh, Yoojung  and
      Yang, Sijia  and
      Zhang, Jingwen  and
      Yu, Zhou",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1566/",
    doi = "10.18653/v1/P19-1566",
    pages = "5635--5649",
    abstract = "Developing intelligent persuasive conversational agents to change people{'}s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals' demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals' personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system."
}


@misc{wei2024measuringshortformfactualitylarge,
      title={Measuring short-form factuality in large language models}, 
      author={Jason Wei and Nguyen Karina and Hyung Won Chung and Yunxin Joy Jiao and Spencer Papay and Amelia Glaese and John Schulman and William Fedus},
      year={2024},
      eprint={2411.04368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.04368}, 
}

@misc{zhu2025bareleveragingbaselanguage,
      title={BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation}, 
      author={Alan Zhu and Parth Asawa and Jared Quincy Davis and Lingjiao Chen and Boris Hanin and Ion Stoica and Joseph E. Gonzalez and Matei Zaharia},
      year={2025},
      eprint={2502.01697},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.01697}, 
}

@misc{west2025basemodelsbeataligned,
      title={Base Models Beat Aligned Models at Randomness and Creativity}, 
      author={Peter West and Christopher Potts},
      year={2025},
      eprint={2505.00047},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.00047}, 
}


@misc{paech2023eqbench,
	title={EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models}, 
	author={Samuel J. Paech},
	year={2023},
	eprint={2312.06281},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{wang2023self,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13484--13508},
  year={2023}
}
@article{dubois2023alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={30039--30069},
  year={2023}
}
@article{si2024can,
  title={Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers},
  author={Si, Chenglei and Yang, Diyi and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2409.04109},
  year={2024}
}

@article{ACKLEY1985147,
title = {A learning algorithm for boltzmann machines},
journal = {Cognitive Science},
volume = {9},
number = {1},
pages = {147-169},
year = {1985},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(85)80012-4},
url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
author = {David H. Ackley and Geoffrey E. Hinton and Terrence J. Sejnowski},
abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.}
}

@misc{fan2018hierarchicalneuralstorygeneration,
      title={Hierarchical Neural Story Generation}, 
      author={Angela Fan and Mike Lewis and Yann Dauphin},
      year={2018},
      eprint={1805.04833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1805.04833}, 
}

@misc{holtzman2020curiouscaseneuraltext,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09751}, 
}

@misc{hewitt2022truncationsamplinglanguagemodel,
      title={Truncation Sampling as Language Model Desmoothing}, 
      author={John Hewitt and Christopher D. Manning and Percy Liang},
      year={2022},
      eprint={2210.15191},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.15191}, 
}
@misc{basu2021mirostatneuraltextdecoding,
      title={Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity}, 
      author={Sourya Basu and Govardana Sachitanandam Ramachandran and Nitish Shirish Keskar and Lav R. Varshney},
      year={2021},
      eprint={2007.14966},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.14966}, 
}

@inproceedings{li-etal-2016-diversity,
    title = "A Diversity-Promoting Objective Function for Neural Conversation Models",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Gao, Jianfeng  and
      Dolan, Bill",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1014/",
    doi = "10.18653/v1/N16-1014",
    pages = "110--119"
}

@inproceedings{zhang2024improving,
  title={Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning},
  author={Zhang, Tianhui and Peng, Bei and Bollegala, Danushka},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={9226--9242},
  year={2024}
}
@article{rosch1973natural,
  title={Natural categories},
  author={Rosch, Eleanor H},
  journal={Cognitive psychology},
  volume={4},
  number={3},
  pages={328--350},
  year={1973},
  publisher={Elsevier}
}

@article{flesch1948new,
  title={A new readability yardstick},
  author={Flesch, Rudolph},
  journal={Journal of Applied Psychology},
  volume={32},
  number={3},
  pages={221},
  year={1948},
  publisher={American Psychological Association},
  url={https://pubmed.ncbi.nlm.nih.gov/18867058/}
}
@misc{tian_just_2023,
    title = {Just {Ask} for {Calibration}: {Strategies} for {Eliciting} {Calibrated} {Confidence} {Scores} from {Language} {Models} {Fine}-{Tuned} with {Human} {Feedback}},
    shorttitle = {Just {Ask} for {Calibration}},
    url = {http://arxiv.org/abs/2305.14975},
    doi = {10.48550/arXiv.2305.14975},
    abstract = {A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pretraining produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widelyused LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHFLMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model’s conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50\%.},
    language = {en},
    urldate = {2025-05-17},
    publisher = {arXiv},
    author = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D.},
    month = oct,
    year = {2023},
    note = {arXiv:2305.14975 [cs]},
    keywords = {Computer Science - Computation and Language},
}
@misc{meister_benchmarking_2024,
    title = {Benchmarking {Distributional} {Alignment} of {Large} {Language} {Models}},
    url = {http://arxiv.org/abs/2411.05403},
    doi = {10.48550/arXiv.2411.05403},
    abstract = {Language models (LMs) are increasingly used as simulacra for people, yet their ability to match the distribution of views of a specific demographic group and be {\textbackslash}textit\{distributionally aligned\} remains uncertain. This notion of distributional alignment is complex, as there is significant variation in the types of attributes that are simulated. Prior works have underexplored the role of three critical variables -- the question domain, steering method, and distribution expression method -- which motivates our contribution of a benchmark explicitly addressing these dimensions. We construct a dataset expanding beyond political values, create human baselines for this task, and evaluate the extent to which an LM can align with a particular group's opinion distribution to inform design choices of such simulation systems. Our analysis reveals open problems regarding if, and how, LMs can be used to simulate humans, and that LLMs can more accurately describe the opinion distribution than simulate such distributions.},
    urldate = {2024-12-14},
    publisher = {arXiv},
    author = {Meister, Nicole and Guestrin, Carlos and Hashimoto, Tatsunori},
    month = nov,
    year = {2024},
    note = {arXiv:2411.05403},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
@misc{padmakumar_does_2024,
    title = {Does {Writing} with {Language} {Models} {Reduce} {Content} {Diversity}?},
    url = {http://arxiv.org/abs/2309.05196},
    doi = {10.48550/arXiv.2309.05196},
    abstract = {Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.},
    urldate = {2025-06-29},
    publisher = {arXiv},
    author = {Padmakumar, Vishakh and He, He},
    month = jul,
    year = {2024},
    note = {arXiv:2309.05196 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}
@misc{hu_fine-tuning_2024,
    title = {Fine-tuning {Large} {Language} {Models} with {Sequential} {Instructions}},
    url = {http://arxiv.org/abs/2403.07794},
    doi = {10.48550/arXiv.2403.07794},
    abstract = {Despite the success of existing instruction-tuned models, we find that they usually struggle to respond to queries with multiple instructions. This impairs their performance in complex problems whose solution consists of multiple intermediate tasks. Thus, we contend that part of the fine-tuning data mixture should be sequential--containing a chain of interrelated tasks. We first approach sequential instruction tuning from a task-driven perspective, manually creating interpretable intermediate tasks for multilingual and visual question answering: namely "translate then predict" and "caption then answer". Next, we automate this process by turning instructions in existing datasets (e.g., Alpaca and FlanCoT) into diverse and complex sequential instructions, making our method general-purpose. Models that underwent our sequential instruction tuning show improved results in coding, maths, and open-ended generation. Moreover, we put forward a new benchmark named SeqEval to evaluate a model's ability to follow all the instructions in a sequence, which further corroborates the benefits of our fine-tuning method. We hope that our endeavours will open new research avenues on instruction tuning for complex tasks.},
    urldate = {2025-06-29},
    publisher = {arXiv},
    author = {Hu, Hanxu and Yu, Simon and Chen, Pinzhen and Ponti, Edoardo M.},
    month = jul,
    year = {2024},
    note = {arXiv:2403.07794 [cs]},
    keywords = {Computer Science - Computation and Language},
}
@misc{yang_how_2025,
    title = {How {Alignment} {Shrinks} the {Generative} {Horizon}},
    url = {http://arxiv.org/abs/2506.17871},
    doi = {10.48550/arXiv.2506.17871},
    abstract = {Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.},
    urldate = {2025-06-29},
    publisher = {arXiv},
    author = {Yang, Chenghao and Holtzman, Ari},
    month = jun,
    year = {2025},
    note = {arXiv:2506.17871 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}
@misc{brown_large_2024,
    title = {Large {Language} {Monkeys}: {Scaling} {Inference} {Compute} with {Repeated} {Sampling}},
    shorttitle = {Large {Language} {Monkeys}},
    url = {http://arxiv.org/abs/2407.21787},
    doi = {10.48550/arXiv.2407.21787},
    abstract = {Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9\% with one sample to 56\% with 250 samples, outperforming the single-attempt state-of-the-art of 43\% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95\% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.},
    urldate = {2024-08-07},
    publisher = {arXiv},
    author = {Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V. and Ré, Christopher and Mirhoseini, Azalia},
    month = jul,
    year = {2024},
    note = {arXiv:2407.21787 [cs]
version: 1},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}
@misc{snell_scaling_2024,
    title = {Scaling {LLM} {Test}-{Time} {Compute} {Optimally} can be {More} {Effective} than {Scaling} {Model} {Parameters}},
    url = {http://arxiv.org/abs/2408.03314},
    abstract = {Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.},
    language = {en},
    urldate = {2024-08-07},
    publisher = {arXiv},
    author = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
    month = aug,
    year = {2024},
    note = {arXiv:2408.03314 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{anthisposition,
  title={Position: LLM Social Simulations Are a Promising Research Method},
  author={Anthis, Jacy Reese and Liu, Ryan and Richardson, Sean M and Kozlowski, Austin C and Koch, Bernard and Brynjolfsson, Erik and Evans, James and Bernstein, Michael S},
  booktitle={Forty-second International Conference on Machine Learning Position Paper Track},
  year={2025},
}

@misc{lin2025usersimulators,
  author       = {Lin, Jessy},
  title        = {User simulators bridge RL with real-world interaction},
  year         = {2025},
  month        = {July},
  day          = {10},
  howpublished = {\url{https://jessylin.com/2025/07/10/user-simulators-1/}},
}
@misc{furumai_zero-shot_2024,
    title = {Zero-shot {Persuasive} {Chatbots} with {LLM}-{Generated} {Strategies} and {Information} {Retrieval}},
    url = {http://arxiv.org/abs/2407.03585},
    doi = {10.48550/arXiv.2407.03585},
    abstract = {Persuasion plays a pivotal role in a wide range of applications from health intervention to the promotion of social good. Persuasive chatbots employed responsibly for social good can be an enabler of positive individual and social change. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. Furthermore, they employ only a handful of pre-defined persuasion strategies. We propose PersuaBot, a zero-shot chatbot based on Large Language Models (LLMs) that is factual and more persuasive by leveraging many more nuanced strategies. PersuaBot uses an LLM to first generate natural responses, from which the strategies used are extracted. To combat hallucination of LLMs, Persuabot replace any unsubstantiated claims in the response with retrieved facts supporting the extracted strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots.},
    urldate = {2025-04-02},
    publisher = {arXiv},
    author = {Furumai, Kazuaki and Legaspi, Roberto and Vizcarra, Julio and Yamazaki, Yudai and Nishimura, Yasutaka and Semnani, Sina J. and Ikeda, Kazushi and Shi, Weiyan and Lam, Monica S.},
    month = oct,
    year = {2024},
    note = {arXiv:2407.03585 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@inproceedings{cox2021directed,
  title={Directed diversity: Leveraging language embedding distances for collective creativity in crowd ideation},
  author={Cox, Samuel Rhys and Wang, Yunlong and Abdul, Ashraf and Von Der Weth, Christian and Y. Lim, Brian},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--35},
  year={2021}
}

@misc{cann2023usingsemanticsimilaritytext,
      title={Using Semantic Similarity and Text Embedding to Measure the Social Media Echo of Strategic Communications}, 
      author={Tristan J. B. Cann and Ben Dennes and Travis Coan and Saffron O'Neill and Hywel T. P. Williams},
      year={2023},
      eprint={2303.16694},
      archivePrefix={arXiv},
      primaryClass={cs.SI},
      url={https://arxiv.org/abs/2303.16694}, 
}

@misc{openai2024embedding,
  author = {{OpenAI}},
  title = {New embedding models and {API} updates},
  year = {2024},
  howpublished = {\url{https://openai.com/index/new-embedding-models-and-api-updates/}}
}
@misc{guan_deliberative_2025,
    title = {Deliberative {Alignment}: {Reasoning} {Enables} {Safer} {Language} {Models}},
    shorttitle = {Deliberative {Alignment}},
    url = {http://arxiv.org/abs/2412.16339},
    doi = {10.48550/arXiv.2412.16339},
    abstract = {As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.},
    urldate = {2025-07-16},
    publisher = {arXiv},
    author = {OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{comanici2025gemini25pushingfrontier,
      title={Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities},
author={Gemini Team},
      year={2025},
      eprint={2507.06261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.06261}, 
}


@misc{AnthropicClaude4,
  author       = {Anthropic},
  title        = {Introducing Claude 4},
  year         = {2025},
  month        = {May},
  url          = {https://www.anthropic.com/news/claude-4},
  note         = {Accessed on July 16, 2025}
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
author={Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{OpenAIO3O4mini,
  author       = {OpenAI},
  title        = {Introducing OpenAI o3 and o4-mini},
  year         = {2025},
  month        = {April},
  url          = {https://openai.com/index/introducing-o3-and-o4-mini/},
  note         = {Accessed on July 16, 2025}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}
@misc{xiong_can_2024,
    title = {Can {LLMs} {Express} {Their} {Uncertainty}? {An} {Empirical} {Evaluation} of {Confidence} {Elicitation} in {LLMs}},
    shorttitle = {Can {LLMs} {Express} {Their} {Uncertainty}?},
    url = {http://arxiv.org/abs/2306.13063},
    doi = {10.48550/arXiv.2306.13063},
    abstract = {Empowering large language models (LLMs) to accurately express confidence in their answers is essential for reliable and trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks—confidence calibration and failure prediction—across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs. The code is publicly available at https://github.com/MiaoXiong2320/llm-uncertainty.},
    language = {en},
    urldate = {2025-05-17},
    publisher = {arXiv},
    author = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
    month = mar,
    year = {2024},
    note = {arXiv:2306.13063 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@misc{cui2025entropymechanismreinforcementlearning,
      title={The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models}, 
      author={Ganqu Cui and Yuchen Zhang and Jiacheng Chen and Lifan Yuan and Zhi Wang and Yuxin Zuo and Haozhan Li and Yuchen Fan and Huayu Chen and Weize Chen and Zhiyuan Liu and Hao Peng and Lei Bai and Wanli Ouyang and Yu Cheng and Bowen Zhou and Ning Ding},
      year={2025},
      eprint={2505.22617},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.22617}, 
}

@misc{janus2022modecollapse,
  author       = {Janus},
  title        = {Mysteries of mode collapse},
  year         = {2022},
  howpublished = {\url{https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse}},
  note         = {Accessed: 2025-07-16}
}
@misc{chiang_chatbot_2024,
    title = {Chatbot {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}},
    shorttitle = {Chatbot {Arena}},
    url = {http://arxiv.org/abs/2403.04132},
    doi = {10.48550/arXiv.2403.04132},
    abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at {\textbackslash}url\{https://chat.lmsys.org\}.},
    urldate = {2024-12-09},
    publisher = {arXiv},
    author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
    month = mar,
    year = {2024},
    note = {arXiv:2403.04132},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  month = {5},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@misc{sorensen2024roadmappluralisticalignment,
      title={A Roadmap to Pluralistic Alignment}, 
      author={Taylor Sorensen and Jared Moore and Jillian Fisher and Mitchell Gordon and Niloofar Mireshghallah and Christopher Michael Rytting and Andre Ye and Liwei Jiang and Ximing Lu and Nouha Dziri and Tim Althoff and Yejin Choi},
      year={2024},
      eprint={2402.05070},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.05070}, 
}

@misc{desai2020calibrationpretrainedtransformers,
      title={Calibration of Pre-trained Transformers}, 
      author={Shrey Desai and Greg Durrett},
      year={2020},
      eprint={2003.07892},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2003.07892}, 
}
@misc{nguyen_turning_2025,
    title = {Turning {Up} the {Heat}: {Min}-p {Sampling} for {Creative} and {Coherent} {LLM} {Outputs}},
    shorttitle = {Turning {Up} the {Heat}},
    url = {http://arxiv.org/abs/2407.01082},
    doi = {10.48550/arXiv.2407.01082},
    abstract = {Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to incoherent or repetitive outputs. We propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by using the top token's probability as a scaling factor. Our experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing show that min-p sampling improves both the quality and diversity of generated text across different model families (Mistral and Llama 3) and model sizes (1B to 123B parameters), especially at higher temperatures. Human evaluations further show a clear preference for min-p sampling, in both text quality and creativity. Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its considerable impact on improving text generation quality.},
    urldate = {2025-06-22},
    publisher = {arXiv},
    author = {Nguyen, Minh Nhat and Baker, Andrew and Neo, Clement and Roush, Allen and Kirsch, Andreas and Shwartz-Ziv, Ravid},
    month = may,
    year = {2025},
    note = {arXiv:2407.01082 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@misc{desai2020calibrationpretrainedtransformers,
      title={Calibration of Pre-trained Transformers}, 
      author={Shrey Desai and Greg Durrett},
      year={2020},
      eprint={2003.07892},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2003.07892}, 
}
@inproceedings{sheng_woman_2019,
    address = {Hong Kong, China},
    title = {The {Woman} {Worked} as a {Babysitter}: {On} {Biases} in {Language} {Generation}},
    shorttitle = {The {Woman} {Worked} as a {Babysitter}},
    url = {https://aclanthology.org/D19-1339/},
    doi = {10.18653/v1/D19-1339},
    abstract = {We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.},
    urldate = {2025-06-06},
    booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
    publisher = {Association for Computational Linguistics},
    author = {Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
    editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
    month = nov,
    year = {2019},
    pages = {3407--3412},
}
@inproceedings{blodgett-etal-2020-language,
    title = "Language (Technology) is Power: A Critical Survey of ``Bias'' in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485/",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing ``bias'' in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing ``bias'' is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating ``bias'' are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing ``bias'' in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of ``bias''{---}i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities."
}
@misc{lambert2025tulu3pushingfrontiers,
      title={Tulu 3: Pushing Frontiers in Open Language Model Post-Training}, 
      author={Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James V. Miranda and Alisa Liu and Nouha Dziri and Shane Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Chris Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi},
      year={2025},
      eprint={2411.15124},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15124}, 
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@misc{qi2024safetyalignmentjusttokens,
      title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep}, 
      author={Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson},
      year={2024},
      eprint={2406.05946},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.05946}, 
}
@misc{zhang_diverging_2024,
    title = {Diverging {Preferences}: {When} do {Annotators} {Disagree} and do {Models} {Know}?},
    shorttitle = {Diverging {Preferences}},
    url = {http://arxiv.org/abs/2410.14632},
    doi = {10.48550/arXiv.2410.14632},
    abstract = {We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning 10 categories across four high-level classes -- task underspecification, response style, refusals, and annotation errors. We find that the majority of disagreements are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. We also find that these tendencies are also echoed by popular LLM-as-Judge evaluation methods, which consistently identify a winning response in cases of diverging preferences. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence on evaluation and training.},
    urldate = {2025-07-24},
    publisher = {arXiv},
    author = {Zhang, Michael JQ and Wang, Zhilin and Hwang, Jena D. and Dong, Yi and Delalleau, Olivier and Choi, Yejin and Choi, Eunsol and Ren, Xiang and Pyatkin, Valentina},
    month = nov,
    year = {2024},
    note = {arXiv:2410.14632 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@article{meister-etal-2023-locally,
    title = "Locally Typical Sampling",
    author = "Meister, Clara  and
      Pimentel, Tiago  and
      Wiher, Gian  and
      Cotterell, Ryan",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.7/",
    doi = "10.1162/tacl_a_00536",
    pages = "102--121",
    abstract = "Today{'}s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process{---}which allows for an information-theoretic analysis{---}can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions."
}
@misc{ismayilzada_creative_2025,
    title = {Creative {Preference} {Optimization}},
    url = {http://arxiv.org/abs/2505.14442},
    doi = {10.48550/arXiv.2505.14442},
    abstract = {While Large Language Models (LLMs) have demonstrated impressive performance across natural language generation tasks, their ability to generate truly creative content-characterized by novelty, diversity, surprise, and quality-remains limited. Existing methods for enhancing LLM creativity often focus narrowly on diversity or specific tasks, failing to address creativity's multifaceted nature in a generalizable way. In this work, we propose Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion. We train and evaluate creativity-augmented versions of several models using CrPO and MuCE, a new large-scale human preference dataset spanning over 200,000 human-generated responses and ratings from more than 30 psychological creativity assessments. Our models outperform strong baselines, including GPT-4o, on both automated and human evaluations, producing more novel, diverse, and surprising generations while maintaining high output quality. Additional evaluations on NoveltyBench further confirm the generalizability of our approach. Together, our results demonstrate that directly optimizing for creativity within preference frameworks is a promising direction for advancing the creative capabilities of LLMs without compromising output quality.},
    urldate = {2025-07-24},
    publisher = {arXiv},
    author = {Ismayilzada, Mete and Jr, Antonio Laverghetta and Luchini, Simone A. and Patel, Reet and Bosselut, Antoine and Plas, Lonneke van der and Beaty, Roger},
    month = may,
    year = {2025},
    note = {arXiv:2505.14442 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
@misc{chung2025modifyinglargelanguagemodel,
      title={Modifying Large Language Model Post-Training for Diverse Creative Writing}, 
      author={John Joon Young Chung and Vishakh Padmakumar and Melissa Roemmele and Yuqian Sun and Max Kreminski},
      year={2025},
      eprint={2503.17126},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.17126}, 
}
@misc{lanchantin2025diversepreferenceoptimization,
      title={Diverse Preference Optimization}, 
      author={Jack Lanchantin and Angelica Chen and Shehzaad Dhuliawala and Ping Yu and Jason Weston and Sainbayar Sukhbaatar and Ilia Kulikov},
      year={2025},
      eprint={2501.18101},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.18101}, 
}
@misc{zhou2025bridgingcreativityunderstandinggap,
      title={Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs}, 
      author={Kuan Lok Zhou and Jiayi Chen and Siddharth Suresh and Reuben Narad and Timothy T. Rogers and Lalit K Jain and Robert D Nowak and Bob Mankoff and Jifan Zhang},
      year={2025},
      eprint={2502.20356},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.20356}, 
}
@misc{zhang2024forcingdiffusedistributionslanguage,
      title={Forcing Diffuse Distributions out of Language Models}, 
      author={Yiming Zhang and Avi Schwarzschild and Nicholas Carlini and Zico Kolter and Daphne Ippolito},
      year={2024},
      eprint={2404.10859},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.10859}, 
}
@misc{tian2025macgyverlargelanguagemodels,
      title={MacGyver: Are Large Language Models Creative Problem Solvers?}, 
      author={Yufei Tian and Abhilasha Ravichander and Lianhui Qin and Ronan Le Bras and Raja Marjieh and Nanyun Peng and Yejin Choi and Thomas L. Griffiths and Faeze Brahman},
      year={2025},
      eprint={2311.09682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09682}, 
}
@misc{mehrotra2024enhancingcreativitylargelanguage,
      title={Enhancing Creativity in Large Language Models through Associative Thinking Strategies}, 
      author={Pronita Mehrotra and Aishni Parab and Sumit Gulwani},
      year={2024},
      eprint={2405.06715},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.06715}, 
}
@inproceedings{SummersStay2023BrainstormTS,
  title={Brainstorm, then Select: a Generative Language Model Improves Its Creativity Score},
  author={Douglas Summers-Stay and Stephanie M. Lukin and Clare R. Voss},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259305709}
}


@misc{jaroslawicz2025instructionsllmsfollowonce,
      title={How Many Instructions Can LLMs Follow at Once?}, 
      author={Daniel Jaroslawicz and Brendan Whiting and Parth Shah and Karime Maamari},
      year={2025},
      eprint={2507.11538},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2507.11538}, 
}

@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@misc{shaikh2025creatinggeneralusermodels,
      title={Creating General User Models from Computer Use}, 
      author={Omar Shaikh and Shardul Sapkota and Shan Rizvi and Eric Horvitz and Joon Sung Park and Diyi Yang and Michael S. Bernstein},
      year={2025},
      eprint={2505.10831},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2505.10831}, 
}

@inproceedings{jin-etal-2024-persuading,
    title = "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model",
    author = "Jin, Chuhao  and
      Ren, Kening  and
      Kong, Lingzhen  and
      Wang, Xiting  and
      Song, Ruihua  and
      Chen, Huan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.92/",
    doi = "10.18653/v1/2024.acl-long.92",
    pages = "1678--1706",
    abstract = "Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrieval-based models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT-4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user{'}s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io."
}

@misc{wang20258020rulehighentropyminority,
      title={Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning}, 
      author={Shenzhi Wang and Le Yu and Chang Gao and Chujie Zheng and Shixuan Liu and Rui Lu and Kai Dang and Xionghui Chen and Jianxin Yang and Zhenru Zhang and Yuqiong Liu and An Yang and Andrew Zhao and Yang Yue and Shiji Song and Bowen Yu and Gao Huang and Junyang Lin},
      year={2025},
      eprint={2506.01939},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.01939}, 
}

@incollection{SWELLER201137,
title = {Cognitive Load Theory},
editor = {Jose P. Mestre and Brian H. Ross},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {55},
pages = {37-76},
year = {2011},
issn = {0079-7421},
doi = {https://doi.org/10.1016/B978-0-12-387691-1.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123876911000028},
author = {John Sweller},
keywords = {Working Memory, Long-term Memory, Human Cognitive Architecture, Evolutionary Psychology, Instructional Processes},
abstract = {Cognitive load theory uses evolutionary theory to consider human cognitive architecture and uses that architecture to devise novel, instructional procedures. The theory assumes that knowledge can be divided into biologically primary knowledge that we have evolved to acquire and biologically secondary knowledge that is important for cultural reasons. Secondary knowledge, unlike primary knowledge, is the subject of instruction. It is processed in a manner that is analogous to the manner in which biological evolution processes information. When dealing with secondary knowledge, human cognition requires a very large information store, the contents of which are acquired largely by obtaining information from other information stores. Novel information is generated by a random generate and test procedure with only very limited amounts of novel information able to be processed at any given time. In contrast, very large amounts of organized information stored in the information store can be processed in order to generate complex action. This architecture has been used to generate instructional procedures, summarized in this chapter.}
}
@misc{narad_which_2025,
    title = {Which {LLMs} {Get} the {Joke}? {Probing} {Non}-{STEM} {Reasoning} {Abilities} with {HumorBench}},
    shorttitle = {Which {LLMs} {Get} the {Joke}?},
    url = {http://arxiv.org/abs/2507.21476},
    doi = {10.48550/arXiv.2507.21476},
    abstract = {We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.},
    urldate = {2025-08-04},
    publisher = {arXiv},
    author = {Narad, Reuben and Suresh, Siddharth and Chen, Jiayi and Dysart-Bricken, Pine S. L. and Mankoff, Bob and Nowak, Robert and Zhang, Jifan and Jain, Lalit},
    month = jul,
    year = {2025},
    note = {arXiv:2507.21476 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{huang2024posthocrewardcalibrationcase,
      title={Post-hoc Reward Calibration: A Case Study on Length Bias}, 
      author={Zeyu Huang and Zihan Qiu and Zili Wang and Edoardo M. Ponti and Ivan Titov},
      year={2024},
      eprint={2409.17407},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.17407}, 
}

@misc{zhu2025charmcalibratingrewardmodels,
      title={CHARM: Calibrating Reward Models With Chatbot Arena Scores}, 
      author={Xiao Zhu and Chenmien Tan and Pinzhen Chen and Rico Sennrich and Yanlin Zhang and Hanxu Hu},
      year={2025},
      eprint={2504.10045},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.10045}, 
}

@misc{liu2024rmbenchbenchmarkingrewardmodels,
      title={RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style}, 
      author={Yantao Liu and Zijun Yao and Rui Min and Yixin Cao and Lei Hou and Juanzi Li},
      year={2024},
      eprint={2410.16184},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.16184}, 
}

@misc{chen_diversity_2024,
    title = {On the {Diversity} of {Synthetic} {Data} and its {Impact} on {Training} {Large} {Language} {Models}},
    url = {http://arxiv.org/abs/2410.15226},
    doi = {10.48550/arXiv.2410.15226},
    abstract = {The rise of Large Language Models (LLMs) has accentuated the need for diverse, high-quality pre-training data. Synthetic data emerges as a viable solution to the challenges of data scarcity and inaccessibility. While previous literature has focused predominantly on the quality and quantity of real data, our work enables the measurement of diversity in synthetic data and explores its impact on LLM performance. We study the downstream effects of synthetic data diversity during both the pre-training and fine-tuning stages by introducing a new diversity metric, {\textbackslash}textit\{LLM cluster-agent\}, designed to evaluate the diversity of synthetic datasets. Through a series of controlled experiments with models of 350M and 1.4B parameters, we demonstrate that the proposed cluster-based LLM scoring of diversity correlates positively with both pre-training and supervised fine-tuning performance. Our findings also reveal that synthetic data diversity in pre-training affects supervised fine-tuning more significantly than pre-training itself, even for smaller models. We hope this study advances our understanding of the optimal use of synthetic data in LLM training and opens new avenues for efficient data generation processes.},
    urldate = {2025-08-06},
    publisher = {arXiv},
    author = {Chen, Hao and Waheed, Abdul and Li, Xiang and Wang, Yidong and Wang, Jindong and Raj, Bhiksha and Abdin, Marah I.},
    month = oct,
    year = {2024},
    note = {arXiv:2410.15226 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@misc{rafailov2024directpreferenceoptimizationlanguage,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290}, 
}

@misc{kirk2024prismalignmentdatasetparticipatory,
      title={The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models}, 
      author={Hannah Rose Kirk and Alexander Whitefield and Paul Röttger and Andrew Bean and Katerina Margatina and Juan Ciro and Rafael Mosquera and Max Bartolo and Adina Williams and He He and Bertie Vidgen and Scott A. Hale},
      year={2024},
      eprint={2404.16019},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16019}, 
}

@misc{xiao2024algorithmicbiasaligninglarge,
      title={On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization}, 
      author={Jiancong Xiao and Ziniu Li and Xingyu Xie and Emily Getzen and Cong Fang and Qi Long and Weijie J. Su},
      year={2024},
      eprint={2405.16455},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.16455}, 
}

@article{
judgement_bias,
author = {Amos Tversky  and Daniel Kahneman },
title = {Judgment under Uncertainty: Heuristics and Biases},
journal = {Science},
volume = {185},
number = {4157},
pages = {1124-1131},
year = {1974},
doi = {10.1126/science.185.4157.1124},
URL = {https://www.science.org/doi/abs/10.1126/science.185.4157.1124},
eprint = {https://www.science.org/doi/pdf/10.1126/science.185.4157.1124},
abstract = {This article described three heuristics that are employed in making judgments under uncertainty: (i) representativeness, which is usually employed when people are asked to judge the probability that an object or event A belongs to class or process B; (ii) availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development; and (iii) adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available. These heuristics are highly economical and usually effective, but they lead to systematic and predictable errors. A better understanding of these heuristics and of the biases to which they lead could improve judgments and decisions in situations of uncertainty.}}

@misc{ge2025scalingsyntheticdatacreation,
      title={Scaling Synthetic Data Creation with 1,000,000,000 Personas}, 
      author={Tao Ge and Xin Chan and Xiaoyang Wang and Dian Yu and Haitao Mi and Dong Yu},
      year={2025},
      eprint={2406.20094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.20094}, 
}

@misc{zhang2024improvingdiversitycommonsensegeneration,
      title={Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning}, 
      author={Tianhui Zhang and Bei Peng and Danushka Bollegala},
      year={2024},
      eprint={2404.16807},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16807}, 
}

@misc{chen2025personavectorsmonitoringcontrolling,
      title={Persona Vectors: Monitoring and Controlling Character Traits in Language Models}, 
      author={Runjin Chen and Andy Arditi and Henry Sleight and Owain Evans and Jack Lindsey},
      year={2025},
      eprint={2507.21509},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.21509}, 
}

@misc{chakrabarty2024artartificelargelanguage,
      title={Art or Artifice? Large Language Models and the False Promise of Creativity}, 
      author={Tuhin Chakrabarty and Philippe Laban and Divyansh Agarwal and Smaranda Muresan and Chien-Sheng Wu},
      year={2024},
      eprint={2309.14556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.14556}, 
}

@inproceedings{
omahony2024attributing,
title={Attributing Mode Collapse in the fine-tuning of Large Language Models},
author={Laura O'Mahony and Leo Grinsztajn and Hailey Schoelkopf and Stella Biderman},
booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
year={2024},
url={https://openreview.net/forum?id=3pDMYjpOxk}
}

@misc{kirk2024understandingeffectsrlhfllm,
      title={Understanding the Effects of RLHF on LLM Generalisation and Diversity}, 
      author={Robert Kirk and Ishita Mediratta and Christoforos Nalmpantis and Jelena Luketina and Eric Hambro and Edward Grefenstette and Roberta Raileanu},
      year={2024},
      eprint={2310.06452},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.06452}, 
}

@misc{anthis2025llmsocialsimulationspromising,
      title={LLM Social Simulations Are a Promising Research Method}, 
      author={Jacy Reese Anthis and Ryan Liu and Sean M. Richardson and Austin C. Kozlowski and Bernard Koch and James Evans and Erik Brynjolfsson and Michael Bernstein},
      year={2025},
      eprint={2504.02234},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2504.02234}, 
}

@misc{lum2025biaslanguagemodelstrick,
      title={Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation}, 
      author={Kristian Lum and Jacy Reese Anthis and Kevin Robinson and Chirag Nagpal and Alexander D'Amour},
      year={2025},
      eprint={2402.12649},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.12649}, 
}

@misc{manning2024automatedsocialsciencelanguage,
      title={Automated Social Science: Language Models as Scientist and Subjects}, 
      author={Benjamin S. Manning and Kehang Zhu and John J. Horton},
      year={2024},
      eprint={2404.11794},
      archivePrefix={arXiv},
      primaryClass={econ.GN},
      url={https://arxiv.org/abs/2404.11794}, 
}

@unpublished{hewitt2025llm_predictions,
  author    = {Hewitt, L. and Ashokkumar, A. and Ghezae, I. and Willer, R.},
  title     = {Predicting Results of Social Science Experiments Using Large Language Models},
  note      = {Supplementary Materials. Web demo and discussion thread: LLM Predictions. Media coverage: HAI: LLM-Aided Social Science},
  year      = {2025},
}

@misc{zhou2024sotopiainteractiveevaluationsocial,
      title={SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents}, 
      author={Xuhui Zhou and Hao Zhu and Leena Mathur and Ruohong Zhang and Haofei Yu and Zhengyang Qi and Louis-Philippe Morency and Yonatan Bisk and Daniel Fried and Graham Neubig and Maarten Sap},
      year={2024},
      eprint={2310.11667},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.11667}, 
}

@article{xiao2024algorithmic,
  title={On the algorithmic bias of aligning large language models with RLHF: Preference collapse and matching regularization},
  author={Xiao, Jiancong and Li, Ziniu and Xie, Xingyu and Getzen, Emily and Fang, Cong and Long, Qi and Su, Weijie J},
  journal={arXiv preprint arXiv:2405.16455},
  year={2024}
}

@inproceedings{chakraborty2024maxmin,
  title={Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences},
  author={Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Huang, Furong and Manocha, Dinesh and Bedi, Amrit and Wang, Mengdi},
  booktitle={ICML 2024 Workshop on Models of Human Feedback for AI Alignment},
  year={2024}
}

@article{lanchantin2025diverse,
  title={Diverse preference optimization},
  author={Lanchantin, Jack and Chen, Angelica and Dhuliawala, Shehzaad and Yu, Ping and Weston, Jason and Sukhbaatar, Sainbayar and Kulikov, Ilia},
  journal={arXiv preprint arXiv:2501.18101},
  year={2025}
}

@article{li2024preserving,
  title={Preserving diversity in supervised fine-tuning of large language models},
  author={Li, Ziniu and Chen, Congliang and Xu, Tian and Qin, Zeyu and Xiao, Jiancong and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2408.16673},
  year={2024}
}

@article{yun2025price,
  title={The Price of Format: Diversity Collapse in LLMs},
  author={Yun, Longfei and An, Chenyang and Wang, Zilong and Peng, Letian and Shang, Jingbo},
  journal={arXiv preprint arXiv:2505.18949},
  year={2025}
}

@article{duanempirical,
  title={Empirical Validation and Measurement of the Collapse Phenomenon},
  author={Duan, Yucong},
  year={2025}
}

@misc{ouyang2023improvingadversarialrobustnesscontrastive,
      title={Improving Adversarial Robustness by Contrastive Guided Diffusion Process}, 
      author={Yidong Ouyang and Liyan Xie and Guang Cheng},
      year={2023},
      eprint={2210.09643},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.09643}, 
}

@inproceedings{Bartolo_2021,
   title={Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation},
   url={http://dx.doi.org/10.18653/v1/2021.emnlp-main.696},
   DOI={10.18653/v1/2021.emnlp-main.696},
   booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
   publisher={Association for Computational Linguistics},
   author={Bartolo, Max and Thrush, Tristan and Jia, Robin and Riedel, Sebastian and Stenetorp, Pontus and Kiela, Douwe},
   year={2021} }

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{mei2025reasoninguncertaintyreasoningmodels,
      title={Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?}, 
      author={Zhiting Mei and Christina Zhang and Tenny Yin and Justin Lidard and Ola Shorinwa and Anirudha Majumdar},
      year={2025},
      eprint={2506.18183},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.18183}, 
}

@misc{yang2025qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Team Qwen},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}

@article{ks-test,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2280095},
 abstract = {The test is based on the maximum difference between an empirical and a hypothetical cumulative distribution. Percentage points are tabled, and a lower bound to the power function is charted. Confidence limits for a cumulative distribution are described. Examples are given. Indications that the test is superior to the chi-square test are cited.},
 author = {Frank J. Massey},
 journal = {Journal of the American Statistical Association},
 number = {253},
 pages = {68--78},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {The Kolmogorov-Smirnov Test for Goodness of Fit},
 urldate = {2025-08-22},
 volume = {46},
 year = {1951}
}

@misc{setlur2024rlincorrectsyntheticdata,
      title={RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold}, 
      author={Amrith Setlur and Saurabh Garg and Xinyang Geng and Naman Garg and Virginia Smith and Aviral Kumar},
      year={2024},
      eprint={2406.14532},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.14532}, 
}

@inproceedings{zhang-etal-2021-trading,
    title = "Trading Off Diversity and Quality in Natural Language Generation",
    author = "Zhang, Hugh  and
      Duckworth, Daniel  and
      Ippolito, Daphne  and
      Neelakantan, Arvind",
    editor = "Belz, Anya  and
      Agarwal, Shubham  and
      Graham, Yvette  and
      Reiter, Ehud  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.humeval-1.3/",
    pages = "25--33",
    abstract = "For open-ended language generation tasks such as storytelling or dialogue, choosing the right decoding algorithm is vital for controlling the tradeoff between generation \textit{quality} and \textit{diversity}. However, there presently exists no consensus on which decoding procedure is best or even the criteria by which to compare them. In this paper, we cast decoding as a tradeoff between response quality and diversity, and we perform the first large-scale evaluation of decoding methods along the entire quality-diversity spectrum. Our experiments confirm the existence of the likelihood trap: the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We also find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, nucleus sampling (Holtzman et al., 2019) outperforms all other evaluated decoding algorithms."
}

@misc{shurofry2024growingtailincreasingoutput,
      title={Growing a Tail: Increasing Output Diversity in Large Language Models}, 
      author={Michal Shur-Ofry and Bar Horowitz-Amsalem and Adir Rahamim and Yonatan Belinkov},
      year={2024},
      eprint={2411.02989},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02989}, 
}

@misc{lu2025benchmarkinglanguagemodelcreativity,
      title={Benchmarking Language Model Creativity: A Case Study on Code Generation}, 
      author={Yining Lu and Dixuan Wang and Tianjian Li and Dongwei Jiang and Sanjeev Khudanpur and Meng Jiang and Daniel Khashabi},
      year={2025},
      eprint={2407.09007},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09007}, 
}
@misc{zhang_noveltybench_2025,
    title = {{NoveltyBench}: {Evaluating} {Language} {Models} for {Humanlike} {Diversity}},
    shorttitle = {{NoveltyBench}},
    url = {http://arxiv.org/abs/2504.05228},
    doi = {10.48550/arXiv.2504.05228},
    abstract = {Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from mode collapse, the inability to generate diverse and novel outputs. Our work introduces NoveltyBench, a benchmark specifically designed to evaluate the ability of language models to produce multiple distinct and high-quality outputs. NoveltyBench utilizes prompts curated to elicit diverse answers and filtered real-world user queries. Evaluating 20 leading language models, we find that current state-of-the-art systems generate significantly less diversity than human writers. Notably, larger models within a family often exhibit less diversity than their smaller counterparts, challenging the notion that capability on standard benchmarks translates directly to generative utility. While prompting strategies like in-context regeneration can elicit diversity, our findings highlight a fundamental lack of distributional diversity in current models, reducing their utility for users seeking varied responses and suggesting the need for new training and evaluation paradigms that prioritize diversity alongside quality.},
    urldate = {2025-08-26},
    publisher = {arXiv},
    author = {Zhang, Yiming and Diddee, Harshita and Holm, Susan and Liu, Hanchen and Liu, Xinyue and Samuel, Vinay and Wang, Barry and Ippolito, Daphne},
    month = aug,
    year = {2025},
    note = {arXiv:2504.05228 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@misc{padmakumar2025memorizationmappingoriginalityqualityfrontier,
      title={Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models}, 
      author={Vishakh Padmakumar and Chen Yueh-Han and Jane Pan and Valerie Chen and He He},
      year={2025},
      eprint={2504.09389},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.09389}, 
}

@misc{Tan2025RL2,
    author={Chenmien Tan and Simon Yu and Lanbo Lin and Ze Zhang and Yuanwu Xu and Chenhao Jiang and Tianyuan Yang and Sicong Xie and Guannan Zhang},
    title={RL2: Ray Less Reinforcement Learning},
    note={GitHub repository},
    howpublished={\url{https://github.com/ChenmienTan/RL2}},
    year={2025}
}

@misc{turgeman2025jokeruleallimpossibility,
      title={One Joke to Rule them All? On the (Im)possibility of Generalizing Humor}, 
      author={Mor Turgeman and Chen Shani and Dafna Shahaf},
      year={2025},
      eprint={2508.19402},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.19402}, 
}

@misc{reddit_dad_jokes_2023,
  author = {{Reddit}},
  title = {Reddit dad jokes},
  year = {2023},
  url = {https://www.kaggle.com/datasets/oktayozturk010/reddit-dad-jokes/data},
  urldate = {YYYY-MM-DD}
}

@misc{narad2025llmsjokeprobingnonstem,
      title={Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench}, 
      author={Reuben Narad and Siddharth Suresh and Jiayi Chen and Pine S. L. Dysart-Bricken and Bob Mankoff and Robert Nowak and Jifan Zhang and Lalit Jain},
      year={2025},
      eprint={2507.21476},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.21476}, 
}

@misc{openai2025gpt41,
  author       = {OpenAI},
  title        = {Introducing GPT-4.1 in the API},
  howpublished = {\url{https://openai.com/index/gpt-4-1/}},
  note         = {Accessed: 2025-09-14},
  year         = {2025},
  month        = apr
}

@inproceedings{stienon2020learning,
  author = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  title = {Learning to summarize from human feedback},
  booktitle = {NeurIPS},
  year = 2020,
}

@misc{cui2023ultrafeedback,
      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, 
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2024helpsteer2,
      title={HelpSteer2: Open-source dataset for training top-performing reward models}, 
      author={Zhilin Wang and Yi Dong and Olivier Delalleau and Jiaqi Zeng and Gerald Shen and Daniel Egert and Jimmy J. Zhang and Makesh Narsimhan Sreedhar and Oleksii Kuchaiev},
      year={2024},
      eprint={2406.08673},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{liu2024skywork,
  title={Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs},
  author={Liu, Chris Yuhao and Zeng, Liang and Liu, Jiacai and Yan, Rui and He, Jujie and Wang, Chaojie and Yan, Shuicheng and Liu, Yang and Zhou, Yahui},
  journal={arXiv preprint arXiv:2410.18451},
  year={2024}
}

@misc{hong2023zeroshotgoaldirecteddialoguerl,
      title={Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations}, 
      author={Joey Hong and Sergey Levine and Anca Dragan},
      year={2023},
      eprint={2311.05584},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.05584}, 
}

@misc{hong2024interactivedialogueagentsreinforcement,
      title={Interactive Dialogue Agents via Reinforcement Learning on Hindsight Regenerations}, 
      author={Joey Hong and Jessica Lin and Anca Dragan and Sergey Levine},
      year={2024},
      eprint={2411.05194},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.05194}, 
}

@misc{hong2025planningsearchrefiningfrontier,
      title={Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL}, 
      author={Joey Hong and Anca Dragan and Sergey Levine},
      year={2025},
      eprint={2505.18098},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.18098}, 
}

@misc{yao2024taubenchbenchmarktoolagentuserinteraction,
      title={$\tau$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains}, 
      author={Shunyu Yao and Noah Shinn and Pedram Razavi and Karthik Narasimhan},
      year={2024},
      eprint={2406.12045},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.12045}, 
}

@misc{light2023avalonbenchevaluatingllmsplaying,
      title={AvalonBench: Evaluating LLMs Playing the Game of Avalon}, 
      author={Jonathan Light and Min Cai and Sheng Shen and Ziniu Hu},
      year={2023},
      eprint={2310.05036},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.05036}, 
}

@misc{souly2024strongreject,
      title={A StrongREJECT for Empty Jailbreaks},
      author={Alexandra Souly and Qingyuan Lu and Dillon Bowen and Tu Trinh and Elvis Hsieh and Sana Pandey and Pieter Abbeel and Justin Svegliato and Scott Emmons and Olivia Watkins and Sam Toyer},
      year={2024},
      eprint={2402.10260},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{krippendorff2018content,
  title={Content analysis: An introduction to its methodology},
  author={Krippendorff, Klaus},
  year={2018},
  publisher={Sage publications}
}

@inproceedings{chen-etal-2022-semeval,
    title = "{S}em{E}val-2022 Task 8: Multilingual news article similarity",
    author = {Chen, Xi  and
      Zeynali, Ali  and
      Camargo, Chico  and
      Fl{\"o}ck, Fabian  and
      Gaffney, Devin  and
      Grabowicz, Przemyslaw  and
      Hale, Scott A.  and
      Jurgens, David  and
      Samory, Mattia},
    editor = "Emerson, Guy  and
      Schluter, Natalie  and
      Stanovsky, Gabriel  and
      Kumar, Ritesh  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      Singh, Siddharth  and
      Ratan, Shyam",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.155/",
    doi = "10.18653/v1/2022.semeval-1.155",
    pages = "1094--1106",
    abstract = "Thousands of new news articles appear daily in outlets in different languages. Understanding which articles refer to the same story can not only improve applications like news aggregation but enable cross-linguistic analysis of media consumption and attention. However, assessing the similarity of stories in news articles is challenging due to the different dimensions in which a story might vary, e.g., two articles may have substantial textual overlap but describe similar events that happened years apart. To address this challenge, we introduce a new dataset of nearly 10,000 news article pairs spanning 18 language combinations annotated for seven dimensions of similarity as SemEval 2022 Task 8. Here, we present an overview of the task, the best performing submissions, and the frontiers and challenges for measuring multilingual news article similarity. While the participants of this SemEval task contributed very strong models, achieving up to 0.818 correlation with gold standard labels across languages, human annotators are capable of reaching higher correlations, suggesting space for further progress."
}

@article{tversky1973availability,
  title={Availability: A heuristic for judging frequency and probability},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={Cognitive psychology},
  volume={5},
  number={2},
  pages={207--232},
  year={1973},
  publisher={Elsevier}
}

@article{zajonc1968attitudinal,
  title={Attitudinal effects of mere exposure.},
  author={Zajonc, Robert B},
  journal={Journal of personality and social psychology},
  volume={9},
  number={2p2},
  pages={1},
  year={1968},
  publisher={American Psychological Association}
}

@article{bornstein1989exposure,
  title={Exposure and affect: overview and meta-analysis of research, 1968--1987.},
  author={Bornstein, Robert F},
  journal={Psychological bulletin},
  volume={106},
  number={2},
  pages={265},
  year={1989},
  publisher={American Psychological Association}
}

@article{alter2009uniting,
  title={Uniting the tribes of fluency to form a metacognitive nation},
  author={Alter, Adam L and Oppenheimer, Daniel M},
  journal={Personality and social psychology review},
  volume={13},
  number={3},
  pages={219--235},
  year={2009},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{reber2004processing,
  title={Processing fluency and aesthetic pleasure: Is beauty in the perceiver's processing experience?},
  author={Reber, Rolf and Schwarz, Norbert and Winkielman, Piotr},
  journal={Personality and social psychology review},
  volume={8},
  number={4},
  pages={364--382},
  year={2004},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@incollection{mandler2014structure,
  title={The structure of value: Accounting for taste},
  author={Mandler, George},
  booktitle={Affect and cognition},
  pages={3--36},
  year={2014},
  publisher={Psychology Press}
}

@article{meyers1989schema,
  title={Schema congruity as a basis for product evaluation},
  author={Meyers-Levy, Joan and Tybout, Alice M},
  journal={Journal of consumer research},
  volume={16},
  number={1},
  pages={39--54},
  year={1989},
  publisher={The University of Chicago Press}
}

@article{tversky1974judgment,
  title={Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty.},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={science},
  volume={185},
  number={4157},
  pages={1124--1131},
  year={1974},
  publisher={American association for the advancement of science}
}

@article{nickerson1998confirmation,
  title={Confirmation bias: A ubiquitous phenomenon in many guises},
  author={Nickerson, Raymond S},
  journal={Review of general psychology},
  volume={2},
  number={2},
  pages={175--220},
  year={1998},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{hasher1977frequency,
  title={Frequency and the conference of referential validity},
  author={Hasher, Lynn and Goldstein, David and Toppino, Thomas},
  journal={Journal of verbal learning and verbal behavior},
  volume={16},
  number={1},
  pages={107--112},
  year={1977},
  publisher={Elsevier}
}

@inproceedings{li2024entropic,
  title={Entropic distribution matching for supervised fine-tuning of LLMs: Less overfitting and better diversity},
  author={Li, Ziniu and Chen, Congliang and Xu, Tian and Qin, Zeyu and Xiao, Jiancong and Sun, Ruoyu and Luo, Zhi-Quan},
  booktitle={NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability},
  year={2024}
}


@misc{hamilton2024detectingmodecollapselanguage,
      title={Detecting Mode Collapse in Language Models via Narration}, 
      author={Sil Hamilton},
      year={2024},
      eprint={2402.04477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04477}, 
}

@misc{shi2024detectingpretrainingdatalarge,
      title={Detecting Pretraining Data from Large Language Models}, 
      author={Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
      year={2024},
      eprint={2310.16789},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.16789}, 
}


@article{Xu_2025,
   title={Echoes in AI: Quantifying lack of plot diversity in LLM outputs},
   volume={122},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.2504966122},
   DOI={10.1073/pnas.2504966122},
   number={35},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Xu, Weijia and Jojic, Nebojsa and Rao, Sudha and Brockett, Chris and Dolan, Bill},
   year={2025},
   month=aug 
}

@misc{shaib2025standardizingmeasurementtextdiversity,
      title={Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores}, 
      author={Chantal Shaib and Joe Barrow and Jiuding Sun and Alexa F. Siu and Byron C. Wallace and Ani Nenkova},
      year={2025},
      eprint={2403.00553},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.00553}, 
}

@misc{stiennon2022learningsummarizehumanfeedback,
      title={Learning to summarize from human feedback}, 
      author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
      year={2022},
      eprint={2009.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.01325}, 
}

@misc{cui2024ultrafeedbackboostinglanguagemodels,
      title={UltraFeedback: Boosting Language Models with Scaled AI Feedback}, 
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Bingxiang He and Wei Zhu and Yuan Ni and Guotong Xie and Ruobing Xie and Yankai Lin and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.01377}, 
}

@misc{wang2023helpsteermultiattributehelpfulnessdataset,
      title={HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM}, 
      author={Zhilin Wang and Yi Dong and Jiaqi Zeng and Virginia Adams and Makesh Narsimhan Sreedhar and Daniel Egert and Olivier Delalleau and Jane Polak Scowcroft and Neel Kant and Aidan Swope and Oleksii Kuchaiev},
      year={2023},
      eprint={2311.09528},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09528}, 
}

@misc{liu2024skyworkrewardbagtricksreward,
      title={Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs}, 
      author={Chris Yuhao Liu and Liang Zeng and Jiacai Liu and Rui Yan and Jujie He and Chaojie Wang and Shuicheng Yan and Yang Liu and Yahui Zhou},
      year={2024},
      eprint={2410.18451},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.18451}, 
}

@article{gneiting2007strictly,
  title={Strictly proper scoring rules, prediction, and estimation},
  author={Gneiting, Tilmann and Raftery, Adrian E},
  journal={Journal of the American statistical Association},
  volume={102},
  number={477},
  pages={359--378},
  year={2007},
  publisher={Taylor \& Francis}
}

@inproceedings{chen-etal-2024-evaluating-diversity,
    title = "Evaluating Diversity in Automatic Poetry Generation",
    author = {Chen, Yanran  and
      Gr{\"o}ner, Hannes  and
      Zarrie{\ss}, Sina  and
      Eger, Steffen},
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1097/",
    doi = "10.18653/v1/2024.emnlp-main.1097",
    pages = "19671--19692",
    abstract = "Natural Language Generation (NLG), and more generally generative AI, are among the currently most impactful research fields. Creative NLG, such as automatic poetry generation, is a fascinating niche in this area. While most previous research has focused on forms of the Turing test when evaluating automatic poetry generation {---} can humans distinguish between automatic and human generated poetry {---} we evaluate the diversity of automatically generated poetry (with a focus on quatrains), by comparing distributions of generated poetry to distributions of human poetry along structural, lexical, semantic and stylistic dimensions, assessing different model types (word vs. character-level, general purpose LLMs vs. poetry-specific models), including the very recent LLaMA3-8B, and types of fine-tuning (conditioned vs. unconditioned). We find that current automatic poetry systems are considerably underdiverse along multiple dimensions {---} they often do not rhyme sufficiently, are semantically too uniform and even do not match the length distribution of human poetry. Our experiments reveal, however, that style-conditioning and character-level modeling clearly increases diversity across virtually all dimensions we explore. Our identified limitations may serve as the basis for more genuinely diverse future poetry generation models."
}

@misc{kim2025aihumorgenerationcognitive,
      title={AI Humor Generation: Cognitive, Social and Creative Skills for Effective Humor}, 
      author={Sean Kim and Lydia B. Chilton},
      year={2025},
      eprint={2502.07981},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2502.07981}, 
}

@article{gwet2008computing,
  title={Computing inter-rater reliability and its variance in the presence of high agreement},
  author={Gwet, Kilem Li},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={61},
  number={1},
  pages={29--48},
  year={2008},
  publisher={Wiley Online Library}
}


@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. the method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}



@misc{ye2025limoreasoning,
      title={LIMO: Less is More for Reasoning}, 
      author={Yixin Ye and Zhen Huang and Yang Xiao and Ethan Chern and Shijie Xia and Pengfei Liu},
      year={2025},
      eprint={2502.03387},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.03387}, 
}

@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Team Qwen},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}
@misc{jain_livecodebench_2024,
    title = {{LiveCodeBench}: {Holistic} and {Contamination} {Free} {Evaluation} of {Large} {Language} {Models} for {Code}},
    shorttitle = {{LiveCodeBench}},
    url = {http://arxiv.org/abs/2403.07974},
    doi = {10.48550/arXiv.2403.07974},
    abstract = {Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model},
    urldate = {2025-09-23},
    publisher = {arXiv},
    author = {Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
    month = jun,
    year = {2024},
    note = {arXiv:2403.07974 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{qian-etal-2024-towards,
    title = "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
    author = "Qian, Chen  and
      Zhang, Jie  and
      Yao, Wei  and
      Liu, Dongrui  and
      Yin, Zhenfei  and
      Qiao, Yu  and
      Liu, Yong  and
      Shao, Jing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.290/",
    doi = "10.18653/v1/2024.findings-acl.290",
    pages = "4864--4888",
    abstract = "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM{'}s pre-training checkpoints to enhance the LLM{'}s trustworthiness. Finally, inspired by the theoretical result that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field."
}

@misc{weidinger2021ethicalsocialrisksharm,
      title={Ethical and social risks of harm from Language Models}, 
      author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
      year={2021},
      eprint={2112.04359},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.04359}, 
}

@misc{together2023redpajama,
  author       = {Together Computer},
  title        = {RedPajama: An Open Dataset for Training Large Language Models},
  year         = {2023},
  howpublished = {\url{https://github.com/togethercomputer/RedPajama-Data}},
  note         = {Accessed: 2025-09-23}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}
@misc{he2024olympiadbench,
      title={OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems}, 
      author={Chaoqun He and Renjie Luo and Yuzhuo Bai and Shengding Hu and Zhen Leng Thai and Junhao Shen and Jinyi Hu and Xu Han and Yujie Huang and Yuxiang Zhang and Jie Liu and Lei Qi and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.14008},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@misc{anthropic2025claude37,
  author       = {Anthropic},
  title        = {Claude 3.7 Sonnet and Claude Code},
  year         = {2025},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-7-sonnet}},
  note         = {Accessed: 2025-09-24}
}

@misc{openai2025deepresearch,
  author       = {OpenAI},
  title        = {Introducing Deep Research},
  year         = {2025},
  howpublished = {\url{https://openai.com/index/introducing-deep-research/}},
  note         = {Accessed: 2025-09-24}
}

@misc{liu2025understandingr1zeroliketrainingcritical,
      title={Understanding R1-Zero-Like Training: A Critical Perspective}, 
      author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},
      year={2025},
      eprint={2503.20783},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.20783}, 
}

@misc{wang2023helpsteer,
      title={HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM}, 
      author={Zhilin Wang and Yi Dong and Jiaqi Zeng and Virginia Adams and Makesh Narsimhan Sreedhar and Daniel Egert and Olivier Delalleau and Jane Polak Scowcroft and Neel Kant and Aidan Swope and Oleksii Kuchaiev},
      year={2023},
      eprint={2311.09528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}