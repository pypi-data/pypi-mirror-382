% ---------- (Optional) local macros if not already defined ----------
\newcommand{\one}{\mathbf{1}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\simplex}{\Delta}
% -------------------------------------------------------------------

\section{Method: Verbalized Sampling}\label{sec:vs}
 We have shown that for a mode-collapsed model, any response $y^* \in \arg\max_y \pi_\text{ref}(y \mid x)$ on $\mathcal{S}$, which suggests the need to study the base model $\pi_\text{ref}  $. %aligned models will generate the mode of the base model $\pi_{\textrm{ref}}$. 
 Empirical studies \citep{west2025basemodelsbeataligned, zhu2025bareleveragingbaselanguage} have shown that base models do exhibit diversity. Therefore, we propose \emph{Verbalized Sampling} as a prompting strategy to recover the diversity level of $\pi_{\textrm{ref}}$, to bypass mode collapse. 



\subsection{Different Prompts Collapse to Different Modes}\label{subsec:constrained}


For a mode-collapsed LLM, we find that different prompts $x$ collapse to different modes of $\pi_\text{ref}$. This is how VS can mitigate mode collapse. We categorize prompting strategies into three types and provide their corresponding modes. Detailed assumptions and proof are provided in \S\ref{appendix:anthony's proof on the mode for different prompts}. %See \S\ref{appendix:probing_pre_training_data} for empirical results on probing pre-training data distributions. 

% the output depends on the query type. When given an instance-level query $x_{\text{instance}}$, the model's output collapses to the single prototype response $y_{\text{proto}}$ 
% % \wyshi{this is not the most precise representation, right, in 3.3, this is a broader concept than the single instance} 
% annotated during RLHF. When given a distribution-level query $x_{\text{distribution}}$, its output collapses to a string that reflects the distribution of responses $P_{\text{learned}}(y|x)$ learned during pre-training.

% For a mode-collapsed LLM, we find that different prompting strategies elicit different modes 
% the output depends on the query type. When given an instance-level query $x_{\text{instance}}$, the model's output collapses to the single prototype response $y_{\text{proto}}$ 
% % \wyshi{this is not the most precise representation, right, in 3.3, this is a broader concept than the single instance} 
% annotated during RLHF. When given a distribution-level query $x_{\text{distribution}}$, its output collapses to a string that reflects the distribution of responses $P_{\text{learned}}(y|x)$ learned during pre-training.


% a prompt $x$ with an associated sampling strategy $s_x$, as defined below. See the detailed proof in \S\ref{appendix:anthony's proof on the mode for different prompts} %Consider the following definitions of $x$ and $s_x$.
% Here, we allow $s_x$ to sample from $\pi$ in non-traditional ways. For example, consider the following definitions of $x$ and $s_x$.
\begin{enumerate}[nolistsep, leftmargin=*]
\item \textbf{Instance-level prompt}: This is the most traditional prompt $x$, requesting one instance (e.g., ``Tell me a joke about coffee''). The mode is the mode instance (the mode joke) of the base model. % So $s_x$ acts like an identity function: $s_x(\pi(\cdot | x)) = \pi(\cdot | x)$. 
%In contrast to these non-traditional sampling strategies, we can also sample $\pi$ in the usual manner. So, $s_x$ would act like an identity function: $s_x(\pi(\cdot | x)) = \pi(\cdot | x)$.
    \item \textbf{List-level prompt}: This prompt $x$ requests a list of outputs (e.g., ``Tell me $k$ jokes about coffee''), as used in~\cite{wang2023self, dubois2023alpacafarm}. The mode is a uniform distribution of related items (a uniformly-distributed list of jokes) learned by the base model during pretraining.    
    % a reasonable choice for $s_x$ is to %sample $\pi(\cdot | x)$ to produce a list and then 
    % randomly sample any element from it with uniform probability as the output. %\as{potentially useful to provide a pointer to other work here}
    \item \textbf{Distribution-level prompt (ours)}: We propose this prompt $x$ which requests $k$ outputs with corresponding probabilities (e.g., ``Tell $k$ jokes about coffee with their probabilities''), and name it \textbf{\textit{Verbalized Sampling (VS)}}. The mode is a \text{distribution} capable of approximating
    %approximately
    the distribution of related items learned by the base model during pretraining. 
    Figure~\ref{fig:qualitative} and \S\ref{appendix:probing_pre_training_data} show that when an LLM is prompted to generate a distribution of the 50 US states, its verbalized probability distribution aligns with a proxy of the same distribution in a pre-training corpus (RedPajama), where the KL divergence is 0.12 for Claude-4-Sonnet. %Figure~\ref{fig:qualitative} and \S\ref{appendix:probing_pre_training_data} ask LLM to generate the distribution of all 50 US states, and show that the VS-verbalized probability distribution align with a proxy of this learned  distribution in a pretraining corpus. 
 %and ground truth probability distributions.
%, The model will generate $k$ distinct jokes $J_i$ with $k$ probabilities $p_i$. $s_x$ can randomly pick from each $J_i$ according to the associated probabilities $p_i$.
    % (DC: Actively working on this section...)
\end{enumerate}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/appendix/gt_vs_Claude-4-Sonnet_1.pdf}
%     \caption{Enter Caption}
%     \label{fig:placeholder}
% \end{figure}



\begin{table}[t]
\centering
\caption{Comparison of different prompting methods, given the same computation budget of $N$ total responses. %Computational comparison of inference methods for generating $N$ total responses. 
% \wyshi{need to universally change $c$ to $k$} 
$k$ is the number of candidates generated per LLM call, specified in the prompt (e.g., $k=5$ for the joke task). %For methods that generate multiple candidates per call, we use $\lceil N/k \rceil$ calls to ensure at least $N$ total responses. 
$y_i$ denotes the $i$-th generated candidate, $\hat{p}_i$  denotes its verbalized probability, and $\pi(\cdot|x)$ represents the LLM's output distribution conditioned on the prompt $x$. For Multi-Turn and VS-Multi, $h_{i-1}$ denotes the conversation history up to turn $i-1$, and $t$ denotes the $t$-th turn. 
}
\label{tab:method_comparison_table}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{LLM Calls} & \textbf{Candidates} & \textbf{Turns} & \textbf{Prompt Example} & \textbf{Definition} \\
% & & \textbf{(per call)} & & & \\
\midrule
\addlinespace[0.5ex]
\multicolumn{6}{l}{\textbf{\textit{1. Instance-level Prompt}}} \\
\midrule
\addlinespace[0.5ex]
\quad \colorbox{LightGray}{Direct} & $N$ & 1 & 1 & ``Tell a joke about coffee'' & $y_i \sim \pi(y|x)$ \\
\addlinespace[0.5ex]
\hdashline
\addlinespace[0.5ex]
\quad \colorbox{LightGray}{CoT} & $N$ & 1 & 1 & ``Think step-by-step, then tell a joke'' & $y_i \sim \pi(y|x_{\text{CoT}})$ \\
\midrule
\multicolumn{6}{l}{\textbf{\textit{2. List-level Prompt}}} \\
\midrule
\addlinespace[0.5ex]
\quad \colorbox{LightGray}{Sequence} & $\lceil N/k \rceil$ & $k$ & 1 & ``Tell 5  jokes about coffee'' & $(y_1, ..., y_k) \sim \pi(y_1, ..., y_k|x_{\text{seq}})$ \\
\addlinespace[0.5ex]
\hdashline
\addlinespace[0.5ex]
\multirow{2}{*}{\quad \colorbox{LightGray}{Multi-Turn}} & \multirow{2}{*}{$N$} & \multirow{2}{*}{1} & \multirow{2}{*}{$N$} & Turn 1: ``Tell a joke about coffee'' & \multirow{2}{*}{$y_i \sim \pi(y|x_{\text{multi}}, h_{i-1})$} \\
& & & & Turn 2+: ``Tell another joke about coffee'' & \\
\midrule
\multicolumn{6}{l}{\colorbox{white}{\textbf{\textit{3. Distribution-level Prompt (Ours)}}}} \\
\midrule
\addlinespace[0.5ex]
\quad \colorbox{LightGray}{VS-Standard} & $\lceil N/k \rceil$ & $k$ & 1 & ``Tell 5 jokes with their probabilities'' & $(y_1, \hat{p}_1), ..., (y_k, \hat{p}_k) \sim \pi(\cdot|x_{\text{VS}})$ \\
\addlinespace[0.5ex]
\hdashline
\addlinespace[0.5ex]
\quad \colorbox{LightGray}{VS-CoT} & $\lceil N/k \rceil$ & $k$ & 1 & \begin{tabular}{c}``Think step-by-step, then tell 5\\jokes with probabilities''\end{tabular} & $(y_1, \hat{p}_1), ..., (y_k, \hat{p}_k) \sim \pi(\cdot|x_{\text{VS-CoT}})$ \\
\addlinespace[0.5ex]
\hdashline
\addlinespace[0.5ex]
\multirow{2}{*}{\quad \colorbox{LightGray}{VS-Multi}} & \multirow{2}{*}{$\lceil N/k \rceil$} & \multirow{2}{*}{$k$} & \multirow{2}{*}{$\lceil N/k \rceil$} & Turn 1: ``Tell 5 jokes with probabilities'' & \multirow{2}{*}{\begin{tabular}{c}$(y_1^{(1)}, \hat{p}_1^{(1)}), ..., (y_k^{(t)}, \hat{p}_k^{(t)})$\\$\sim \pi(\cdot|x_{\text{VS}}, h_{t-1})$\end{tabular}} \\
& & & & Turn 2+: ``Tell 5 more with probabilities'' & \\
\addlinespace[0.5ex]
\bottomrule
\end{tabular}
}
\vspace{-1em}
\end{table}


In Table~\ref{tab:method_comparison_table}, we summarize how to implement different prompting methods in practice, under the same computation budget of $N$ total generated responses for a fair comparison. In theory, the number of candidates $k$ in each LLM call could be equal to $N$; but in practice, we notice that if $k$ is too large, the generation quality degrades, so usually $k < N$ and we will generate $N$ total responses across $\lceil N/k \rceil$ calls. For \textbf{(2) List-level prompt}, we test another variant, \textit{multi-turn} \citep{west2025basemodelsbeataligned}, which elicits $N$ responses across $N$ turns in a conversation. 
For \textbf{(3) Distribution-level prompt}, we propose two variants: \textbf{\textit{VS-CoT}} and \textbf{\textit{VS-Multi}}, to further enhance diversity.
% In Table~\ref{tab:method_comparison}, we summarize how to implement different methods in practice, under the same computing budget of $N$ total generated responses for a fair comparison. 


\subsection{Experimental Setup} \label{sec:experimental_setup}
\paragraph{LLMs.} 

Our method is training-free, model-agnostic, and requires no logit access. We test it on a suite of models: (1) closed models like {GPT Series} (\textbf{GPT-4.1-mini}, \textbf{GPT-4.1}), {Gemini Series} (\textbf{Gemini-2.5-Flash}, \textbf{Gemini-2.5-Pro}) and {Claude Series} (\textbf{Claude-3.7-Sonnet}, \textbf{Claude-4-Sonnet}); (2) open ones like \textbf{Llama-3.1-70B-Instruct} and \textbf{Qwen3-235B-A22B-2507-Instruct-2507}; and (3) {reasoning models} like \textbf{OpenAI o3} and \textbf{DeepSeek R1}. See  \S\ref{appendix:experiment_settings} for generation  hyperparameters.


% Our method is training-free, model-agnostic, and requires no logit access, so we test it on a suite of models. Closed-source models include \underline{GPT Series} (\textbf{GPT-4.1-mini}, \textbf{GPT-4.1})~\citep{openai2024gpt4technicalreport}, \underline{Gemini Series} (\textbf{Gemini-2.5-Flash}, \textbf{Gemini-2.5-Pro})~\citep{comanici2025gemini25pushingfrontier} and \underline{Claude Series} (\textbf{Claude-3.7-Sonnet}, \textbf{Claude-4-Sonnet})~\citep{AnthropicClaude4}; for open-source models, we use \textbf{Llama-3.1-70B-Instruct}~\citep{grattafiori2024llama3herdmodels} and \textbf{Qwen3-235B-A22B-2507-Instruct-2507}~\citep{yang2025qwen3technicalreport}. We also test \underline{reasoning models} like \textbf{OpenAI o3}~\citep{OpenAIO3O4mini} and \textbf{DeepSeek R1}~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. %See  \Cref{appendix:experiment_settings} for details on the generation hyperparameters.


% Our method is training-free, model-agnostic, and requires no logit access, allowing us to test across different models as follows:  
% % Our approach is training-free, model-agnostic and does not require access to the logit. This makes it broadly applicable across different model families and sizes. 
% Specifically, we test the following models. 

\paragraph{Tasks.} We conduct comprehensive experiments on creative writing (\S\ref{sec:creative_writing}), dialogue simulation (\S\ref{sec:dialogue_simulation_task}),  open-ended QA (\S\ref{main:open_ended_qa}), synthetic data generation (\S\ref{sec:sythetic data} and \S\ref{sec:negative synthetic data}), random number generation (\S\ref{sec:random_number_generation}), along with commonsense reasoning (\S\ref{appendix:commonsense}) and safety (\S\ref{sec:safety}) to show that our method maintains factual accuracy and safety.

% also test on commonsense reasoning (\S\ref{sec:commonsense}) and safety (\S\ref{sec:synthetic_data}) to show that VS does not sacrifice factualness and safety. 
% \Cref{fig:intro_qualitative_examples} shows qualitative examples generated by our method.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/intro/intro_qualitative.pdf}
%     \caption{Qualitative examples on creative writing and dialogue simulation task with VS.
%     \vspace{-2em}}
%     \label{fig:intro_qualitative_examples}
% \end{figure}


% To ensure fair computational comparison, we evaluate all methods under a fixed budget of generating $N$ total responses, with the key differences summarized in Table~\ref{tab:method_comparison}. We group these methods based on how they query the model's underlying distribution.

% Our evaluation begins with \textbf{Baseline Instance Queries}, such as standard \textbf{Direct} sampling and \textbf{Chain-of-Thought (CoT)}~\citep{wei2023chainofthoughtpromptingelicitsreasoning}. These methods rely on single instance queries, making them highly susceptible to mode collapse.

% We then consider \textbf{Implicit Distributional Queries}, which improve diversity by implicitly asking the model to sample from its learned distribution. This category includes \textbf{Sequence} generation~\citep{meister_benchmarking_2024}, which prompts for a batch of varied responses, and \textbf{Multi-Turn} conversation, which uses dialogue history to elicit new ones. While effective, these methods still frame the task as instance sampling rather than explicit statistical reporting.

% Finally, we introduce our \textbf{Explicit Distributional Queries (\ours)}. This approach, which we term \ourslower, directly operationalizes the Mode-Collapsed Oracle Principle. The primary method, \textbf{VS-Standard}, uses an explicit distributional query to prompt the model to generate responses along with their probability estimates in a structured format. We also test two performance-enhancing variants: \textbf{VS-CoT}, which adds a reasoning step, and \textbf{VS-Multi}, which integrates multi-turn generation.


% \paragraph{\ours}
% Our \ourslower approach prompts the model to generate $N$ responses along with their probability estimates in a structured format. We introduce the base method, \textbf{VS-Standard}; and two more variants, (2) \textbf{VS-CoT} (includes explicit reasoning before probability assessment). (3) \textbf{VS-Multi} (integrates multi-turn generation with verbalized probability estimation to maintain both diversity and generation quality). 
% \wyshi{give an example on how this is done exactly}



% \jiayicomment{Adding cognitive burden? Formalize the VS methods and all baseline methods}

% Given the task input prompt $x$, aligned language models typically generate responses from a narrow probability distribution $P(y|x)$ that concentrates around high-probability, safe outputs. This leads to mode collapse where the model repeatedly generates similar responses, limiting diversity and creativity. Our goal is to recover a more balanced distribution $\hat{P}(y|x)$ that better represents the full spectrum of plausible responses.

% \subsection{Methodology Overview}

% Our approach consists of four distinct sampling strategies that we compare against traditional baselines. Each method represents a different way of eliciting diverse responses from language models.

% \subsubsection{Direct Sampling (Baseline)}
% \label{sec:direct}

% Direct sampling serves as our primary baseline, where the model generates a single response to the input prompt without any special instructions. This represents the standard inference behavior of aligned models:

% \begin{equation}
% y = \text{LLM}(x)
% \end{equation}

% where $x$ is the input prompt and $y$ is the generated response.

% \subsubsection{Sequence Sampling}
% \label{sec:sequence}

% In sequence sampling, we prompt the model to generate multiple responses within a single inference call. The model is instructed to produce $N$ different responses in a list format:

% \begin{equation}
% [y_1, y_2, ..., y_N] = \text{LLM}(\text{``Generate } N \text{ different responses to: } x\text{''})
% \end{equation}

% The responses are returned as a Python list of strings, allowing the model to consider multiple alternatives simultaneously within a single context window.

% \subsubsection{Multi-turn Sampling}
% \label{sec:multiturn}

% Multi-turn sampling generates responses across multiple conversation turns, where each turn produces one response. After the initial response, we prompt the model to generate alternative responses:

% \begin{align}
% y_1 &= \text{LLM}(x) \\
% y_i &= \text{LLM}(x, y_1, ..., y_{i-1}, \text{``Generate an alternative response''})
% \end{align}

% for $i = 2, ..., N$. This approach allows the model to build upon previous responses and explore different directions iteratively.

% \subsubsection{\ours}
% \label{sec:verbalized}

% Our core contribution is \textbf{\ours}, which explicitly prompts the model to reason about the likelihood of different responses and provide probability estimates. The model generates responses in a structured JSON format:

% \begin{equation}
% \{y_i, p_i\}_{i=1}^N = \text{LLM}(\text{CoT-prompt}(x))
% \end{equation}

% where each response $y_i$ is paired with a probability estimate $p_i \in [0,1]$ representing the model's assessment of how likely that response would be. The chain-of-thought (CoT) prompt encourages the model to:

% \begin{enumerate}
%     \item \textbf{Reason explicitly} about different possible responses
%     \item \textbf{Assess probabilities} for each response based on factors like commonality, appropriateness, and creativity
%     \item \textbf{Generate diverse outputs} by considering both high and low-probability responses
% \end{enumerate}

% The key insight is that by verbalizing the sampling process, the model gains access to responses that would typically be suppressed in the aligned distribution, effectively recovering the diversity present in the pre-alignment model.

% \subsubsection{\ours (Combined)}
% \label{sec:verbalized_combined}

% The \textbf{Combined} variant of \ours uses a different conceptual framework for response assessment. Instead of asking for empirical probabilities, it prompts the model to provide confidence scores based on conventionality:

% \begin{equation}
% \{y_i, c_i\}_{i=1}^N = \text{LLM}(\text{Combined-prompt}(x))
% \end{equation}

% where each response $y_i$ is paired with a confidence score $c_i \in [0,1]$ representing how conventional or typical the response is (1.0 = very typical/common, 0.0 = highly original/unconventional). This approach explicitly encourages the model to:

% \begin{enumerate}
%     \item \textbf{Generate responses across the creativity spectrum} from conventional to highly original
%     \item \textbf{Assess conventionality} rather than pure likelihood
%     \item \textbf{Balance diversity and appropriateness} by considering both typical and creative responses
% \end{enumerate}

% This method differs from the \ours approach by focusing on creativity/conventionality rather than empirical probability, potentially surfacing more innovative responses while maintaining a structured evaluation framework.


% \subsection{Response Selection Strategies}
% While some tasks can benefit from retaining all sampled responses, many practical applications require selecting a single candidate to proceed. For example, in dialogue simulation tasks (\S \ref{sec:dialogue_simulation_task}), maintaining all possible responses would lead to exponential computational costs as the conversation progresses. To address this constraint, we employ two principled selection strategies:

% \paragraph{Random Selection} We uniformly randomly select one response from the generated set: $y = y_{\text{uniform}(\{1,...,N\})}$. This approach ensures unbiased sampling and prevents systematic preference for any particular response type.

% \paragraph{Probability-weighted Selection} For \ourslower methods, we leverage the model's probability estimates to guide selection: $y = y_i$ where $i \sim \text{Categorical}(\{p_1, ..., p_N\})$. This strategy respects the model's confidence assessments while maintaining stochasticity in the selection process.


% \begin{table}[!h]
% \centering
% \caption{
% }\label{tab:main}
% \begin{tabular}{l|c|ccccc}
% \toprule
% \multirow{2}{*}{Model} & \multirow{2}{*}{Method}
% & \multicolumn{2}{c}{Creativity} & {Bias} & {Simulation} & {Commonsense} \\
% & & Poem & Jokes & & & \\
% \midrule
% \multirow{3}{*}{GPT-4.1}
% & Baseline & 5.3 & 72.1 & & & \\
% & Verbalized & 12.8 & 15.5 & & & \\
% & Gap & \uag{7.5} & \uag{56.6} & & & \\
% \midrule
% \multirow{3}{*}{GPT-4.1-Mini}
% & Baseline & 4.2 & 74.7 & & & \\
% & Verbalized & 7.5 & 12.4 & & & \\
% & Gap & \uag{3.3} & \uag{62.3} & & & \\
% \midrule
% \multirow{3}{*}{Claude-4-Sonnet}
% & Baseline & 5.1 & 86.9 & & & \\
% & Verbalized & 15.2 & 13.1 & & & \\
% & Gap & \uag{10.1} & \uag{73.8} & & & \\
% \midrule
% \multirow{3}{*}{Claude-3.7-Sonnet}
% & Baseline & 5.4 & 70.1 & & & \\
% & Verbalized & 10.8 & 12.5 & & & \\
% & Gap & \uag{5.4} & \uag{57.6} & & & \\
% \midrule
% \multirow{3}{*}{Gemini-2.5-Pro}
% & Baseline & 6.7 & 14.2 & & & \\
% & Verbalized & 14.7 & 11.2 & & & \\
% & Gap & \uag{8.0} & \uag{3.0} & & & \\
% \midrule
% \multirow{3}{*}{Gemini-2.5-Flash}
% & Baseline & 5.5 & 77.3 & & & \\
% & Verbalized & 10.4 & 13.9 & & & \\
% & Gap & \uag{4.9} & \uag{63.4} & & & \\
% \midrule
% \multirow{3}{*}{OpenAI-o3}
% & Baseline & 6.6 & 38.0 & & & \\
% & Verbalized & 14.0 & 10.0 & & & \\
% & Gap & \uag{7.4} & \uag{28.0} & & & \\
% \midrule
% \multirow{3}{*}{DeepSeek-R1}
% & Baseline & 6.2 & & & & \\
% & Verbalized & 12.4 & & & & \\
% & Gap & \uag{6.2} & & & & \\
% \midrule
% \multirow{3}{*}{Llama-3.1-70B}
% & Baseline & 6.2 & & & & \\
% & Verbalized & 16.1 & & & & \\
% & Gap & \uag{9.9} & & & & \\
% \bottomrule
% \end{tabular}
% \end{table}