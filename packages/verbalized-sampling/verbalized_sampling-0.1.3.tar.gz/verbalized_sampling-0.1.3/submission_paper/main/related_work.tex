\section{Related Work}\label{sec:related_work}
% \jiayicomment{shrink to half page and move the rest to the appendix}
\paragraph{Mode Collapse and Alignment.} 
% While RLHF~\citep{brown_large_2024} effectively enhances safety and alignment with human preferences~\citep{alpaca_eval, chiang_chatbot_2024, sorensen2024roadmappluralisticalignment}, it also causes a substantial loss in output diversity and often only generates specific outputs, a phenomenon known as \emph{mode collapse}~\citep{janus2022modecollapse,omahony2024attributing,kirk2024understandingeffectsrlhfllm}. 

% \wyshi{how is our figure different from this work?}.  \citet{zhu2025bareleveragingbaselanguage} also showed that for synthetic data generation, base models can generate more diverse training data compared to their aligned counterparts. Recently, \citet{yang_how_2025} systematically investigated how alignment affects generation diversity, by introducing a metric called the branching factor. %\wyshi{how is our different?} \simoncomment{it's solely an analysis paper and doesn't propose any solution to mitigate that. Also their analysis are based on statistical+  empirical results, whereas ours are theoretical}
% \simoncomment{Should be highlighting the first input}

Previous studies \citep{padmakumar_does_2024, west2025basemodelsbeataligned} have observed that compared to their base counterparts, aligned models suffer from mode collapse, a significant drop in output diversity. \citet{lu2025aihumanityssalieriquantifying} quantified this issue, showing that the creative capacity of LLMs diminishes after alignment. Existing research has primarily attributed this phenomenon to algorithmic limitations~\citep{Casper2023OpenPA}. \cite{chakraborty2024maxmin} suggested that it is inadequate to rely on a single reward model to capture diverse human preferences, while \cite{xiao2024algorithmic} showed that the KL-regularized optimization used in RLHF tends to amplify common, majority-style responses. The issue is compounded further by practices even before alignment: for instance, SFT can lead to overfitting and limited diversity due to its cross-entropy loss function,  and rigid chat templates further restrict its creativity \citep{yun2025price}.  %\wyshi{add somehting like "past work also identified issues \cite{Wen2024LanguageML} in RLHF preference data collection but didn't link this to mode collapse."}
% \derek{Everything below this comment feels a bit redundant / in the wrong place? All were just covered in the intro.}
Our work complements existing studies by introducing a fundamental data-driven perspective, where we identify a pervasive data bias (i.e., \textit{typicality bias}) that exacerbates the algorithmic causes of mode collapse. 
% Moreover, we build on this theoretical contribution and propose a practical, inference-time prompting approach, \emph{\ours}, designed to recover the model's output diversity.

\paragraph{Methods to Improve Diversity.} Previous efforts to improve LLM diversity include training interventions~\citep{chung2025modifyinglargelanguagemodel, zhou2025bridgingcreativityunderstandinggap}, decoding strategies~\citep{holtzman2020curiouscaseneuraltext,lanchantin2025diversepreferenceoptimization} and prompting methods. For example, \cite{ismayilzada_creative_2025} introduced an alignment method for multifaceted creativity preferences. Decoding techniques like $\mu$-sampling~\citep{hewitt2022truncationsamplinglanguagemodel},  mirostat~\citep{basu2021mirostatneuraltextdecoding}, and \textit{min-p} sampling~\citep{nguyen_turning_2025} improve diversity by regulating the text perplexity during generation. 
However, these methods are either computationally expensive or restricted to open-sourced models. While prompting-based techniques offer a lightweight alternative~\citep{SummersStay2023BrainstormTS, mehrotra2024enhancingcreativitylargelanguage, tian2025macgyverlargelanguagemodels}, they often rely on prescriptive, handcrafted prompts \citep{zhang2024improvingdiversitycommonsensegeneration, shurofry2024growingtailincreasingoutput, ge2025scalingsyntheticdatacreation, lu2025benchmarkinglanguagemodelcreativity, wong2024simplestratdiversifyinglanguagemodel}. 
In contrast, our \ourslower is training-free, simple but principled, and broadly applicable.
% Compared to these methods, \ourslower is training-free, principled, and applicable to various models. 


Another line of work also uses LLMs to generate lists of responses or verbalize their knowledge in tasks like question answering \citep{tian_just_2023, xiong_can_2024,tao2024trust}, commonsense reasoning \citep{zhang2024improving}, survey simulations \citep{meister_benchmarking_2024} and synthetic data generation \citep{wang2023self, si2024can}. These methods mainly focused on empirical observation without theoretical grounding to fully leverage this verbalizing strategy; %these methods implicitly utilize distributional queries, but %can be viewed as an implicit. 
% have mainly focused on empirical observation without theoretical grounding to ask for a response distribution explicitly; 
our work proves that verbalizing the distribution and probabilities is the key towards diversity improvement, and our VS method enhances the performance over all baselines and also allows output diversity tuning. %as they address what we call a \textit{flat rewards} problem.

% Prior work has tried different prompting strategies to enhance output diversity. %with For instance, in-context learning \cite{zhang2024improvingdiversitycommonsensegeneration}, or adopting different personas \cite{ge2025scalingsyntheticdatacreation}. 
% For instance, \citet{zhang2024improvingdiversitycommonsensegeneration} utilized few-shot in-context examples, \citet{ge2025scalingsyntheticdatacreation} and~\citet{shurofry2024growingtailincreasingoutput} instruct the model to adopt different personas (e.g., ``Act as a skeptical scientist''), and \citet{lu2025benchmarkinglanguagemodelcreativity} condition the model on the past generation to improve diversity. \citet{wong2024simplestratdiversifyinglanguagemodel} asks the model to partition the solution space,  estimates how common each partition is, and then ask it to sample from them, to improve diversity. However, these methods rely on extra prior knowledge (e.g., few-shot examples, personas, a complex pipeline) to implicitly steer the model's output. In contrast, our work directly leverages the model's inherent capability to respond to explicit distributional queries to mitigate \emph{mode collapse}~\citep{hamilton2024detectingmodecollapselanguage}. 
% % \wyshi{can you better describe the difference here, it sounds very similar now, "verbally state the porpotion of answer choices, blah" I thikn you can mimic the last past of paragraph 3 in the introduction. }
% Another line of work also used LLMs to generate lists of responses or verbalize their knowledge in tasks like question answering \citep{tian_just_2023, xiong_can_2024}, commonsense reasoning \citep{zhang2024improving}, survey simulations \cite{meister_benchmarking_2024} and synthetic data generation \citep{wang2023self, dubois2023alpacafarm, zhu2025bareleveragingbaselanguage, si2024can}. These methods can also be seen as (simplified) form of \ourslower, but they mainly focused on empirical observation and lack the theoretical grounding to fully leverage this strategy; %these methods implicitly utilize distributional queries, but %can be viewed as an implicit. 
% % have mainly focused on empirical observation without theoretical grounding to ask for a response distribution explicitly; 
% our theoretical work suggests distributional queries are a better solution for improving diversity, as they address what we call a \textit{flat rewards} problem.





% While effective, these decoding methods share a fundamental limitation: they all require direct access to the model's logits, making them incompatible with most proprietary models with only API access. 
% % Besides, they operate on the token level and can be computationally expensive.
% Different from these decoding-based methods, our \ourslower method is simple, lightweight, and can work on both open-source and proprietary models: it requires only a moderate change in prompt to implement.

% Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion

% established
% principled truncation methods like $\mu$-sampling~\citep{hewitt2022truncationsamplinglanguagemodel} and adaptive methods like mirostat~\citep{basu2021mirostatneuraltextdecoding} or \textit{min-p} sampling~\citep{nguyen_turning_2025}, which regulate the text perplexity during generation. 

% Established techniques such as \texttt{top-k} sampling~\citep{fan2018hierarchicalneuralstorygeneration} and \texttt{top-p} (nucleus) sampling~\citep{holtzman2020curiouscaseneuraltext} moved beyond greedy decoding by sampling from a truncated probability distribution. More recent innovations include principled truncation methods like $\mu$-sampling~\citep{hewitt2022truncationsamplinglanguagemodel} and adaptive methods like mirostat~\citep{basu2021mirostatneuraltextdecoding} or \textit{min-p} sampling~\citep{nguyen_turning_2025}, which regulate the text perplexity during generation. While effective, these decoding methods share a fundamental limitation: they all require direct access to the model's logits, making them incompatible with most proprietary models with only API access. 
% % Besides, they operate on the token level and can be computationally expensive.
% Different from these decoding-based methods, our \ourslower method is simple, lightweight, and can work on both open-source and proprietary models: it requires only a moderate change in prompt to implement.


% But these methods are often computationally expensive or restricted to open models. While prompting-based techniques offer a lightweight alternative~\citep{SummersStay2023BrainstormTS, mehrotra2024enhancingcreativitylargelanguage, tian2025macgyverlargelanguagemodels}, they can depend on overly prescriptive, handcrafted prompts. %Meanwhile, recent work has empirically shown that base models are inherently diverse~\citep{west_base_2025, zhu2025bareleveragingbaselanguage}, suggesting that the post-training RLHF reduces this ability. Yet a theoretical understanding of this effect remains lacking. \wyshi{the flow is a bit off, should it be about other sources of mode collapse?}

% Previous efforts to improve LLM diversity have focused mainly on training interventions ~\citep{ ismayilzada_creative_2025, chung2025modifyinglargelanguagemodel, zhou2025bridgingcreativityunderstandinggap} 
% or decoding strategies~\citep{holtzman2020curiouscaseneuraltext,lanchantin2025diversepreferenceoptimization,nguyen_turning_2025}.
% %While these approaches can increase diversity, 
% But these approaches often require substantial computational resources, extra care during inference time, or only work on open-source models. 
% % Furthermore, training-based methods may undermines the safety benefits achieved by alignment~\citep{qi2024safetyalignmentjusttokens}, and decoding strategies often fail to fully address alignment-induced mode collapse~\citep{yang_how_2025}.
% Prompting-based techniques have emerged as a lightweight alternative~\citep{SummersStay2023BrainstormTS, mehrotra2024enhancingcreativitylargelanguage, wong2024simplestratdiversifyinglanguagemodel, tian2025macgyverlargelanguagemodels}, but many require manual, handcrafted prompts that can be overly prescriptive and complex. 
% On the other hand, recent studies empirically showed that base models  are actually capable of generating diverse outputs~\citep{west_base_2025, zhu2025bareleveragingbaselanguage}, suggesting that it is the post-training process that reduces its diversity. % by shifting probability mass toward safer but less varied responses. 
% Yet a theoretical understanding of this phenomenon is still lacking.



%Our work builds on these existing studies by identifying a pervasive data bias (i.e., \textit{typicality}) that compounds existing algorithmic causes of mode collapse in RLHF. Moreover, we build on this theoretical contribution with practical, inference-time prompting approach, \emph{\ours}, to recover the model's output diversity.

% KL-regularized optimization that amplifies majority modes \citep{xiao2024algorithmic} and to the inadequacy of single-reward modeling under heterogeneous preferences \citep{chakraborty2024maxmin}, while pre-RLHF choices—cross-entropy SFT that narrows token distributions \citep{li2024entropic} and rigid chat templating \citep{yun2025price}—further compress diversity. 

% However, these studies mainly focus on the empirical analysis of the phenomenon without providing theoretical understanding and a solution. In this work, we formally prove that RLHF causes mode collapse and grounded in the theory, propose a practical, inference-time prompting approach, \emph{\ours}, to recover models' diversity level. % recovers the full spectrum of an aligned model's potential responses, naturally eliciting both probable and more surprising answers without manual partitioning.


% Although mode collapse has been explored comprehensively, few works have actually tackled the problem. SimpleStrat~\citep{wong2024simplestratdiversifyinglanguagemodel} is a recent attempt that uses manually defined categories to guide the model's generation process. However, we argue that such manual intervention is unnecessary; models themselves
% can provide diverse outputs without external structuring.  \ours provides the key: a practical, inference-time technique that recovers the full spectrum of an aligned model's potential responses, naturally eliciting both probable and more surprising answers without manual partitioning.


%%%%%%%%%%%% original %%%%%%%%%%%%%
% \paragraph{Prompting Strategies for Diversity.} Prior work has tried different prompting strategies to enhance output diversity. %with For instance, in-context learning \cite{zhang2024improvingdiversitycommonsensegeneration}, or adopting different personas \cite{ge2025scalingsyntheticdatacreation}. 
% For instance, \citet{zhang2024improvingdiversitycommonsensegeneration} utilized few-shot in-context examples, \citet{ge2025scalingsyntheticdatacreation} and~\citet{shurofry2024growingtailincreasingoutput} instruct the model to adopt different personas (e.g., ``Act as a skeptical scientist''), and \citet{lu2025benchmarkinglanguagemodelcreativity} condition the model on the past generation to improve diversity. \citet{wong2024simplestratdiversifyinglanguagemodel} asks the model to partition the solution space,  estimates how common each partition is, and then ask it to sample from them, to improve diversity. However, these methods rely on extra prior knowledge (e.g., few-shot examples, personas, a complex pipeline) to implicitly steer the model's output. In contrast, our work directly leverages the model's inherent capability to respond to explicit distributional queries to mitigate \emph{mode collapse}~\citep{hamilton2024detectingmodecollapselanguage}. 
% % \wyshi{can you better describe the difference here, it sounds very similar now, "verbally state the porpotion of answer choices, blah" I thikn you can mimic the last past of paragraph 3 in the introduction. }
% Another line of work also used LLMs to generate lists of responses or verbalize their knowledge in tasks like question answering \citep{tian_just_2023, xiong_can_2024}, commonsense reasoning \citep{zhang2024improving}, survey simulations \cite{meister_benchmarking_2024} and synthetic data generation \citep{wang2023self, dubois2023alpacafarm, zhu2025bareleveragingbaselanguage, si2024can}. These methods can also be seen as (simplified) form of \ourslower, but they mainly focused on empirical observation and lack the theoretical grounding to fully leverage this strategy; %these methods implicitly utilize distributional queries, but %can be viewed as an implicit. 
% % have mainly focused on empirical observation without theoretical grounding to ask for a response distribution explicitly; 
% our theoretical work suggests distributional queries are a better solution for improving diversity, as they address what we call a \textit{flat rewards} problem.
% % \as{ATM, the theory doesn't corroborate this tail claim fully. With tweaks it may, but do we want to lean into the idea of diversity tuning? I think we can also still mention our contribution is the \textit{why} of it all.}

% \paragraph{Decoding Strategies for Diversity.}
% Besides prompting, changing the model's decoding strategies is another common approach to improve output diversity. %  sampling diversity can also be improved by altering the model's decoding strategy. 
% Established techniques such as \texttt{top-k} sampling~\citep{fan2018hierarchicalneuralstorygeneration} and \texttt{top-p} (nucleus) sampling~\citep{holtzman2020curiouscaseneuraltext} moved beyond greedy decoding by sampling from a truncated probability distribution. More recent innovations include principled truncation methods like $\mu$-sampling~\citep{hewitt2022truncationsamplinglanguagemodel} and adaptive methods like mirostat~\citep{basu2021mirostatneuraltextdecoding} or \textit{min-p} sampling~\citep{nguyen_turning_2025}, which regulate the text perplexity during generation. While effective, these decoding methods share a fundamental limitation: they all require direct access to the model's logits, making them incompatible with most proprietary models with only API access. 
% % Besides, they operate on the token level and can be computationally expensive.
% Different from these decoding-based methods, our \ourslower method is simple, lightweight, and can work on both open-source and proprietary models: it requires only a moderate change in prompt to implement.