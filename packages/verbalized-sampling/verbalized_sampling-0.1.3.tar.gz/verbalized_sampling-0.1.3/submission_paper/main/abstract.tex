% Reinforcement Learning from Human Feedback (RLHF) improves alignment but often reduces output diversity---a form of mode collapse. We identify a data-level driver: a typicality component \wyshi{in the intro and teaser we call it typicality bias} in human preferences (favoring familiar, fluent, schema-consistent text) that, under a standard KL-regularized objective, provably sharpens the reference policy and compresses probability mass toward stereotypical completions on semantic plateaus. Motivated by a view of prompting as constrained reporting, in which point prompts return a mode   whereas distribution prompts recover the model's latent predictive distribution, we introduce \textbf{Verbalized Sampling (VS)}: a simple, training-free prompting strategy that asks the model to output a small set of candidates with associated probabilities. This reframes instance queries as distributional ones and bypasses typicality-induced collapse. VS is model-agnostic and requires no access to logits. Across creative writing (poems, stories, jokes), multi-turn social dialogue simulation, and open-ended QA, VS increases diversity and produces broader, less-skewed answer distributions, while maintaining factual accuracy on commonsense reasoning tasks; we also observe that larger and reasoning-focused models benefit more. Overall, this work provides both a theoretical account of typicality-driven mode collapse and a practical, inference-time remedy that unlocks the base model's latent generative diversity.

% Post-training with human feedback (RLHF) can narrow the breadth of large language model (LLM) outputs—often called mode collapse. We identify a data-level driver: human raters prefer typical text. We formalize this by decomposing the latent reward as
% $r(x,y)=r_{\mathrm{sem}}(x,y)+\alpha\log\pi_{\mathrm{ref}}(y\mid x)+c(x)$,
% using $\log\pi_{\mathrm{ref}}$ as a tractable proxy for typicality. Under standard KL-anchored objectives, the optimizer is
% $\pi^\star(y\mid x)\propto \pi_{\mathrm{ref}}(y\mid x)^{\,1+\alpha/\beta}\exp\!\big(r_{\mathrm{sem}}(x,y)/\beta\big)$;
% on near-flat semantic slates this reduces to a power transform $\pi^\star\propto\pi_{\mathrm{ref}}^{\,1+\alpha/\beta}$ that sharpens any pre-existing skew and concentrates mass on prototypical responses. 
% We provide complementary evidence that $\alpha>0$: (i) across multiple preference datasets and non-RLHF base models, higher-likelihood candidates align with human choices above chance; (ii) on matched pairs and rating-level analyses, annotator preferences and Likert ratings positively track base-model token probabilities; and (iii) an ordinal regression on HelpSteer indicates log-likelihood predicts helpfulness even when controlling for correctness. \wyshi{usually in the abstract you don't get so detailed on the formula, you give intuitions and high-level ideas. the alpha may be included, but maybe more high-level like we analyzed several dataset and confirmed the parameters for this biased term} 
% Guided by this account, we introduce Verbalized Sampling (VS)—explicit distributional prompts that elicit $c$ candidates with verbalized probabilities, from which we sample. VS consistently increases semantic and lexical diversity in creative writing, better matches human distributions in dialogue simulation, and broadens answer distributions in open-ended QA without degrading judged quality, with stronger gains for larger models. As a training-free remedy, VS mitigates diversity loss while preserving alignment.

% Reinforcement Learning from Human Feedback (RLHF) is a key method in aligning large language models (LLMs) with human preferences. However, %a major challenge is that RLHF can significantly reduce the diversity of the model's outputs, a phenomenon known as \emph{mode collapse}. While recent studies have observed this empirically, we provide the first theoretical proof that RLHF indeed causes this issue. 
% % empirical observations show that it introduces a significant loss of output diversity, a phenomenon known as \emph{mode collapse}.
% recent studies observe that it can lead to a significant loss in output diversity, a phenomenon called \emph{mode collapse}. 

{Post-training alignment often reduces LLM diversity, leading to a phenomenon known as \emph{mode collapse}}. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: \emph{typicality bias} in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce \emph{\textbf{\ours (VS)}}, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities'').
Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1$\times$ over direct prompting. 
We further observe an emergent trend that more capable models benefit more from VS.
In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity. Our code is available at \url{https://github.com/CHATS-lab/verbalize-sampling}. 


% Intuitively, this simple reframing asks a question that is unaffected by human typicality bias.
% Intuitively, this simple reframing asks for a distribution a question that is unaffected by human typicality bias.
% this method operates by asking a question that is inherently unaffected by human typicality bias. 
% output diversity in creative writing (poem, story, and joke), simulates more human-like social dialogues,  generates broader answer distributions for open-ended questions, improves synthetic data generation, without sacrificing safety and factual accuracy on commonsense reasoning tasks. 
% attribute this phenomenon  -- the human cognitive bias to prefer more typical text -- which, in turn, biases the underlying preference data \wyshi{i think this sentence is a bit weak, the data bias is actually really important and not avoidable by clever algorithms, so we should sell that} to \textit{typicality bias}. Based on a theoretical formalism with complementary empirical verification, 

% As shown in our proof, this technique can break mode collapse because it explicitly instructs the model to sample from its original, diverse pre-training data. Our extensive experiments show that \ours significantly improves output diversity and simulation performance %while maintaining quality across multiple tasks and LLM families. Specifically
 %unlocks the base model's latent generative diversity
%our work provides both a theoretical foundation for understanding mode collapse in RLHF and a practical, scalable solution for improving diversity and simulation in aligned LLMs, unlocking . 
% Our code and data are available in \href{https://github.com/CHATS-lab/verbalize-sampling}{\texttt{https://github.com/CHATS-lab/verbalize-sampling}}.


% DC: This was the last version before the rework 250918
% Reinforcement Learning from Human Feedback (RLHF) is a key method in aligning large language models (LLMs) with human preferences. However, %a major challenge is that RLHF can significantly reduce the diversity of the model's outputs, a phenomenon known as \emph{mode collapse}. While recent studies have observed this empirically, we provide the first theoretical proof that RLHF indeed causes this issue. 
% % empirical observations show that it introduces a significant loss of output diversity, a phenomenon known as \emph{mode collapse}.
% recent studies observe that it can lead to a significant loss in output diversity, a phenomenon called \emph{mode collapse}. 
% In this paper, we attribute this phenomenon  -- the human cognitive bias to prefer more typical text -- which, in turn, biases the underlying preference data \wyshi{i think this sentence is a bit weak, the data bias is actually really important and not avoidable by clever algorithms, so we should sell that} to \textit{typicality bias}. Based on a theoretical formalism with complementary empirical verification, we introduce \textit{\textbf{\ours} }(VS), a simple yet principled prompting strategy that circumvents mode collapse by prompting the model to verbalize a probability distribution over responses (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities'').
% % As shown in our proof, this technique can break mode collapse because it explicitly instructs the model to sample from its original, diverse pre-training data. Our extensive experiments show that \ours significantly improves output diversity and simulation performance %while maintaining quality across multiple tasks and LLM families. Specifically
% Intuitively, this method operates by asking a question that is inherently unaffected by human typicality bias. In experiments, VS boosts output diversity in creative writing (poems, stories, and jokes), simulates more human-like social dialogues in multi-turn settings, and generates less biased and broader answer distributions for open-ended questions, without sacrificing factual accuracy on commonsense reasoning tasks. We also observe an \emph{emergent trend} that larger, more capable models benefit more from \ours. Our work provides both a theoretical foundation and a practical solution for mode collapse in RLHF, unlocking LLM potentials in creative writing,  social simulation, synthetic data generation, and various creative applications\footnote{Our code is submitted as supplementary materials.}. 

% Our code is submitted as supplementary materials.
% \derek{When you think about it, this actually unblocks / improves / formalizes a bunch of research areas. Silicon sampling, multi-agent dialogue, synthetic negatives, all kinds of creativity tasks – if we can find a strong framing, this is a really strong paper}
% \wyshi{add synthetic data}


% Large language models (LLMs) have achieved impressive capabilities. Yet, alignment methods such as Reinforcement Learning from Human Feedback (RLHF), while essential for aligning models with human preferences and ensuring safety, comes with major reduction in output diversity.
% % \wyshi{make it concise and readable}. 
% This leads to \emph{mode collapse} in subjective tasks. 
% Alignment methods such as Reinforcement Learning from Human Feedback (RLHF) are essential for making large language models (LLMs) safe and aligned with human preferences. However,  recent studies have empirically shown that RLHF can reduce output diversity %and concentrate on certain outputs 
% in tasks like creative writing and social simulation, leading to a phenomenon called \emph{mode collapse}. %\wyshi{Also, are we sure that "mode collapse" is the right term? It doesn't seems to be widely recognized?}. 
% In this work, we theoretically prove that indeed RLHF causes \emph{mode collapse}. Grounded in the proof, 
% we propose \textbf{\textit{\ours}}, a simple yet principled inference-time prompting strategy to effectively mitigate mode collapse and improve output diversity in aligned LLMs, without re-training or complex decoding. \ours works by prompting the model to generate a distribution of responses and explicitly \textbf{\emph{verbalize}} their associated probabilities (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities''). In this way, it elicits the model to sample from real distributions learned during pre-training to bypass mode collapse. 
% % reformulating the query as a distribution task, \ourslower elicits a high-fidelity approximation of the model’s latent knowledge, thereby mitigating the mode collapse. 
% % . We further provide a theoretical proof showing that, by reformulating the query as a distribution task, \ourslower elicits a high-fidelity approximation of the model’s latent knowledge, thereby mitigating the mode collapse. 
% Empirical results across different LLMs and tasks show that (1) on creative writing tasks like story, joke and poem generation, \ourslower significantly improves diversity while maintaining quality, (2) on a social dialogue simulation task, \ourslower simulates more human-like behaviors and increases the simulated dialogue's linguistic diversity; (3) on open-ended QA tasks with multiple valid answers, it also yields broader and a more balanced response distribution. 
% \wyshi{add synthetic data task}
% % \wyshi{do we really want to call this bias mitigation task?} 
% Moreover, experiments on commonsense reasoning tasks show that these gains are achieved without compromising truthfulness.
% We also observe an \emph{emerging trend} that larger, more reasoning-capable models tend to benefit more from \ourslower. In sum, our findings provide both a theoretical foundation for understanding mode collapse in RLHF and a practical, scalable solution to effectively improve diversity in aligned LLMs.
% % \wyshi{add the emergent trend} \simoncomment{@jiayi we properly don't want to only talk about deepseek-r1 here}
% % \wyshi{add the theoritical proof, we theoritically prove that VS is xxx} 
% Our code and data are available in \href{https://github.com/CHATS-lab/verbalize-sampling}{\texttt{https://github.com/CHATS-lab/verbalize-sampling}}.