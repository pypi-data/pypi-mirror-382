% \newpage

\section{Typicality Bias Causes Mode Collapse}
\label{sec:typicality}
%
%\as{TODO: clean, check math, and cross-check with previous arguments for Sec 3 and 4. structure with paragraph headers for sign-posting}
% \subsection{Motivation and intuition}
%
% \wyshi{stereotypical-->, what's actually hapenn, stereotypical is comgin from that, 
% prototypical: term people use}

% Aligned LLMs often become \emph{more stereotypical} than their base counterparts. 
In this section, we show that \emph{typicality bias} in human preference data is one pervasive cause of mode collapse. This bias sharpens the probability distribution towards a few stereotypical completions. When many high-quality completions are possible (e.g., in joke generation), this sharpening becomes a tie-breaker, resulting in mode collapse.

% LLMs aligned with human preferences often become \emph{more stereotypical} than their base counterparts. We hypothesis that this is because there exists that a \emph{typicality} component in human feedback sharpens probability onto a few stereotypical completions. When many high-quality completions are possible (e.g., in story generation), this sharpening from typicality becomes a tie-breaker, causing a form of mode collapse.



% ; when the semantic reward is locally flat \wyshi{i got a bit lost on the semantic reward because it hasn't been introduced yet}, this sharpening compresses probability mass onto stereotypical completions, i.e., \emph{instance‑mode collapse}.%
% This section consolidates the two earlier drafts (v01,v02), shortens the empirical evidence to the single ``sniff test'' (with controls moved to the appendix), and replaces the three‑panel figure with a 1‑column table as requested.

\subsection{Typicality Bias in Preference Data: Cognitive \& Empirical Evidence}
\label{sec:mc-typicality}
% \subsection{An Assumption with Cognitive and Empirical Basis}
% \as{Somebody can pick a better section headers than me}

% \wyshi{need to justify the fact that people like things, a combo of findings, }


\paragraph{Typicality Bias Hypothesis.} 

Cognitive psychology shows that people prefer text that is \textit{familiar}, \textit{fluent}, and \textit{predictable}. 
% Cognitive psychology shows that people favor text that is \emph{familiar}, \emph{fluent}, and \emph{predictable}. 
This preference is rooted in various principles. For instance, the \textit{mere‑exposure effect} ~\citep{zajonc1968attitudinal,bornstein1989exposure} and \textit{availability heuristic} ~\citep{tversky1973availability}  imply that frequent or easily recalled content feels more likely and is liked more. \textit{Processing fluency}~\citep{alter2009uniting,reber2004processing} suggests that %ease of processing is linked to perceptions of truth and quality.
easy-to-process content is automatically perceived as more truthful and higher quality. Moreover,  \textit{schema congruity} theory~\citep{mandler2014structure,meyers1989schema} predicts that information that aligns with existing mental models will be accepted with less critical thought. 
We therefore hypothesize that these cognitive tendencies lead to a \textit{typicality bias} in preference data, in which annotators systematically favor conventional text.
% So we hypothesize that during RLHF, annotators also exhibit such \textbf{typicality bias},  where they systematically prefer typical text. 
% Naturally, these together with other tendencies (e.g., confirmation bias, representativeness heuristic) push annotators to prefer typical text during RLHF \citep{tversky1974judgment,nickerson1998confirmation,hasher1977frequency}.



% Cognitive science shows that people systematically favor text that is \emph{familiar}, \emph{fluent}, and \emph{schema‑congruent}. For instance, the \textit{availability heuristic} \citep{tversky1973availability} and \textit{mere‑exposure effect} \citep{zajonc1968attitudinal,bornstein1989exposure} imply that frequent or easily recalled content feels more likely and is liked more. Similarly, processing fluent content inflates perceived truth, quality, and aesthetic appeal \citep{alter2009uniting,reber2004processing}. Moreover, Schema‑congruity accounts predict that responses fitting learned templates are judged more appropriate with less scrutiny \citep{mandler2014structure,meyers1989schema}. Related tendencies such as the representativeness heuristic, confirmation bias, and illusory truth effect further push rater preferences toward prototypical strings \citep{tversky1974judgment,nickerson1998confirmation,hasher1977frequency} \wyshi{can we shorten the citation?}. 


\paragraph{Modeling Rewards with Typicality Bias.} To capture this hypothesized bias, we model the reward function, which reflects human preferences, as a combination of \emph{true task utility} and \emph{typicality bias}. For a tractable proxy of typicality bias, we employ the log-likelihood from a pretrained base model, $\log \pi_{\mathrm{ref}}(y\mid x)$: as the base model has been trained to maximize likelihood on massive text corpora, its probability scores inherently capture text typicality. {Without loss of generality, we use the Bradley-Terry model common in RLHF~\citep{bradley1952rank,christiano2017deep,ouyang2022training} and formulate this combination in reward models in Eq.~\ref{eq:bt-assumption}}: 
% \derek{We're referencing the equation directly below? - I can see Overleaf cursors nearby, so maybe this is WIP, so I'm leaving it for now}

%We formulate this combination in Eq.~\ref{eq:bt-assumption} with the Bradley-Terry model common in RLHF~\citep{bradley1952rank,christiano2017deep,ouyang2022training}. 
% we posit that the reward model, which reflects human preferences,  combines \emph{true task utility} with the \emph{typicality bias}. We formulate this decomposition in Equation~\ref{eq:bt-assumption} with the Bradley-Terry model common in RLHF~\citep{bradley1952rank,christiano2017deep,ouyang2022training}. 




% because its extensive pretraining makes it a robust estimator typical text  \derek{check this sentence}
% because its extensive pretraining allows it to learn a robust the distribution of typical responses.
 % and use the base model's log‑likelihood $\log \pi_{\mathrm{ref}}(y\mid x)$ as a tractable proxy for typicality, because extensive pretraining allows 
 % the base model to serve as a strong estimator for typical text. 


\begin{equation}
r(x,y) \;=\; r_{\text{true}}(x,y) \;+\; \alpha \,\log \pi_{\text{ref}}(y \mid x) \;+\; \epsilon(x),
\label{eq:bt-assumption}
\end{equation}

where \(r_{\text{true}}\) is the true task utility, 
%\(\omega_{\text{ref}}\) is a pretrained base model policy, 
$\alpha$ is the typicality bias weight, 
and
\(\epsilon\) is a noise term. \(\alpha>0\) means that, \emph{holding the true utility fixed},
higher typicality bias increases the reward.


\paragraph{Verifying Typicality Bias in Preference Data.}  
We test this hypothesis on \textsc{HelpSteer}~\citep{wang2023helpsteer}, a preference dataset which provides per-response ratings for both \emph{correctness} (true task utility) and \emph{overall helpfulness} (the final reward). 
From the training set, we form $6{,}874$ pairs of responses to the same prompt with the same correctness ratings. We then compute their per-token log-likelihoods under both \emph{Llama~3.1~405B Base} and \emph{GLM~4.5 Base}, the base models used as $\pi_{\text{ref}}$. 
% Fitting the Bradley–Terry logistic model implied in Eq.~\ref{eq:bt-assumption} to predict which is the more helpful response, 
Fitting these values to Eq.~\ref{eq:bt-assumption}, yields $\hat{\alpha}=0.57\pm0.07$ and $0.65\pm0.07$ with the respective base models (both $p<10^{-14}$). This provides empirical evidence for a positive $\alpha$ in Eq.~\ref{eq:bt-assumption}, i.e., human raters are biased towards responses more typical for the base model, independent of correctness (true task utility). See \S\ref{app:evidence-controls} and \S\ref{appendix:preference_bias_base_model} for the verification experiments on more preference datasets.

% We then fit the Bradley–Terry logistic model implied by Eq.~\ref{eq:bt-assumption}. This model predicts which response receives higher helpfulness score. Then 
% As the main predictor, we used the difference in average log-likelihood $\Delta\bar{\ell}=\bar{\ell}_i-\bar{\ell}_j$, which measures how ``typical'' each response is to the base model. 
% The coefficient on  gives us an estimate of the bias, The coefficient on $\Delta\bar{\ell}$ is the estimate of $\alpha$: we obtain $\hat{\alpha}=0.57\pm0.07$ and $0.65\pm0.07$ with the respective base models (both $p<10^{-14}$). This provides empirical evidence for a positive $\alpha$ in Eq.~\ref{eq:bt-assumption}, i.e., human raters are biased towards responses more typical for the base model, independent of correctness (true task utility). See \cref{app:evidence-controls} for verification experiments on more preference datasets.
% with the binary outcome ``which response receives higher helpfulness'' and predictor $\Delta\bar{\ell}=\bar{\ell}_i-\bar{\ell}_j$ (difference in average log-likelihood under $\pi_{\text{ref}}$). The coefficient on $\Delta\bar{\ell}$ is the estimate of $\alpha$: we obtain $\hat{\alpha}=0.57\pm0.07$ and $0.65\pm0.07$ with the respective base models (cluster-robust SEs; both $p<10^{-14}$). 
% This provides direct evidence for a positive $\alpha$ term in Eq.~\ref{eq:bt-assumption}, i.e., human raters do exhibit bias towards base-model typicality,  independent of  correctness (true task utility).
% Additional analyses and experimental validation series are provided in \cref{app:evidence-controls}.
% The regression yields \(\hat{\alpha}\approx 0.51\) (p\(<\)0.001), empirically confirming the typicality bias. Consistent with this, experiments on additional preference datasets show that annotators systematically prefer responses with higher likelihood across different base models. See \cref{app:evidence-controls} for more experimental detail and results. 


% To verify our hypothesis, we perform empirical verification on \textsc{HelpSteer}, an preference dataset with ``correctness'' (the true task utility) and ``helpfulness'' (the final reward). We use matched pairs with the same correctness ratings, and the base LLM's average log-likehood as \(\log \omega_{\text{ref}}\).
% Then we fitting a within-prompt Bradley–Terry model with the reward model defined in eq1 \derek{TODO: find a better and simpler expression}. The results yields \(\hat{\alpha}\approx 0.51\) (p\(<\)0.001). A one–SD increase in \(\Delta\log p_{\text{ref}}\) raises the odds of being chosen as more helpful overall
% by \(37\%\) (OR \(=1.37\)), and win probability increases from \(\sim45\%\) to \(\sim61\%\) from -1SD to +1SD \wyshi{@derek check this},
% \emph{holding correctness fixed}. This empirically supports that \(\alpha>0\) predicted by \eqref{eq:bt-assumption}.
% (Details in App.~\S X.)%

% This assumption is consistent with data:
% on \textsc{HelpSteer}, matching pairs on correctness ratings and fitting a within-prompt Bradley–Terry model \derek{TODO: find a better and simpler expression} 
% \wyshi{with the reward model we defined above} 
% \(\Delta\log p_{\text{ref}}\) as the typicality regressor yields \(\hat{\alpha}\approx 0.51\) (p\(<\)0.001).
% A one–SD increase in \(\Delta\log p_{\text{ref}}\) raises the odds of being chosen as more helpful overall
% by \(37\%\) (OR \(=1.37\)), and win probability increases from \(\sim45\%\) to \(\sim61\%\) from -1SD to +1SD \wyshi{@derek check this},
% \emph{holding correctness fixed}. This directly supports \(\alpha>0\) predicted by \eqref{eq:bt-assumption}.
% (Details in App.~\S X.)%
% \footnote{Numbers from the S3 analysis note. We used matched pairs with equal correctness ratings
% and the base LLM’s average log-likelihood as \(\log \omega_{\text{ref}}\).}


% To model this hypothesized tendency, we posit a \emph{typicality bias} in RLHF data and use the base model's log‑likelihood $\log \pi_{\mathrm{ref}}(y\mid x)$ as a tractable proxy for typicality.
% For example, consider the prompt \emph{``Tell a joke about coffee.''} A base model places non‑trivial mass on many plausible punchlines; after RLHF, the aligned model disproportionately favors the most stereotypical answers. We capture this with a Bradley–Terry latent reward, as commonly assumed in RLHF \citep{bradley1952rank,christiano2017deep,ouyang2022training}. We posit that latent reward models (or the human preferences used to define them) mix \emph{semantic utility} \wyshi{true reward, need a broder name} with \emph{typicality}:
% \begin{equation}
% r(x,y) \;=\; r_{\text{sem}}(x,y) \;+\; \alpha \,\log \omega_{\text{ref}}(y \mid x) \;+\; \vartheta(x,y),
% \label{eq:bt-assumption}
% \end{equation}
% where \(r_{\text{sem}}\) is task/semantic utility, \(\omega_{\text{ref}}\) is a reference (base/SFT) policy, and
% \(\vartheta\) is zero-mean noise. \as{v(x) needs to be c(x) or eq 3 is not true. It might be better to absorb more responsibility into rsem and change the name.} The interpretation is simple: \(\alpha>0\) means that, \emph{holding semantics fixed},
% higher typicality (larger \(\log \omega_{\text{ref}}\)) increases latent reward. This assumption is consistent with data:
% on \textsc{HelpSteer}, matching pairs on correctness ratings and fitting a within-prompt Bradley–Terry model \derek{TODO: find a better and simpler expression} 
% \wyshi{with the reward model we defined above} 
% \(\Delta\log p_{\text{ref}}\) as the typicality regressor yields \(\hat{\alpha}\approx 0.51\) (p\(<\)0.001).
% A one–SD increase in \(\Delta\log p_{\text{ref}}\) raises the odds of being chosen as more helpful overall
% by \(37\%\) (OR \(=1.37\)), and win probability increases from \(\sim45\%\) to \(\sim61\%\) from -1SD to +1SD \wyshi{@derek check this},
% \emph{holding correctness fixed}. This directly supports \(\alpha>0\) predicted by \eqref{eq:bt-assumption}.
% (Details in App.~\S X.)%
% \footnote{Numbers from the S3 analysis note. We used matched pairs with equal correctness ratings
% and the base LLM’s average log-likelihood as \(\log \omega_{\text{ref}}\).}

% Encoding this intuition in a formal assumption, we consider the Bradley-Terry preference model commonly assumed in RLHF settings \as{CITE}. 
% \begin{equation}
% r(x,y)\ =\ r_{\text{sem}}(x,y)\ +\ \alpha\,\log \pi_{\mathrm{ref}}(y\mid x)\ +\ \varepsilon(x,y),
% \label{eq:typicality}
% \end{equation}
% where $r_{\text{sem}}$ captures task utility and $\varepsilon$ is zero‑mean noise. The interpretation is simple: $\alpha>0$ means that, \emph{holding semantics fixed}, higher typicality (larger $\log \pi_{\mathrm{ref}}$) increases latent reward. We find this assumption to be consistent empirically. Indeed, we verify this by fitting a Bradley Terry preference model on the HelpSteer dataset, utilizing 
% %\wyshi{[insert field]} 
% correctness ratings as a proxy for semantic reward. 
% % DC: and overall helpfulness as full reward - phrasing TBD
% Our findings show alpha = xxx (alpha > 0, p = …). This result, among other corroborating empirical evidence, is presented in Appendix X. 
% \as{Derek, can you please fill in missing details?}

\subsection{How Typicality Bias Causes Mode Collapse}
Having confirmed typicality bias, we need to show how it leads to mode collapse. The RLHF optimization objective under the Bradley-Terry model is as follows, 
% We study a standard KL‑regularized RLHF objective under the Bradley-Terry preference model:
% \begin{definition}[RLHF]
% The standard RLHF optimization objective is:
% \begin{equation}
% \pi_{\text{new}} = \arg\max_{\pi} \mathbb{E}_{x \sim D}\left[\mathbb{E}_{y \sim \pi(y|x)}[r(x,y)] - \beta \cdot \text{KL}(\pi(\cdot|x) \| \pi_{\text{old}}(\cdot|x))\right]
% \label{eq:rlhf_objective}
% \end{equation}
% \end{definition}
% \vspace{-0.5em}
\begin{equation}
\max_{\pi}\ \ \mathbb{E}_{x \sim \mathbb{D}, y\sim \pi(\cdot\mid x)}\!\big[r(x,y) -\;\beta\,\mathrm{KL}\!\big(\pi(\cdot\mid x)\,\|\,\pi_{\mathrm{ref}}(\cdot\mid x)\big) \big]\;,
\label{eq:objective}
\end{equation}
where \(\beta>0\) is the KL coefficient, \(\pi_{\text{ref}}\) is the reference policy (e.g., the base model), and $\pi$ is the learned policy. %\wyshi{we used these notations earlier to mean slightly different things, need to make them consistent}. \as{Response on slack}.
%
% whose unique optimum has the Gibbs form
% \begin{equation}
% \pi^*(y\mid x)\ \propto\ \pi_{\mathrm{ref}}(y\mid x)\,\exp\!\left(\frac{r(x,y)}{\beta}\right).
% \label{eq:optimum}
% \end{equation}
% \as{Remove this and mix into text with cites to save space}
% \subsection{Typicality hypothesis (data‑level bias)}
% We posit that reward models (or the human preferences used to train them) mix \emph{semantic utility} with \emph{typicality}:
% \begin{equation}
% r(x,y)\ =\ r_{\text{sem}}(x,y)\ +\ \alpha\,\log \pi_{\mathrm{ref}}(y\mid x)\ +\ \varepsilon(x,y),
% \label{eq:typicality}
% \end{equation}
% where $r_{\text{sem}}$ captures task utility and $\varepsilon$ is zero‑mean noise. The interpretation is simple: $\alpha>0$ means that, \emph{holding semantics fixed}, higher typicality (larger $\log \pi_{\mathrm{ref}}$) increases reward.
%

Plugging Eq.~\ref{eq:bt-assumption} 
%into~\eqref{eq:optimum} yields a \emph{power‑sharpened} optimum:
into the closed-form solution of Eq.~\ref{eq:objective} \citep{rafailov2024directpreferenceoptimizationlanguage}  yields an optimum, sharpened by $\gamma$ (derivation in \S\ref{app:power-sharpening}):
%\wyshi{i am not following this step}:
\begin{equation}
\pi^*(y\mid x)\ \propto\ \pi_{\mathrm{ref}}(y\mid x)^{\,\gamma}\ \exp\!\left(\frac{r_{\text{true}}(x,y)}{\beta}\right),\qquad
\gamma\ :=\ 1+\frac{\alpha}{\beta}\ >\ 1\ \ \text{when}\ \alpha>0.
\label{eq:power}
\end{equation}
So any positive typicality bias weight $\alpha$ strictly \emph{sharpens} the distribution of $\pi_{\text{ref}}$. Leaving all else fixed, larger $\alpha$ (stronger typicality in preference data) increases the strength of this effect.
% Two knobs govern the strength of this effect: larger $\alpha$ (stronger typicality in feedback) or smaller $\beta$ (tighter KL regularization) both increase $\gamma$.

% \wyshi{nonsense can also have the same reward}
% \wyshi{same true reward, same functioning}
Further, suppose there exists a subset $\mathcal{S}$ of responses such that for all $y,y'\!\in\!\mathcal{S}$\footnote{For example, we can restrict our analysis to $\mathcal{S}$ with only meaningful responses, because nonsensical or erroneous responses are unlikely to be sampled from a well-trained $\pi^*$. 
} we have 
flat true rewards, $r_{\text{true}}(x,y)=r_{\text{true}}(x,y')$\footnote{This assumption can be relaxed to approximate flatness. We just need bounds on the deviations of $r_{\mathrm{\text{true}}}$ between $y$ and $y'$ to claim mode collapse, but the overall argument (and result) is consistent. 
}. %\wyshi{what does the footnote mean?}
Then by Eq.~\ref{eq:power} the optimum within $\mathcal{S}$ reduces to
\[
\pi^*(\cdot\mid x)\ \propto\ \pi_{\mathrm{ref}}(\cdot\mid x)^{\,\gamma}\quad\text{on}\ \mathcal{S},
\qquad \gamma>1.
\]
% \as{Alternate opening: This behaves like temperature scaling. As $\gamma$ grows very large, we will have $y^* \in \arg\max_y \pi_\text{ref}(y \mid x)$ for all $y^* \sim \pi(\cdot | x)$ with $y^* \in \mathcal{S}$}.
%%% OLD OPENING
% As $\gamma > 1$, we will have $y^* = \arg\max_y \pi_\text{ref}(y \mid x)$ on $\mathcal{S}$.
%%% OLD OPENING END
This behaves like temperature scaling. As $\gamma$ grows very large, we will have $y^* \in \arg\max_y \pi_\text{ref}(y \mid x)$ for all $y^* \sim \pi(\cdot | x)$ with $y^* \in \mathcal{S}$.
This shows that the probability mass is \emph{compressed} toward typical completions (those already favored by $\pi_{\mathrm{ref}}$), yielding a form of \emph{mode collapse} on set $\mathcal{S}$. %\wyshi{temperature scaling}
Intuitively this means that, when many answers are tied on true task utility (a common scenario in creative writing, social simulation, etc), typicality bias acts as a tiebreaker that sharpens the output of the aligned model into the \textit{mode} of the base model. % \wyshi{@Anthony, check this}
% \begin{equation}
% \pi^*(y|x) \to \delta_{y^*} \quad \text{where } y^* = \arg\max_y \pi_\text{ref}(y \mid x)
% \quad\text{on}\ \mathcal{S}
% % add: on S
% \end{equation}

% So this means that on the set of $\mathcal{S}$, as $\gamma$ increases, the probability mass of $\pi^*(\cdot\mid x)$ is \emph{compressed} toward typical completions (those already favored by $\pi_{\mathrm{ref}}$), yielding a form of \emph{mode collapse}. %\wyshi{temperature scaling}
% Intuitively this means that, when many answers are tied on task utility (common in creative writing, social simulation, etc), typicality bias acts as a tiebreaker that sharpens the output into a narrow peak, causing mode collapse. 





% The sharpening exponent $\gamma > 1$ in Equation~\eqref{eq:sharpened} has a profound effect on the output distribution. As $\gamma$ increases, the distribution $\pi^*(y \mid x) \propto \pi_\text{ref}(y \mid x)^\gamma$ becomes increasingly concentrated on high-probability regions of $\pi_\text{ref}$—analogous to temperature scaling but in the opposite direction. In the limit $\gamma \to \infty$, the probability ratio between any suboptimal response $y$ and the mode $y^*$ vanishes as $[\pi_\text{ref}(y \mid x) / \pi_\text{ref}(y^* \mid x)]^\gamma \to 0$, causing $\pi^*$ to converge to a Dirac delta:
% AS: As gamma increases, probability shifts to the mode, so pi* is argmax
% WS: Shorten para into 1 sentence - as sharpening exponent gamma increases, distribution ... will become equation 4
% Wiser to avoid extensive formality
% Move to 200?


%\derek{Should this section end on the above conclusion? Anthony's subtle point below is insightful but I fear that it may confuse readers who are barely following the narrative. I wonder if we can sharpen this into a tighter bundle in the middle of the framing instead.}
%To observe mode collapse in practice, elements of $\mathcal{S}$ should not only share reward, but share a near optimal reward. Otherwise, the elements of S are unlikely to be sampled from pi star. The intuition remains that many high quality completions (jokes, poems, stories) can be semantically tied, and therefore, cause collapse.
% \as{Above can be more intuitive - I don't think we need Shannon entropy} \as{We are missing discussion about which slates actually cause the mode collapse we observe in practice (i.e., those with high reward and a real chance of being sampled)}

% \paragraph{Scope and caveats.} We expect local flatness in tasks with many equally valid answers (poem/story/joke generation, dialogue simulation, pluralistic QA), and not in sharply peaked domains with essentially unique correct outputs (e.g., single‑answer arithmetic). In the former, RLHF that inherits a typicality prior should reduce diversity (as we later observe); in the latter, typicality has little room to act. Finally, Eq.~\eqref{eq:typicality} is an approximation: $\log \pi_{\mathrm{ref}}$ can correlate with $r_{\text{sem}}$, which is precisely why our controlled analyses (ii)–(iii) live in the appendix.
%
% \subsection{Empirical validation}
% \as{Cut this and just talk about fitting bradley terry model empirically to save space (details in appendix)}
% To validate the modeling hypothesis made in \Cref{eq:typicality}, we test a base model’s average per-token log‑probabilities favor the \emph{chosen} label more often than chance on human preference datasets. Across four datasets and multiple base models, we observe statistically significant above‑chance agreement rates (Table~\ref{tab:evidence-i}), suggesting that typicality, proxied by $\log \pi_{\mathrm{ref}}$, is positively associated with preference labels.
%
% To further verify this hypothesis, we control for the possible confounding between $\log \pi_{\mathrm{ref}}$ and semantic utility $r_{\text{sem}}$, and in doing so, confirm empirically that $\alpha > 0$. We therefore include two additional analyses in App.~\ref{app:evidence-controls}: a controlled regression and a within‑prompt Bradley–Terry identification. In doing so, we isolate a data‑level cause of mode collapse—typicality‑induced sharpening on semantic plateaus, motivating querying \emph{distributions} rather than \emph{instances}, and predicting diversity gains on tasks with many valid outputs.
%
%
% % Requires \usepackage{booktabs}
% \begin{table}[t]
% \centering
% \small
% \caption{Agreement of base-model log-probabilities with the ``chosen'' option across public preference datasets. We reported the mean (min–max) agreement averaged across four base models (Gemma‑3‑4B/27B, Qwen‑3‑4B‑Base, and Llama‑3.1‑8B/70B). Base-models show consistent above-chance preference; detailed per-model results with 95\% CIs are reported in~\Cref{app:evidence-controls}. 
% % Base‑model log‑probability prefers the ``chosen'' option above chance across public preference datasets. Values are mean (min–max) agreement averaged across four base models (Gemma‑3‑4B/27B, Qwen‑3‑4B‑Base, and Llama‑3.1‑8B/70B); full per‑model results and 95\% CIs appear in App.~\ref{app:evidence-controls}.
% }
% \label{tab:evidence-i}
% \begin{tabular}{@{}l r@{}}
% \toprule
% Dataset & Agreement (\%) \\
% \midrule
% Summarize from Feedback~\citep{stiennon2022learningsummarizehumanfeedback} & 53.8\ \ (51.6--56.4) \\
% UltraFeedback (binarized)~\citep{cui2024ultrafeedbackboostinglanguagemodels}      & 58.8\ \ (57.5--60.2) \\
% HelpSteer~\citep{wang2023helpsteermultiattributehelpfulnessdataset}               & 58.6\ \ (56.2--60.8) \\
% Skywork Reward Preference~\citep{liu2024skyworkrewardbagtricksreward}      & 59.9\ \ (58.8--61.7) \\
% \bottomrule
% \end{tabular}
% \vspace{-0.25em}
% \end{table}
%
% DC: Reminder to self: Verify these average post mailout
%
%
% \newpage
%
% \section{Connecting Typicality with Mode Collapse}
% \label{sec:typicality}
%
% \subsection{Why human feedback prefers ``typical'' text}
% People systematically favor text that is \emph{familiar}, \emph{fluent}, and \emph{schema‑congruent}. The availability heuristic and mere‑exposure effect imply that frequent or easily recalled content feels more likely and is liked more \citep{tversky1973availability,zajonc1968attitudinal,bornstein1989exposure}. Processing fluency inflates perceived truth, quality, and aesthetic appeal \citep{alter2009uniting,reber2004processing}. Schema‑congruity accounts predict that responses fitting learned templates are judged more appropriate with less scrutiny \citep{mandler2014structure,meyers1989schema}. Related biases (representativeness, confirmation bias, illusory truth) further push raters toward prototypical strings \citep{tversky1974judgment,nickerson1998confirmation,hasher1977frequency}. We therefore posit a \emph{typicality preference} in RLHF data and use the base model’s log‑likelihood $\log \pi_{\mathrm{ref}}(y\mid x)$ as a tractable proxy for typicality.
%
% \subsection{Setup and fixed point}
% \label{subsec:setup}
% Fix a prompt space $\mathcal{X}$ and a finite candidate set $\mathcal{Y}$. For $x\in\mathcal{X}$, let $\pi_{\mathrm{ref}}(\cdot\mid x)$ denote a fixed reference policy (e.g., the SFT/base model) and $\pi(\cdot\mid x)$ the trainable policy during post‑training. With a scalar reward $r(x,y)$, the standard KL‑regularized single‑prompt objective is
% \begin{equation}
% J(\pi)
% \;=\;
% \mathbb{E}_{y\sim \pi(\cdot\mid x)}\!\big[r(x,y)\big]
% \;-\;
% \beta\,D_{\mathrm{KL}}\!\big(\pi(\cdot\mid x)\,\|\,\pi_{\mathrm{ref}}(\cdot\mid x)\big),
% \qquad \beta>0,
% \label{eq:rlhf}
% \end{equation}
% whose maximizer over $\pi(\cdot\mid x)$ has the familiar exponential‑tilt form
% \begin{equation}
% \pi^*(y\mid x)
% \;=\;
% \frac{\pi_{\mathrm{ref}}(y\mid x)\,\exp\!\big(r(x,y)/\beta\big)}
% {\sum_{y'\in\mathcal{Y}}\pi_{\mathrm{ref}}(y'\mid x)\,\exp\!\big(r(x,y')/\beta\big)}.
% \label{eq:fixpoint}
% \end{equation}
%
% \subsection{Typicality hypothesis (data‑level bias)}
% \label{subsec:typicality-hypothesis}
% Guided by the cognitive evidence above, we model human preference data as mixing semantic utility with typicality:
% \begin{equation}
% \boxed{\;
% r(x,y)\;=\;r_{\mathrm{sem}}(x,y)\;+\;\alpha\,\log \pi_{\mathrm{ref}}(y\mid x)\;+\;c(x),
% \quad \alpha>0\; }
% \label{eq:typicality}
% \end{equation}
% where $r_{\mathrm{sem}}$ captures task utility and $\log \pi_{\mathrm{ref}}$ proxies how \emph{typical} a response already is under the reference.\footnote{For clarity we omit annotator‑specific noise; our results go through under bounded perturbations and with standard random‑effects augmentations.} 
% Substituting \eqref{eq:typicality} into \eqref{eq:fixpoint} yields
% \begin{equation}
% \pi^*(y\mid x)\;\propto\;\big[\pi_{\mathrm{ref}}(y\mid x)\big]^{\,\gamma}\,
% \exp\!\big(r_{\mathrm{sem}}(x,y)/\beta\big),
% \qquad \gamma \;\equiv\; 1+\alpha/\beta \;>\; 1.
% \label{eq:power}
% \end{equation}
% Thus, whenever $r_{\mathrm{sem}}$ is (approximately) flat over a slate of near ties, $\pi^*(\cdot\mid x)$ reduces to a \emph{power-transformed sharpening} of $\pi_{\mathrm{ref}}$ on that slate.
%
% \subsection{When sharpening becomes collapse}
% \label{subsec:sharpening-collapse}
% Consider creative or loosely specified tasks (e.g., jokes, stories, poems) where many completions are near‑equivalent in semantic utility. Let $S\subseteq\mathcal{Y}$ collect such near ties with $r_{\mathrm{sem}}(x,y)\approx r_0$ for all $y\in S$. Then \eqref{eq:power} implies
% \begin{equation}
% \pi^*(\cdot\mid x)\Big|_{S}\;\propto\;\pi_{\mathrm{ref}}(\cdot\mid x)^{\,\gamma},
% \qquad \gamma>1,
% \end{equation}
% so any pre‑existing skew in $\pi_{\mathrm{ref}}$ over $S$ is \emph{strictly sharpened}. If the reference mode lies inside $S$, probability mass concentrates on that prototype as $\alpha/\beta$ grows—precisely the \emph{mode collapse} phenomenon observed after alignment.
%
% \paragraph{Proposition 1 (Power‑transform sharpening).}
% \emph{Fix $x$ and a slate $S\subset\mathcal{Y}$ with near‑flat $r_{\mathrm{sem}}(x,y)\approx r_0$. Under the typicality model \eqref{eq:typicality} and KL‑RLHF, the optimizer restricted to $S$ satisfies $\pi^*\!\big|_S \propto \pi_{\mathrm{ref}}^{\,\gamma}$ with $\gamma=1+\alpha/\beta>1$. Hence any skew in $\pi_{\mathrm{ref}}$ is strictly amplified; if $\arg\max_{y\in S}\pi_{\mathrm{ref}}(y\mid x)\in S$, then $\pi^*$ collapses toward that mode as $\alpha/\beta$ increases.} \hfill\(\square\)
%
% \subsection{Empirical evidence that \texorpdfstring{$\alpha>0$}{alpha>0}}
% \label{subsec:evidence-alpha}
%
% We test the key implication of \eqref{eq:typicality}—that base-model likelihood predicts human preference beyond semantics—at three levels (Fig.~\ref{fig:triangulation}):
%
% \begin{description}
% \item[(i) Global correlations across datasets and base models.] Using \emph{non-RLHF} base models (Gemma-3-4B/27B, Qwen3-4B, Llama-3.1-8B/70B), the item with higher $\log p$ under the base model matches the human-preferred item \emph{above chance} on four public preference datasets:
% \begin{itemize}
% \item Summarize-from-Feedback: 52.4–56.4\% agreement,
% \item UltraFeedback (binarized): 57.5–60.2\%,
% \item HelpSteer: 56.2–60.8\%,
% \item Skywork-Reward: 58.8–61.7\%.
% \end{itemize}
% (95\% CIs are tight; per-model tables are in \Cref{appendix:preference_bias_base_model}.)
% \item[(ii) Controlled regression (HelpSteer).] On the HelpSteer validation split ($n{=}1{,}789$), an \emph{ordinal logistic regression} predicts overall helpfulness ratings (1–5) from \textbf{correctness} (1–5) and \textbf{average base-model log-likelihood}. Correctness is the dominant predictor (OR $\approx 27$, $p<10^{-3}$), but log-likelihood remains a \emph{significant independent} predictor (OR $\approx 1.33$, $p<10^{-4}$). A likelihood-ratio test confirms that adding log-likelihood improves fit ($\chi^2(1)=21.4$, $p<10^{-5}$). These results indicate that annotators reward higher model confidence even after accounting for correctness.
% \item[(iii) Within-prompt Bradley–Terry identification of $\alpha$.] On HelpSteer, we construct matched pairs with the same prompt and \emph{equal correctness} but different helpfulness ($n{>}5{,}000$ pairs). A within-prompt Bradley–Terry analysis using \(\Delta \log p\) from a base model estimates \(\hat{\alpha}\approx 0.51\) ($p<10^{-3}$): a +1\,SD increase in \(\Delta\log p\) raises the odds of being chosen as more helpful by \(\sim\)37\% (OR=1.37), with win probability rising from \(\sim45\%\to\sim61\%\) from $-1$SD to $+1$SD, \emph{holding correctness fixed}.
% \end{description}
%
% \begin{figure}[t]
%   \centering
%   % --- placeholder panels (replace rules with actual includes) ---
%   \begin{subfigure}[t]{0.32\linewidth}\centering
%     \setlength{\fboxsep}{0pt}\fbox{\rule{0pt}{2.2cm}\rule{0.96\linewidth}{0pt}}
%     \caption{Global agreement rates by dataset$\times$model.}
%     \label{fig:triangulation:global}
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.32\linewidth}\centering
%     \setlength{\fboxsep}{0pt}\fbox{\rule{0pt}{2.2cm}\rule{0.96\linewidth}{0pt}}
%     \caption{Matched pairs \& Likert correlation.}
%     \label{fig:triangulation:pairs}
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.32\linewidth}\centering
%     \setlength{\fboxsep}{0pt}\fbox{\rule{0pt}{2.2cm}\rule{0.96\linewidth}{0pt}}
%     \caption{Within‑prompt BT: win‐prob vs.\ $\Delta\log p$.}
%     \label{fig:triangulation:bt}
%   \end{subfigure}
%   \caption{\textbf{Evidence that $\alpha>0$ (triangulation).}
%   \textbf{A:} Across four preference datasets and five non-RLHF base models, the higher-likelihood item aligns with the human-preferred item above chance (52–62\%; 95\% CIs).
%   \textbf{B:} \emph{HelpSteer} ordinal logistic regression: correctness is the dominant predictor (OR$\approx$27), but \emph{base-model log-likelihood remains independently significant} (OR$\approx$1.33; LR $\chi^2(1){=}21.4$, $p{<}10^{-5}$).
%   \textbf{C:} \emph{HelpSteer} within-prompt Bradley–Terry on matched pairs with equal correctness estimates $\hat\alpha\approx 0.51$; +1\,SD in $\Delta\log p$ raises choice odds by $\sim$37\% (win prob $\sim$45\%$\to$61\%). Appendix reports SFF matched-pair (55.0\%) and Likert–probability correlation ($r{=}0.22$) as supportive evidence.}
%   \label{fig:triangulation}
% \end{figure}
%
% Together these results support $\alpha>0$ in real preference data and empirically justify the power‑transform form in \eqref{eq:power}. Full per‑dataset$\times$model tables, regression specifications (including proportional‑odds checks and length controls), and BT estimation details appear in Appendix.
%
% \subsection{Takeaways and bridge to \S\ref{sec:method}}
% The theory predicts—and our experiments later confirm—that collapse is most acute when: (1) \textbf{reward slates are flat} (creative/pluralistic tasks admit many near‑equivalent completions, so $\pi^*$ inherits a sharpened $\pi_{\mathrm{ref}}^\gamma$ over $S$); and (2) \textbf{the reference mode is ``good enough''} (if the reference mode lies in $S$, $\pi^*$ concentrates there as $\alpha/\beta$ grows; Proposition~1). These are exactly the settings (poems, stories, jokes; dialogue continuations; open‑ended QA with many valid answers) where we observe narrowed output distributions after RLHF.
%
% \begin{quote}
% \textbf{Box 1 — Different modes for different queries.}
% \emph{Instance‑level prompts} on near‑flat slates pick a \emph{prototype}: the sharpened mode of $\pi_{\mathrm{ref}}$. \emph{Distribution‑level prompts} make distributional fidelity the target (``return $c$ candidates with probabilities''), removing flat‑reward pathologies and exposing the underlying diversity—this motivates our method in \S\ref{sec:method}.
% \end{quote}
%
%
%
%
% \newpage
% % DC: Existing draft: Merging in progress 
% \section{Connecting Typicality with Mode Collapse} \label{sec:proof}
%
% % \wyshi{TODO: biggest change, need to read this}
% % \paragraph{Typicality in Cognitive Science.} \as{We can probably use a variant of existing... I'll leave this for someone more familiar with this part.}
% % DC: Done, above
%
%
% \paragraph{Setup.} Consider a fixed prompt space $\mathcal{X}$ and finite response set $\mathcal{Y}$. With $y \in \mathcal{Y}$ and $x \in \mathcal{X}$, let $\pi_{\mathrm{ref}}(\cdot\mid x)$ denote a fixed reference policy over strings in $\mathcal{Y}$ (typically the SFT model), let $\pi(\cdot\mid x)$ be a learnable policy, and let $r(x,y)$ be a latent, scalar reward in context of the Bradley-Terry preference model. Then, the standard KL‑regularized objective in RLHF is given below:
% \begin{equation}
% J(\pi)\;=\;\mathbb{E}_{x \sim \mathbb{D}, y\sim\pi(\cdot\mid x)}\!\big[r(x,y)\;-\;\beta\,D_{\mathrm{KL}}\!\big(\pi(\cdot\mid x)\,\|\,\pi_{\mathrm{ref}}(\cdot\mid x)\big)\;\big],\qquad \beta>0
% \label{eq:rlhf}
% \end{equation}
% where $\mathbb{D}$ is the data distribution over the prompt space $\mathcal{X}$. For fixed latent reward $r$, the well-known maximizer over choice of $\pi$ is defined:
% \begin{equation}
% \pi^*(y\mid x)\;=\;\frac{\pi_{\mathrm{ref}}(y\mid x)\,\exp\!\big(r(x,y)/\beta\big)}{\sum_{y'}\pi_{\mathrm{ref}}(y'\mid x)\,\exp\!\big(r(x,y')/\beta\big)}.
% \label{eq:fixpoint}
% \end{equation}
% This closed-form solution underlies recent analyses of the impact \emph{algorithmic bias} during RLHF and subsequent preference collapse, motivating existing preference‑matching alternatives to KL regularization \citep{xiao2024algorithmic}.
% \paragraph{Typicality Hypothesis} Contrary to this existing work, the main contribution of our theoretical inquiry is exploring the role of \textit{data bias} in RLHF, specifically that stemming from a well-known cognitive bias in humans. We hypothesize mode collapse may also arise from \textit{typicality bias}. To make this claim analytically tractable, we'll assume typicality presents in the latent reward $r$, which is implicitly determined by human annotators and the data they generate. The exact process we assume is described below:
% \begin{equation}
% r(x,y)\;=\;r_{\mathrm{sem}}(x,y)\;+\;\alpha\,\log \pi_{\mathrm{ref}}(y\mid x)\;+\;c(x),\qquad \alpha>0.
% \label{eq:typicality}
% \end{equation}
% Here $r_{\mathrm{sem}}$ captures semantic quality (i.e., an objective task utility), while $\log \pi_{\mathrm{ref}}$ proxies how typical a response already is under the reference model. Essentially, this postulates two points: (i) that the latent rewards motivating human preferences are in fact composed of \textit{both} a functional utility for the human and a spurious correlation capturing typicality bias, and (ii) typicality bias is well-represented by the reference model's probability distribution.
% \paragraph{Verifying the Hypothesis.} While the typicality hypothesis has significant backing in cognitive science literature, the functional form we've assumed is novel. We take a largely empirical approach to verifying the plausibility of the functional form in Eq.~\eqref{eq:typicality}.
% \as{Insert a description of the empirical study and basic results pointing to a figure.}
% \paragraph{Narrowing the Scope of Inquiry.}
% To arrive at a specific realization of mode collapse in RLHF, we need to specify the types of problems where this might occur, and moreover, where mode collapse is a problem. Specifically, the contexts where we prioritize output diversity from LLMs are often creative in nature or have purposefully loose specifications. These can include tasks like story, joke, and poem writing or tasks like synthetic data generation (e.g., synthetic dialogues). We observe a key trait that is common to all of these tasks. For a fixed prompt $x$, they can have \textbf{flat semantic rewards}:
% \begin{equation}
%     \exists \ \mathcal{S} \subset \mathcal{Y}, \ \forall \ y, y' \in \mathcal{S} \ : \ r_{\mathrm{sem}}(x,y) = r_{\mathrm{sem}}(x,y').
% \end{equation}
% That is, the functional utility of many completions can be equivalent or near equivalent:\footnote{We assume identical latent rewards, but approximate flatness could be modeled via an $\epsilon$-sized bound on how $r_\mathrm{sem}$ varies. The behavior of the RLHF model would remain consistent, adding slack dependent on $\epsilon$.} many jokes are funny, many stories can be equally engaging, and many dialogues may be plausible reflections of an actual human exchange.
% % simplifying, cause this isn't what we want idt
% % Second, more specifically, the \textit{mode} of the reference model $\pi_\textrm{ref}$ may be part of such a set that has flat semantic rewards. Specifying the previous assumption, this means:
% % \begin{equation}
% %     \exists \ \mathcal{S} \subset \mathcal{Y}, \ \forall \ y, y' \in \mathcal{S} \ : \ r_{\mathrm{sem}}(x,y) = r_{\mathrm{sem}}(x,y') \quad \text{and} \quad y_\textrm{mode} = \textrm{arg}\max\nolimits_y \ \pi_{\mathrm{ref}}(y\!\mid\!x) \in \mathcal{S}.
% %     \label{eq:assume}
% % \end{equation}
% % This makes intuitive sense. For creative tasks, a completion with high probability under the reference model is likely to have a certain level of quality (i.e., it is not totally random at least), and so many other completions might have that same level of quality. For example, the most likely joke ``about coffee'' under $\pi_\textrm{red}$ is also likely to have a similar quality as many other jokes ``about coffee.''
% \paragraph{Observation of Mode Collapse under Typicality.}
% Plugging Eq.~\ref{eq:typicality} into Eq.~\ref{eq:fixpoint} for fixed $x$ yields: 
% \begin{equation}
% \pi^*(y\mid x)\;\propto\;\big(\pi_{\mathrm{ref}}(y\mid x)\big)^{\,1+\alpha/\beta}\,\exp\!\big(r_{\mathrm{sem}}(x,y)/\beta\big).
% \label{eq:power}
% \end{equation}
% Now, assume we have a set $\mathcal{S}$ with semantically flat rewards. Then,
% \begin{equation}
% \forall y \in \mathcal{S} \ : \ \pi^*(y\!\mid\!x)\propto \pi_{\mathrm{ref}}(\cdot\!\mid\!x)^{\gamma}
% \label{eq:sharpen}
% \end{equation}
% with $\gamma=1+\alpha/\beta>1$. Already, we see that any pre‑existing skew in $\pi_{\mathrm{ref}}$ is \emph{sharpened} for completions in $\mathcal{S}$. This means, \textit{we already have a form of collapse}. On any slate of near ties $\mathcal{S}$, the RLHF model $\pi^*$ is now \textit{even more likely} to return the mode of $\pi_\textrm{ref}$ when compared to the original reference model. The extent of this sharpening is exponential in the bias parameter $\alpha$, indicating a potentially stark re-weighting of probability. We arrive at full-on mode collapse in cases where $\textrm{arg}\max_y \pi^*(y|x) \subset \mathcal{S}$ and $\mathcal{S}$ has nearly flat rewards. Flat reward sets, even containing optimal completions, are realistic in many creative generation tasks, since it can be hard to differentiate functional utility.
% \derek{I'm not sure what you mean by "full-on mode collapse"?}
% \as{Could use an example at the end}
% \as{we can wrap in theorem-proof parlance, but I think I prefer the narrative style better.}

% \newpage


% \paragraph{How to circumvent mode collapse?}
% \derek{I think this should actually be in the next subsection}
% Our theoretical study demonstrates how mode collapse may come about from typicality, especially in creative generation tasks where we'd actually like to prioritize sample diversity. Specifically, it suggests that within any group of completions that all share a similar semantic reward, we observe a form of mode collapse that is exponential in the bias parameter $\alpha$. More generally, \textit{every} set of completions $\mathcal{S}$ with highly similar rewards will experience a sharpening that shifts probability to just a single element of $\mathcal{S}$. When stuck in this situation, \textit{how can we extract a good solution?} Our proposal is to restructure our prompt to work with the mode collapsed model. Specifically, consider the case where we ask the model for a distribution over completions: ``write 10 different stories about pie and report their respective probabilities.'' The important feature of this prompt is that it explicitly requests a sequence of distinct items. Even if the mode of the policy is returned, it will \textit{still} capture some level of sample diversity, e.g., as long as every story is actually distinct. As noted in the related works, the idea of requesting a sequence-like object (e.g., a list of respondents) has been studied before with demonstrated empirical success. 
% \paragraph{Motivating Distribution-level Queries}
% However, our theory adds a complementary observation: the impact of flat semantic rewards. Indeed, in cases where completions can exhibit flat rewards, we observe the policy $\pi^*$ places greater weight on more typical completions (Eq.~\ref{eq:sharpen}) as quantified by their probability under the reference model. Prompts that ask for a list of respondents fall into this trap. For example, \textit{[Tokyo, Paris, Rome]} and \textit{[New York, Amsterdam, Munich]} are both semantically correct respondents, if asked for ``a list of three cities.'' Our proposed modification -- requesting a probability distribution -- enforces a more strict notion of semantic correctness. The response must also encode correct probability information. In turn, latent semantic rewards will be proportional to how accurate the respondent story probabilities are and any set of completions will have varying (non-flat) rewards based on accuracy. Maintaining reward granularity avoids purely typical responses, which we expect to be less diverse.
% \as{Not sure how clean this is - probably can be improved but wanted to get something down to help differentiate from other sequence methods}.

% \newpage