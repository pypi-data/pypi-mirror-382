\section{Creative Writing}\label{sec:creative_writing}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/creative_writing/unified_creativity_w_diversity_tuning.pdf}
    \caption{
    % \textbf{a-c:} Semantic diversity scores in poem continuation (\textbf{a}) story generation (\textbf{b}) and joke writing (\textbf{c}) across different methods. 
    \textbf{a-c: Average semantic diversity scores} (\%) in poem  (\textbf{a}), story (\textbf{b}) and joke (\textbf{c}) across methods and models. Our methods consistently outperform the baselines. We performed a one-tailed t-test between VS-Standard and the baselines (* $p<0.05$, ** $p<0.01$, *** $p<0.001$). 
    % One-tailed t-test shows VS-Standard significantly outperforms the baselines (** $p<0.01$).
    \textbf{d: Diversity vs. Quality trade-off} for the poem task, where VS-Multi and VS-CoT approach the Pareto front. \textbf{e-f: Emergent Trend} where larger models benefit more from VS. We show differences in diversity \textbf{(e)} and quality \textbf{(f)} over Direct across small (GPT-4.1-Mini, Gemini-2.5-Flash) and large (GPT-4.1, Gemini-2.5-Pro) models. 
    \textbf{g-i: Tunable Diversity} shows the diversity tuning results on Gemini-2.5-Flash across tasks. Unlike baseline methods in dashed lines, we can tune the diversity level with VS: as the probability threshold decreases, diversity increases.
  % \wyshi{the y-axis of e and f "delta diversity against direct" is odd, maybe check with Gemini? "delta in diversity against xx"? i am not sure. diversity delta?} 
  % \wyshi{what do these diversity scores mean intuitively? add reference that use this metrics, also need more qualitative examples in the appendix}
  % \wyshi{the sign test results are different than before?}
  % \wyshi{change the font for g-i title to align with others}
  % \wyshi{increase the x and y-axis' font size, too small now}
  % \wyshi{change a color for f, if they are all purple, i wouldn't notice that the y in f is quality difference, i would think that they are all divesity difference}
%\cmcomment{For figure 2, what are the diversity scores? Are they percentages or what? That is, you say in the text above in section 5 that semantic diversity score is normalized to [0,1], but you’re showing numbers like “12”. Moreover, if semantic diversity is 1 - an average cosine similarity, then does it really need normalization? You’d be in the range of [0,1] unless the cosine score went negative (theoretically possible but tends not to happen in such language applications). Couldn’t you just truncate the cosine scores at 0, which is pretty common?}
  \vspace{-1em}
  % \wyshi{can we use a better and more consistent color for e and f}
   % \as{Is this lexical or semantic diversity? Consider putting this type of info in all captions.}
   % \as{Also, in a previous version I believe the error bars of the the model comparison were fairly large. Did we determine why that was? \simoncomment{The reason for the large error bars is that we averaged between the Gemini and GPT models, and the error bar reflected their performance difference.}}
   }
    \label{fig:creativity_main}
\end{figure}
% \wyshi{maybe consider a bold move to start with the task of "random number generation" and bia mitigation, instead of creativity tasks. Since these biased task really makes our point about mode collapse, but Figure~\ref{fig:training_progression} also proves this point. 


% \jiayicomment{Need to define clearly about output diversity/creativity.}
% \wyshi{also mention the joke and story task, you can have a separate section briefing the findings on these tasks. lots of good experiments buried in the appendix} \simoncomment{Will do once the experiment results are clear}

% \wyshi{how did you choose the number of candidates? you two need to align your writing together, in the dialogue simulation task, it's mentioned, but not in the creative writing task?}

% We begin with creativity tasks, as they are a good measure of a language model's output diversity. 
Following prior work on LLM diversity~\citep{lu2025aihumanityssalieriquantifying}, we first study three creative writing tasks: poem continuation, story generation, and joke writing.


\paragraph{Benchmarks.} We evaluate model performance on three benchmarks. For \text{(1) poem continuation} and \text{(2) story generation}, we follow the text continuation  setup in \citet{lu2025aihumanityssalieriquantifying}, and use poems from PoemHunter.com and stories from the BookMIA dataset \citep{shi2024detectingpretrainingdatalarge} for experiments.
% we followed the same setup as the Creativity Index  \wyshi{on xx xx, very short and specific description}~\citep{lu2025aihumanityssalieriquantifying} that used poems collected by PoemHunter.com and Story collected from BookMIA (Shi et al., 2024) dataset. 
For \text{(3) joke writing}: we follow \citet{turgeman2025jokeruleallimpossibility} and curate 100 thematic prompts from the Reddit r/DadJokes dataset~\citep{reddit_dad_jokes_2023}, each structured as ``Write me a joke about [topic]'' (e.g., ``...about an octopus''). 
% to obtain a dataset with 100 curated thematic prompts from the Reddit r/DadJokes dataset~\citep{reddit_dad_jokes_2023} with the following structure: ``Write me a Joke about [Topic]'' (e.g., ``Write me a Joke about Octopus''). 
To reduce computation costs, we randomly select 100 data points for these three tasks, and apply \ourslower to generate $k=5$ candidates and $N=30$ total samples for each data point. Detailed prompts are provided in \Cref{appendix:experiment_prompt}.


\paragraph{Evaluation.}
We evaluate all methods on two metrics: \textit{diversity} and \textit{quality}. (1) For diversity, we assess both semantic and lexical levels: (i) For semantic diversity, we follow prior work~\citep{cox2021directed,cann2023usingsemanticsimilaritytext,lu2025aihumanityssalieriquantifying,zhu2025bareleveragingbaselanguage} and calculate $1 - \bar{s}$, where $\bar{s}$ is the mean pairwise cosine similarity of response embeddings (generated using OpenAI's \texttt{text-embedding-3-small} model). Negative similarities are clipped to 0 to avoid inflating diversity and we present the final score as a percentage, where 100\% represents maximum diversity. 
% and use the following metrics:  $1 - $ the average pairwise cosine similarity of response embeddings (from OpenAI's \texttt{text-embedding-3-small} model). If the cosine similarity is negative, we clip it to 0, so the range is [0,1]. We express the final score in percentage, with 100\% as the maximum diversity.
% Then we normalize this score to $[0, 1]$, where 0 represents semantic identity and 1 represents max diversity.
% The similarity score is transformed to a normalized diversity score in the range $[0, 1]$, where 0 represents semantic identity and 1 represents max diversity. 
(ii) For lexical diversity, we follow~\cite{shaib2025standardizingmeasurementtextdiversity} and use ROUGE-L~\citep{lin-2004-rouge}, where lower scores indicate greater diversity. 
% \wyshi{did we follow some prior work to use ROUGE-L? instead of type/token ratio} \simoncomment{\citep{shaib2025standardizingmeasurementtextdiversity}}
% \as{Lexical diversity often refers to a type/token ratio -- I'm not sure if this is the same as ROUGE-L}. 
(2) To evaluate output quality, we use Claude-3.7-Sonnet as the judge. We score \textit{Poem} and \textit{Story} with the rubrics from Creative Writing v3~\citep{paech2023eqbench}, and jokes with the Humor grader rubrics from HumorBench~\citep{narad2025llmsjokeprobingnonstem}. See \Cref{app:evaluation} for details on evaluation.




\subsection{Results}
% \begin{figure}[t]
%   \centering
%   \begin{subfigure}{0.48\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/creative_writing/poem/method_average_diversity.pdf}
%     % \caption{Average diversity across all models for different generation methods. VS-CoT and VS-Multi achieve significantly higher diversity (12.9\% and 11.6\%) compared to baseline methods. \wyshi{is it fair to compare VS-CoT to sequence? we can also have sequence-CoT, right}}
%     % \label{fig:average_diversity}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.48\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/creative_writing/poem/diversity_vs_quality_average.pdf}
%     % \caption{Diversity-quality trade-offs for four frontier LLMs across different generation methods. Stars indicate Pareto optimal configurations that achieve the best balance between diversity and quality for each model.}
%     % \label{fig:diversity_quality}
%   \end{subfigure}
%   \caption{\textbf{(Left)} Average diversity across all models for different prompting methods on \textbf{Poem Writing}. VS-CoT and VS-Multi achieve significantly higher diversity (12.9\% and 11.6\%) compared to baseline methods. \wyshi{is it fair to compare VS-CoT to sequence? we can also have sequence-CoT, right}
%   \textbf{(Right)} Diversity-quality trade-offs across different prompting methods, averaged across models. Stars indicate Pareto optimal configurations that achieve the best balance between diversity and quality for each model. \wyshi{is this really the pareto optimal? its diversity is not the highest, right?} \wyshi{low priority: make the figure prettier, with seaborn e.g.}
%   }
%   \label{fig:diversity_main_plots}
% \end{figure}

% The strongest \wyshi{this is a subjective claim, just say that xx shows the result} results  of \ourslower are summarized in Figure~\ref{fig:diversity_main_plots}, with all experimental results in \Cref{tab:model_comparison_creativity} and \Cref{tab:joke_writing_performance}. \wyshi{Figure xx(a) shows the  diversity score across all models for different methods.} 
% There are several takeaways: as shown in the left of the \Cref{fig:diversity_main_plots}, VS-CoT achieves the highest average diversity at 12.9\%, more than doubling the Direct baseline (5.7\%) and outperforming traditional approaches like CoT (6.1\%) and Multi-turn (7.5\%). $p$-test results show that the improvement is statistically significant compared to baselines.

% \wyshi{just say 3b} The diversity-quality scatter plots in the right panel of Figure~\ref{fig:diversity_main_plots} reveal that VS-Multi method consistently achieve Pareto optimal configurations across frontier models. \wyshi{Figure 3b is the diversity-quality scatter plot. It shows that our VS-Multi method achieved the pareto optimal, with both a high diversity and a high quality score. }

% === Statistical Tests for Poem ===
% Direct**: VS-Standard (10.93) vs Direct (5.69), t=5.196, p=0.0000
% CoT**: VS-Standard (10.93) vs CoT (6.11), t=4.743, p=0.0001
% Sequence: VS-Standard (10.93) vs Sequence (9.13), t=1.280, p=0.1095
% Multi-turn**: VS-Standard (10.93) vs Multi-turn (7.46), t=2.967, p=0.0045

% === Statistical Tests for Story ===
% Direct**: VS-Standard (17.36) vs Direct (11.09), t=5.315, p=0.0000
% CoT**: VS-Standard (17.36) vs CoT (11.60), t=4.986, p=0.0001
% Sequence**: VS-Standard (17.36) vs Sequence (14.79), t=1.974, p=0.0330
% Multi-turn**: VS-Standard (17.36) vs Multi-turn (13.01), t=3.578, p=0.0013

% === Statistical Tests for Joke ===
% Direct**: VS-Standard (30.78) vs Direct (12.21), t=8.854, p=0.0000
% CoT**: VS-Standard (30.78) vs CoT (13.72), t=8.305, p=0.0000
% Sequence: VS-Standard (30.78) vs Sequence (30.53), t=0.243, p=0.4057
% Multi-turn**: VS-Standard (30.78) vs Multi-turn (27.63), t=2.902, p=0.0052


% \wyshi{can you either have three paragraphs, or put them together in one paragraph, instead of lumping story and joke together. it's not balanced. }
%   \paragraph{Poem Continuation} Figure~\ref{fig:creativity_main}a shows the diversity and quality results on poem
%   continuation, averaged across models. Results on individual model families are in \Cref{tab:model_comparison_creativity} and
%   \Cref{tab:joke_writing_performance}. VS-Standard achieves significant improvements over most baseline methods, nearly doubling Direct performance ($p < 0.001$ by
%   one-tailed t-test) and significantly outperforming CoT ($p < 0.001$) and Multi-turn ($p = 0.005$). The Sequence method shows
%   improvement over simpler baselines but remains statistically comparable to VS-Standard ($p = 0.11$). \Cref{fig:creativity_main}\textbf{(d)} presents the diversity-quality trade-off plot. The VS-Multi method achieves the highest
%   diversity while maintaining high quality scores, demonstrating that increased diversity does not compromise output quality. This demonstrates that our \ours
%    can boost output diversity without hurting quality.

%   \paragraph{Story Generation and Joke Writing} Figure~\ref{fig:creativity_main}\textbf{(b-c)} demonstrates that VS effectiveness
%   extends consistently across creative writing domains. For story generation \textbf{(b)}, VS-Standard significantly outperforms all
%   baseline methods: Direct and CoT ($p < 0.001$), Sequence ($p = 0.033$), and Multi-turn ($p = 0.001$). VS-CoT reaches the highest
%   diversity score, representing a 47\% improvement over the best baseline. For joke writing \textbf{(c)}, the statistical advantage is even more pronounced. VS-Standard significantly outperforms Direct, CoT
%   ($p < 0.001$), and Multi-turn ($p = 0.005$), while showing comparable performance to Sequence ($p = 0.41$). VS methods consistently
%   dominate the top performance ranges across all tasks. Statistical analysis across all creative writing tasks ($N = 8$ models per comparison) reveals robust improvements, with
%   particularly strong effects in poem ($t = 5.20$) and joke writing ($t = 8.85$ vs Direct). This consistent pattern validates
%   the generalizability of our approach while preserving output quality, as shown in
%   \Cref{tab:model_comparison_creativity_story} and \Cref{tab:joke_generation}.

% \simoncomment{Explain why Multi better than CoT, but when VS-Multi under-perform CoT}


\paragraph{Diversity Score.} Figure~\ref{fig:creativity_main}\text{(a)}-\text{(c)} show the semantic diversity score averaged across models on poem, story, and joke, respectively. %Results on lexical diversity and individual model families are in \Cref{tab:model_comparison_creativity}. 
Across tasks, VS-Standard consistently and significantly outperforms baseline methods. The variants, VS-CoT and VS-Multi, further improve generation diversity. Detailed results on lexical diversity and individual model families are in \Cref{tab:model_comparison_creativity}.

\paragraph{Diversity vs. Quality.} \Cref{fig:creativity_main}\text{(d)} shows the diversity-quality trade-off on the poem task. The quality of VS-Standard remains comparable to other methods. Notably, VS-CoT achieves the highest diversity while maintaining a high quality score, pushing the Pareto front of this trade-off~\citep {zhang-etal-2021-trading}. This shows that VS can boost diversity without harming quality. See \Cref{appendix:creativity} for the diversity-quality trade-offs for the story and joke tasks.


% The quality of VS-Standard drops a little bit, but is still comparable to other methods. VS-CoT achieves the highest diversity while maintaining a high quality score, pushing forward the Pareto-front of the ``diversity-quality tradeoff''~\citep {zhang-etal-2021-trading}. This demonstrates that VS can boost output diversity without hurting quality. See \Cref{appendix:creativity} for complete results on the diversity and quality tradeoff for story and joke tasks.




\paragraph{Emergent Trend.}\label{sec:emergent_behavior}
We observe an emergent trend where larger models benefit more from VS. \Cref{{fig:creativity_main}}\text{(e)} shows the diversity gain over the direct prompting which suffers from mode collapse. 
Across all VS variants, larger models (GPT-4.1, Gemini-2.5-Pro) achieve diversity gains \text{1.5 to 2 times greater} than smaller models (GPT-4.1-Mini, Gemini-2.5-Flash). %This representation effectively highlights the net improvement over the baseline, which can suffer from mode collapse. 

% \wyshi{where is cognitive burden? it's in FIg2, but i cannot easily map it to the text}
% This scaling trend also extends to quality, as shown in \Cref{fig:creativity_main}\textbf{(f)}. Prior work \citep{yang_how_2025} found that more complex prompts can cause a ``cognitive burden''  that degrades LLM performance, and we observe similar phenomena that compared to a simple direct prompting baseline, more complex prompts like Sequence and VS-Standard will cause a drop in quality, but it is less severe for larger models. Moreover, more intricate methods (VS-CoT and VS-Multi) actually break this cognitive burden and improves the quality for larger models. This may suggest that eliciting a distribution in VS better utilizes the stronger capabilities of larger models, turnning potential complexity into a benfitl. 


\paragraph{Cognitive Burden.} This scaling trend also extends to quality, as shown in \Cref{fig:creativity_main}\text{(f)}. While prior work \citep{hu_fine-tuning_2024} found complex prompts create a ``cognitive burden'' that degrades LLM performance, our findings are nuanced. Methods like Sequence and VS-Standard do cause a drop in quality, but this effect is less severe for larger models. Notably, more intricate variants like VS-CoT and VS-Multi overcome this burden, even improving quality in larger models. This suggests using VS variants may better utilize the capabilities of advanced models, turning complexity into benefits.

% Prior work found that complex prompts can create a ``cognitive burden'' that degrades LLM performance. We observe a similar phenomenon: compared to the simple direct prompting baseline, more complex prompts like Sequence and VS-Standard cause a drop in quality, although this effect is less severe for larger models. Notably, more intricate methods like VS-CoT and VS-Multi appear to overcome this cognitive burden, even improving quality for larger models. This suggests that eliciting a response distribution via VS may better utilize the advanced capabilities of larger models, turning potential complexity into a benefit.

% \paragraph{Diversity Tuning.} Unlike baseline methods, VS allows us to tune the output diversity by adjusting the probability threshold directly in the prompt (e.g., ``Generate five responses with probabilities below \{threshold\}''), without altering decoding parameters. As shown in \Cref{fig:creativity_main}\textbf{(h-j)}\wyshi{todo}, diversity increases as the probability threshold decreases. In contrast, with baseline methods like sequence, we cannot adjust the diversity level. See \Cref{sec:ablation_diversity_tuning_creativity} for more detailed results.

% \paragraph{Diversity Tuning.} We can also tune the output diversity by adjusting the probability threshold directly in the prompt (e.g., ``Generate five responses with probabilities below \{threshold\}''), without altering decoding parameters. As shown in \Cref{fig:creativity_main}\textbf{(h-j)}\wyshi{todo}, diversity increases as the probability threshold decreases. In contrast, with baseline methods like sequence, we cannot change the diversity level. See \Cref{sec:ablation_diversity_tuning_creativity} for more detailed results.

\paragraph{Diversity Tuning.} Unlike baseline methods, VS allows us to tune the output diversity by adjusting the probability threshold directly in the prompt (e.g., ``Generate five responses with probabilities below \{threshold\}''), without altering decoding parameters. As shown in \Cref{fig:creativity_main}\text{(g-i)}, diversity increases as the probability threshold decreases. See \Cref{sec:ablation_diversity_tuning_creativity} for more detailed results.

\paragraph{Qualitative examples.} Figure~\ref{fig:qualitative} and Table~\ref{tab:qualitative_story_expanded} show qualitative examples on the story task. VS produces more creative stories than direct prompting. See more qualitative examples in \S\ref{appendix:qualitative}.


\begin{table*}[t]
\small
\centering
\caption{Qualitative comparison for story generation. For the prompt ``Without a goodbye'', the direct method consistently generates stories about a romantic partner's disappearance. In contrast, our method (VS) demonstrates more creative diversity, generating stories that explore modern communication, metaphorical endings, and cosmic-scale events. 
% \wyshi{update this now we only have 3 examples}
}
\label{tab:qualitative_story_expanded}
\resizebox{\linewidth}{!}{
\begin{tabular}{p{0.48\textwidth} | p{0.48\textwidth}}
\toprule
\multicolumn{2}{c}{\textit{Instruction: ``Please write a short story starting with the following prompt: Without a goodbye,''
}} \\
\midrule
\multicolumn{1}{c}{\textbf{Direct}} 
& \multicolumn{1}{c}{{\textbf{Our Method (VS)}}}
% \wyshi{a weird line in the middle, style is different from the poem table}
\\
\midrule
% --- Direct Examples ---
\textbf{Example 1} \newline
Without a goodbye, Elara vanished. Her side of the bed was cold, a faint indentation in the pillow the only evidence she'd ever been there. Liam stared at it, a knot tightening in his stomach. No note, no hurried text, just a gaping silence where her vibrant presence used to be. He searched the apartment, a frantic, methodical sweep. Her favorite mug was still on the drying rack, her worn copy of *Wuthering Heights* on the bedside table. Nothing seemed out of place, yet everything felt terribly wrong.

\vspace{1em}\hrule\vspace{1em}

\textbf{Example 2} \newline
Without a goodbye, Elara vanished. Her side of the bed was cold, the coffee mug she’d used that morning still on the nightstand, a faint lipstick stain on the rim. Liam found the note taped to the fridge, a single, elegant script: ``Free at last.'' He stared at it, the words blurring, then sharpening, then blurring again. Free from what? From him? From the life they'd built, over fifteen years? The house felt too big, too silent.

\vspace{1em}\hrule\vspace{1em}

\textbf{Example 3} \newline
Without a goodbye, Elara vanished. Her side of the bed was cold, the coffee machine untouched. A half-read book lay open on the nightstand, a bookmark tucked precisely between pages 107 and 108; a detail that always irritated Liam. She was meticulous, almost to a fault. But this: this was not meticulous. This was an absence, stark and sudden.He checked her usual haunts: the community garden where she nurtured prize-winning tomatoes, the dusty antique shop where she hunted for forgotten treasures, the library where she volunteered on Tuesdays.
&
% --- VS Examples ---
\textbf{Example 1} \newline
Without a goodbye, the email landed in my inbox. It was from Mark, my best friend since kindergarten... The subject line was simply: ``Moving On.'' ... The message itself was a concise, almost clinical statement that he was relocating to Patagonia... There was no ``sorry for the short notice,'' no ``I'll miss you,'' not even a simple ``good luck.'' ... It was a clean cut, a sudden erasure that left me suspended in disbelief, grappling with the phantom limb of a friendship that had simply, inexplicably, ceased to be.


\vspace{1em}\hrule\vspace{1em}

\textbf{Example 2} \newline
% Without a goodbye, the old lighthouse keeper, Silas, simply vanished. One morning, the rotating beam failed to cut through the pre-dawn gloom. The villagers, accustomed to its rhythmic sweep, felt a prickle of unease. Young Elara, who brought Silas his weekly provisions, found the cottage door ajar, a half-eaten bowl of porridge on the table, and his worn oilskin coat still hanging on its peg. The sea, a silent accomplice, offered no answers.
Without a goodbye, the last star flickered out. It wasn't a violent explosion, no supernova spectacle, just a gentle fade, like a candle finally exhausting its wick. The universe, which had always hummed with the silent symphony of distant suns, suddenly became an echoing void. I was the last observer, adrift in my tiny, self-sustaining craft, my instruments registering only an infinite, inky blackness.

\vspace{1em}\hrule\vspace{1em}

\textbf{Example 3} \newline
Without a goodbye, the music simply stopped. Not a fade-out, not a final crashing cymbal, but an abrupt, jarring silence that left the dancers frozen mid-step. The DJ, a wizened man named Frankie who had spun records at `The Blue Note' for forty years, had simply collapsed behind his turntables. One moment, the room throbbed with a soulful blues riff; the next, only the murmur of shocked whispers filled the air. \\

% \vspace{1em}\hrule\vspace{1em}

% \textbf{Example 4} \newline
% Without a goodbye, the last star flickered out. It wasn't a violent explosion, no supernova spectacle, just a gentle fade, like a candle finally exhausting its wick. The universe, which had always hummed with the silent symphony of distant suns, suddenly became an echoing void. I was the last observer, adrift in my tiny, self-sustaining craft, my instruments registering only an infinite, inky blackness. \\
\bottomrule
\end{tabular}
}
\end{table*}

% VS also enables \textbf{tunable diversity}, providing fine-grained control over the quality-diversity trade-off directly via prompting. By manipulating probability thresholds in the prompt, we can steer generation without changing decoding parameters. Full results are in Appendix \Cref{sec:ablation_diversity_tuning_creativity,sec:diversity_tuning_open_ended_qa}.



% test different variations of prompt format, vary how we elicit the distribution in the prompt (e.g., ``generate five responses with their corresponding probability/perplexity/percentage''), and show that there is no significant different as long as it's an explicit distributional prompt, \wyshi{so in the following experient, we stick with the most standard format of asking for the probability}. In all these ablations, VS consistently outperforms the direct and sequence baseline with the same setup.

% achieves better diversity-quality trade-off than sequence when we choose different number of candidates


% Our method mitigates mode collapse during RLHF training and is orthogonal to generation parameters like the {number of candidates}, temperature, top-$p$, and min-$p$. This allows them to be used in combination to steer the quality-diversity trade-off. Detailed analyses are in Appendix \Cref{sec:ablation_mitigation} and \Cref{sec:ablation_decoding_strategies}.

% \paragraph{Ablation on Probability Formats.}
% We tested seven formats for verbalizing probabilities and found that while the optimal choice is task-dependent, using \textbf{Explicit} probabilities or \textbf{Confidence} scores are consistently strong strategies. The full analysis is presented in Appendix \Cref{sec:ablation_probability_format}.



% \wyshi{add the tunable here?}
% \wyshi{in the internal review, several questions on the ablation on decoding methods, etc. we should mention that they are in the appendix.}

% \paragraph{Ablation on temperature, $top-p$, $min-p$ and RLHF stages~\citep{sec:ablation_mitigation}} \Cref{appendix:hyperparameters} \simoncomment{1-2 sentences summarize}

% \paragraph{Ablation on Probability Formats} \Cref{sec:ablation_probability_format}

% \paragraph{Ablation on Diversity tuning} \Cref{sec:ablation_diversity_tuning_creativity} for Creativity; \Cref{sec:diversity_tuning_open_ended_qa} for Open-ended QA
% the structured reasoning of VS leverages the greater capacity of these models, turning potential complexity into a benefit. 
%noted cognitive Notably, while more complex prompts can sometimes create a ``cognitive burden'' that degrades performance as in Sequence and VS-Standard~\citep{yang_how_2025}, especially for smaller models. In contrast, this drop is less significant for larger models, and more intricate methods (VS-CoT and VS-Multi) yield quality \textit{improvements} in larger models. This suggests that the structured reasoning of VS leverages the greater capacity of these models, turning potential complexity into a benefit.
% \paragraph{Poem Continuation} 
% Figure~\ref{fig:creativity_main}a shows the diversity and quality results on poem continuation, averaged across models. Results on individual model families are in \Cref{tab:model_comparison_creativity} and \Cref{tab:joke_writing_performance} \wyshi{missing link}. VS-Standard achieves significant improvements over most baseline methods, nearly doubling Direct performance ($p < 0.001$ by one-tailed t-test) and significantly outperforming CoT ($p < 0.001$) and Multi-turn ($p = 0.005$). %The Sequence method shows improvement over simpler baselines but remains statistically comparable to VS-Standard ($p = 0.11$). 
% \Cref{fig:creativity_main}\textbf{(d)} presents the diversity-quality trade-off plot. The VS-Multi method achieves the highest diversity while maintaining high quality scores, pushing forward the Pareto-front of the ``quality-diversity tradeoff''~\citep {zhang-etal-2021-trading}. This demonstrates that our \ours can boost output diversity without hurting quality.

% \paragraph{Story Generation} 
% Figure~\ref{fig:creativity_main}\textbf{(b)} demonstrates that VS methods substantially improve diversity in story generation. VS-Standard significantly outperforms all baseline methods, including Direct, CoT, Multi-turn, and Sequence. Among all methods, VS-CoT achieves the highest diversity, improving by nearly 30\% over the strongest baseline. These results confirm that VS methods effectively enhance creativity in story generation without compromising quality.

% \paragraph{Joke Writing} 
% Figure~\ref{fig:creativity_main}\textbf{(c)} shows an even stronger advantage for joke writing. VS-Standard significantly outperforms Direct, CoT, and Multi-turn baselines, achieving performance comparable to Sequence. Among all variants, VS-CoT again reaches the highest diversity, slightly exceeding the best baselines and other VS methods. 

% These results demonstrate that VS methods consistently gain better performance against baselines. The complete results for Poem, Story, and Joke are under \Cref{appendix:creativity}.


 % Conversely, smaller models show more modest improvements and occasional quality trade-offs, indicating that the cognitive burden of following multiple instructions and probability estimation may challenge less capable models~\citep{hu_fine-tuning_2024, yang_how_2025}. These results collectively demonstrate that \ours represents a significant advancement in controllable text generation, offering substantial improvements across diverse model architectures while maintaining the quality-diversity balance essential for creative tasks.



\subsection{Human Study on Diversity}
\begin{wrapfigure}{r}{0.5\textwidth}
    \captionof{table}{Human-rated diversity (1 = Very Similar, 4 = Very Dissimilar) for poem, story, and joke tasks under Direct, Sequence, and VS-Standard.%\wyshi{can we do t-test on this?}
    }
    \label{tab:human_study_diversity}
    \resizebox{0.50\textwidth}{!}{
    \centering
    \begin{tabular}{lccc}
    \toprule
    \textbf{Task} & \textbf{Direct} & \textbf{Sequence} & \textbf{VS-Standard}\\
    \midrule
    Poem  & 1.90 & 2.07 & \textbf{2.39} \\ 
    Story  & 2.74 & 2.76 & \textbf{3.06} \\
    Joke  & 1.83 & 2.93 & \textbf{3.01} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-1em}
\end{wrapfigure}

To complement our automatic diversity scores, we conducted a human evaluation on Prolific. Following past work, we provided task-specific diversity definitions (plot, style and setup-punchline, respectively). 
For each task, 30 annotators rated the diversity of 90 output pairs from three prompting methods (Direct, Sequence, VS-Standard)  across ten curated topics. 
Each pair was rated on a four-point Likert scale adopted from \citet{chen-etal-2022-semeval}: Very Similar, Somewhat Similar, Somewhat Dissimilar, or Very Dissimilar. Inter-annotator agreement was moderate for poems (0.54), high for stories (0.87) and jokes (0.86). 
% We calculated the
% inter-annotator agreement on two topics per task, yielding scores of  and 0.54 for poems \wyshi{why did this number change?, it was 0.6 something before?}, 0.87 for stories, 0.86 for jokes.
Table~\ref{tab:human_study_diversity} shows that VS achieves higher diversity than the baselines on all tasks.
See \S\ref{appendix:human_study_creativity} for more details on the human study.

\subsection{Ablation Study}\label{sec:ablation_study}

In this section, we present two ablation studies on the poem task in detail. First, we ablate various post-training stages (SFT, RLHF, RLVR) and show empirical evidence that post-training causes mode collapse and VS can indeed mitigate it and reduce the loss of diversity compared with other methods. Second, we ablate the temperature and show that VS's performance gains are orthogonal to temperature scaling, allowing the two to be combined to further improve the diversity-quality trade-off. %For additional ablation studies on more decoding strategies like top-p and min-p, please see \Cref{sec:ablation_decoding_strategies}.


% A key feature of Verbalized Sampling is that it is orthogonal to the decoding strategy, creating an opportunity to further enhance generation diversity. In this section, we ablate these combinations, specifically layering our method with temperature \citep{ACKLEY1985147}, top-p \citep{holtzman2020curiouscaseneuraltext}, and a recent effort called min-p sampling \citep{nguyen_turning_2025}, to systematically analyze their impact on the quality-diversity trade-off. We show the temperature in and leave the top-p and min-p experiments in the \Cref{sec:ablation_decoding_strategies}.

\paragraph{Ablation on VS across post-training stages} 
\begin{wrapfigure}{r}{0.50\textwidth}
    \captionsetup{skip=2pt} 
    \vspace{-1.4em}
    \centering
    \includegraphics[width=\linewidth]{figures/creative_writing/poem/ablation/training_progression_diversity.pdf}
    \caption{
    \textbf{Diversity scores across post-training stages of Tulu-70B.} ``Tulu-Final-70B'' is the model after RLVR. The red dashed line indicates the base model's diversity level (45.4\%). Baseline
  prompting methods experience major diversity drops (\textit{mode collapse}) after SFT and DPO, 
  with direct prompting showing the most severe drop. In contrast, VS maintains a higher
  diversity scores throughout all training stages, demonstrating that it can mitigate \emph{mode collapse}.
  \vspace{-1em}
    }
    \label{fig:training_progression}
\end{wrapfigure}
We employ the Tulu-3 family~\citep{lambert2025tulu3pushingfrontiers} , which contains checkpoints for SFT, RLHF and RLVR starting from Llama-3.1-70B-base models~\citep{grattafiori2024llama3herdmodels}, for the poem task. Figure~\ref{fig:training_progression} shows the results:  traditional prompting methods do experience much larger diversity drops (\textit{mode collapse}) as models undergo alignment training, and \textbf{VS can mitigate mode collapse and maintain a higher diversity score across different post-training stages} (the diversity still drops after SFT, but SFT is necessary for instruction following capability). {Specifically, direct prompting exhibits the most severe mode collapse, with diversity dropping from 20.8\% after SFT to just 10.8\% after DPO. Other methods like sequence and multi-turn prompting also show decreased diversity. In contrast, VS maintains a stable diversity of around 30\% across stages. After the DPO stage, VS outperforms direct prompting by 182.6\% and retains about 66.8\% of the base model's original diversity. Direct prompting, by comparison, retains only 23.8\%. This suggests that VS effectively mitigates the mode collapse induced by alignment training.
\paragraph{Ablation on Temperature.} 
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/poem_temperature_plot.pdf}
    \caption{\textbf{Ablation study on temperature for poem generation across GPT-4.1 and Gemini-2.5-Flash models.} We set $k=5$ across experiments. Each plot shows the diversity-quality trade-off for three methods (Direct, Sequence, VS-Standard) at different temperature values ($t$). %Higher temperatures generally increase diversity but may reduce quality. 
    VS-Standard can be combined with temperature to further improve the trade-off,  consistently outperforming baselines across both models.
    \vspace{-1em}
    }
    \label{fig:temperature_ablation}
\end{figure}

We investigate the effect of sampling temperature on the diversity-quality trade-off. We vary the sampling temperature ($t \in \{0.4, 0.6, 0.8, 1.0, 1.2, 1.4\}$) for three methods (Direct, Sequence, and VS-Standard) across two models (GPT-4.1 and Gemini-2.5-Flash). Figure \ref{fig:temperature_ablation} presents the diversity-quality Pareto front for each method. The results indicate that \textbf{VS-Standard can be combined with temperature to further improve the diversity-quality trade-off.} VS consistently achieves a better balance between quality and diversity across both models, pushing forward the Pareto front relative to the direct and sequence baselines. %Across all methods, \textbf{higher temperatures generally increase diversity at the cost of reduced quality}.


\paragraph{Ablation on Number of Candidates,  Decoding Methods, and Prompt Formats.} 
We also perform comprehensive ablation studies on the poem task on other factors. %(1) 
% \Cref{sec:ablation_study} confirms that post-training reduces output diversity, and VS improves diversity across all post-training stages (SFT, RLHF, RLVR). 
(1) \Cref{sec:ablation_number_candidates} shows that a higher number of candidates, $k$, leads to greater diversity. (2) In \Cref{sec:ablation_decoding_strategies}, we vary the decoding strategies (top-$p$, and min-$p$), and show that VS is also orthogonal to these decoding strategies and can be combined with them to further enhance the diversity-quality curve. 
(3) In \Cref{sec:ablation_probability_format}, we test different prompt formats for eliciting distributions (e.g., asking for ``probability'', ``percentage'', or ``confidence''). While all formats improve diversity, we use the empirically best-performing format in all of our experiments: ``probability'' for VS-Standard and VS-CoT and ``confidence'' for VS-Multi. 
% as long as it explicitly asks for a distribution. 
% \jiayicomment{However, VS-Standard and VS-CoT generally perform better with ``probability,'' while VS-Multi performs better with ``confidence.'' So we use the same probability setting for the VS variants for the following experiments.}
% So we use the standard probability-based prompt for the following experiments. \wyshi{edit this} 
Across all these ablations, VS consistently outperformed the baselines under the same setups.

\newtakeaway{On creative writing tasks, \ours enhances diversity while maintaining quality and allowing tunable diversity. It also better retains diversity through post-training stages and complements different decoding strategies. Notably, larger models benefit more from VS.}
