\section{Ablation Study}

% \subsection{Ablation on \ours across RLHF stages}\label{sec:ablation_mitigation} 

% \begin{wrapfigure}{r}{0.50\textwidth}
%     \captionsetup{skip=2pt} 
%     \vspace{-1.4em}
%     \centering
%     \includegraphics[width=\linewidth]{figures/creative_writing/poem/ablation/training_progression_diversity.pdf}
%     \caption{
%     \textbf{Diversity scores across the training stages of Tulu-70B.} The red dashed line indicates the base model's diversity level. Baseline
%   prompting methods experience major degradation through SFT and DPO,
%   with Direct prompting showing the most severe decline. In contrast, our methods maintain higher
%   diversity scores throughout all training stages, demonstrating resilience to \emph{mode collapse}.
%   \vspace{-1em}
%     }
%     \label{fig:training_progression}
% \end{wrapfigure}
% We evaluate the output diversity across different post-training stages to provide empirical evidence to show that VS can mitigate mode collapse. 
% %
% To do so, we employ the Tulu-3 family~\citep{lambert2025tulu3pushingfrontiers}. It contains checkpoints for SFT, RLHF and RLVR starting from Llama-3.1-70B-base models~\citep{grattafiori2024llama3herdmodels}. Figure~\ref{fig:training_progression} reveals a critical insight: while traditional prompting methods experience dramatic diversity drops as models undergo alignment training, \ours maintains a high diversity score across different training stages. Specifically, Direct prompting exhibits the most severe mode collapse, dropping from 22.5\% diversity in the base model to just 5.3\% after DPO training. In contrast, \ours shows remarkable resilience, maintaining 15\% diversity throughout, with a particularly striking +182.6\% improvement over Direct prompting at the DPO stage. This suggests that our method bypasses the mode collapse that alignment training induces in standard prompting. 

\subsection{Ablation on the Number of Candidates ($k$) in \ours}\label{sec:ablation_number_candidates}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/num_samples_ablation_comparison.pdf}
    \caption{\textbf{Analysis of the number of candidates ($k$) for poem generation across GPT-4.1 and Gemini-2.5-Flash.} Each plot illustrates the diversity-quality trade-off as $k$ is varied from 1 to 20. Increasing $k$ generally improves diversity but lowers quality. VS-Standard consistently provides the best trade-off compared to the two baseline, approaching the Pareto front.}
    \label{fig:num_candidates_ablation}
\end{figure}

We analyze the impact of the number of candidates ($k$) on the generation process. In this experiment, we vary $k$ within the set $\{1, 3, 5, 10, 15, 20\}$ for the Direct, Sequence, and VS-Standard methods, while keeping other decoding parameters fixed. The results, illustrated in Figure \ref{fig:num_candidates_ablation}, show a trade-off: {increasing the number of candidates consistently boosts diversity at the small expense of quality across all methods and models}. However, {VS-Standard (red) consistently establishes a better \textbf{Pareto front} than the baseline}. For any given level of diversity, it maintains a higher quality score compared to both the Direct (light blue) and Sequence (blue) baselines. This indicates that our method is more effective at leveraging a larger candidate pool to find diverse yet high-quality outputs, mitigating the quality degradation typically seen when increasing $k$.

\subsection{Ablation on Decoding Strategies}\label{sec:ablation_decoding_strategies}
This section extends the temperature ablation from \Cref{sec:ablation_study} to investigate the interaction between VS and two other core decoding strategies: top-p and min-p sampling.

\paragraph{Top-p Sampling.}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/top_p_ablation_inset.pdf}
    \caption{\textbf{Top-p sampling analysis for poem generation across GPT-4.1 and Gemini-2.5-Flash.} The plots show the quality-diversity trade-off for varying $p$ values. VS-Standard demonstrates a superior performance, with an optimal balance often found at $p=0.95$. The inset provides a zoomed-in view of each method's performance curve.}
    \label{fig:top_p_ablation}
\end{figure}

First, we explore the interaction between our method and top-p (or nucleus) sampling by varying $p \in \{0.7, 0.8, 0.9, 0.95, 1.0\}$. As shown in Figure \ref{fig:top_p_ablation}, the effect of top-p is more nuanced than that of temperature. For VS-Standard, we observe that \textbf{both quality and diversity increase as $p$ is raised from 0.7 to an optimal value around 0.95}, after which quality may slightly decline. This suggests a synergistic relationship, where a moderately high $p$ value allows the model to explore a richer set of high-probability tokens that VS-Standard can effectively refine into better outputs. Across both GPT-4.1 and Gemini-2.5-Flash, VS-Standard again carves out a Pareto front, demonstrating its robust compatibility with top-p sampling.

\paragraph{Min-p Sampling.}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/min_p_ablation_comparison.pdf}
    \caption{\textbf{Min-p sampling analysis for poem generation across Qwen3-235B and Llama-3.1-70B-Instruct.} The plots show the quality-diversity trade-off for varying min-p values. Increasing min-p enhances diversity while reducing quality. VS-Standard outperforms the baselines, establishing a much more favorable Pareto front on both open-source models.}
    \label{fig:min_p_ablation}
\end{figure}

Next, we evaluate VS-Standard in conjunction with min-p sampling, a recent technique that requires access to the model's logit distribution~\citep{nguyen_turning_2025}. Accordingly, we conduct this ablation on two powerful open-source models: Qwen3-235B and Llama-3.1-70B-Instruct, with $p \in \{0.0, 0.01, 0.02, 0.05, 0.1\}$. Figure \ref{fig:min_p_ablation} shows the result. While the general trend of \textbf{increasing min-p boosting diversity at the small cost of quality} holds for all methods, VS-Standard achieves a much better diversity-quality trade-off compared to the baselines. %Its Pareto front is superior to the baselines, maintaining high quality even at diversity levels that cause a significant quality collapse in the Direct and Sequence methods. 
This confirms the effectiveness of VS-Standard on leading open-source models and its compatibility with state-of-the-art sampling techniques.


\clearpage
\subsection{Ablation on Probability Definitions in \ours}\label{sec:ablation_probability_format}
As shown in~\Cref{sec:vs}, prompting the model to verbalize the distribution of responses along with their corresponding probabilities allows \ours to overcome the mode collapse by explicitly instructing the model to sample from its original, diverse pre-training distribution. There are multiple ways to elicit these verbalized probabilities, and we explore seven variants \cite{yang2024verbalized}. For example, when prompting the model to ``Generate five jokes about coffee, each response with their corresponding probability. The probability is defined as \texttt{[probability\_definition]}'', we will fill in the following probability definition:
\begin{itemize}
    \item \textbf{Implicit probability (Implicit)}: ``how likely this response would be (from 0.0 to 1.0)'', which mentions the full distribution only implicitly; 
    % \wyshi{@jiayi can you just put an example prompt here?}
    \item \textbf{Explicit probability (Explicit)}: ``the estimated probability from 0.0 to 1.0 of this response given the input prompt (relative to the full distribution)'', which mentions the full distribution explicitly;
    \item \textbf{Relative probability (Relative}: ``the probability between 0.0 and 1.0, reflecting the relative likelihood of this response given the input.''; 
    \item \textbf{Percentage probability (Percentage}: ``the probability of this response relative to the full distribution, expressed as a percentage from 0\% to 100\%'';
    \item \textbf{Confidence}: ``the normalized likelihood score between 0.0 and 1.0 that indicates how representative or typical this response is compared to the full distribution''; 
    \item \textbf{Perplexity}: ``the exponentiated average negative log likelihood of the response tokens, where lower values indicate higher model certainty in predicting each token''; 
    \item \textbf{Negative Log-likelihood (NLL)}: ``the sum of the negative log probabilities of each token in the response given the input prompt, with smaller values reflecting higher model confidence'.
\end{itemize}

The VS prompt can be found in~\Cref{appendix:experiment_prompt}, where the definition in the probability field can be replaced with the exact definition provided above. We conduct an ablation study on these format of verbalize probability on two tasks: poem continuation (a creative writing task) and open-ended QA. We selected these tasks because poem continuation has an unlimited answer space, whereas open-ended QA has a more constrained answer space. This allows us to examine how different forms of verbalized probability affect performance across varying output spaces.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/creative_writing/verbalized_sampling_prob_format.png}
    \caption{\textbf{Ablation of probability formats for \ours on the Poem Continuation Task.} We evaluate VS-Standard (\textcolor{ProcessBlue}{blue}) and VS-Multi (\textcolor{Salmon}{red}) on two models across two metrics: \textbf{(a, c)} Diversity ($\uparrow$) and \textbf{(b, d)} Quality ($\uparrow$). Subplots \textbf{a–b} report results on GPT-4.1, while \textbf{c-d} show results on Gemini 2.5 Flash. Prompt formats include Implicit, Explicit, Relative, Percentage, Confidence, NLL, and Perplexity. 
    % \wyshi{the font of the title is not the same as others}
    }
    \label{fig:ablation_poem_probability_prompts}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures//ablation/bias_prompts_ablation_line_chart.pdf}
    \caption{\textbf{Ablation of probability formats for \ours on the Open-Ended QA Task.} We evaluate VS-Standard (\textcolor{ProcessBlue}{blue}) and VS-Multi (\textcolor{Salmon}{red}) on two models across three metrics: \textbf{(a, d)} KL Divergence (↓), \textbf{(b, e)} Coverage-N (↑), and \textbf{(c, f)} Precision (↑). Subplots \textbf{a–c} report results on GPT-4.1, while \textbf{d–f} show results on Gemini 2.5 Flash.
    }
    \label{fig:ablation_bias_probability_prompts}
\end{figure}

\paragraph{Results and Analysis.} 
As shown in \Cref{fig:ablation_poem_probability_prompts},
% \wyshi{do you mean figure 18 here?}
(a–d), both VS-Standard and VS-Multi outperform the baselines in terms of diversity on GPT-4.1 and Gemini-2.5-Flash. Across probability formats, we observe no significant overall advantage of one format over another. For both models, VS-Standard tends to perform best with \textit{Explicit}, while VS-Multi generally benefits more from \textit{Confidence}. In terms of quality, differences across formats remain small, with VS-Multi showing a slight overall advantage over VS-Standard.  

For open-ended QA (\Cref{fig:ablation_bias_probability_prompts} a–f), VS-Standard (blue) shows limited variance across probability formats, with \textit{Explicit} performing slightly better on KL Divergence and Coverage-N. VS-Multi (red), in contrast, benefits more consistently from \textit{Explicit} and \textit{Confidence}, though other formats are less stable. Precision under VS-Standard remains stable across formats, while VS-Multi exhibits greater sensitivity, particularly on Gemini-2.5-Flash. 

Overall, we find that VS-Standard tends to benefit most from the \textit{Explicit probability} format, while VS-Multi often prefers \textit{Confidence}. However, these preferences vary by model, and no single format provides a universally significant improvement. This suggests that although explicit grounding of likelihood values is often beneficial, the optimal probability format should be adapted to the model and task. 
% \wyshi{In our main experiments,  we use the empirically best-performing format: ``probability'' for VS-Standard and VS-CoT and ``confidence'' for VS-Multi.} \wyshi{check this sentence, and is it "expliti probability" or "implicit probability"}


\newpage
\subsection{Ablation on Probability Tuning in VS on Creative Writing} \label{sec:ablation_diversity_tuning_creativity}

One advantage of \ours over baseline methods is that we can potentially change the diversity level by tuning the probability in VS (e.g., ``sample from tail distribution, where each response should be $< p\%$'').

\paragraph{Experimental Setup.}
We conduct systematic experiments across different probability tuning parameters $p \in \{1.0, 0.9, 0.5, 0.2, 0.05, 0.005, 0.001\}$, where $p = 1.0$ indicates no diversity tuning is applied (standard VS prompt). We prompt models to ``sample from tail distribution, where each word should be $< p\%$'' to tune the probability thresholds in the verbalization process. We evaluate \ours on joke, poem, and story generation tasks using GPT-4.1 and Gemini 2.5 Flash.

\paragraph{Results and Analysis.}
\Cref{fig:diversity_tuning_joke,fig:diversity_tuning_poem,fig:diversity_tuning_story} demonstrate the effectiveness of probability-based diversity tuning across tasks and models. With VS, lower probability thresholds generally produce higher diversity outputs. But with baseline methods: Direct and Sequence, we cannot tune the diversity level to further enhance diversity. This ablation study shows that probability manipulation in \ours provides a practical mechanism for diversity tuning through prompting alone.
%Across tasks and models, when the probability threshold is on the low end, VS

The two VS variants exhibit complementary behaviors. In poem generation (\Cref{fig:diversity_tuning_poem}), for instance, \emph{VS-Multi}'s diversity improves more dramatically with tuning, eventually matching or surpassing \emph{VS-Standard} at lower probability thresholds. We attribute this to a reduced cognitive burden that allows the model to generate more diverse outputs. In joke generation (\Cref{fig:diversity_tuning_joke}), \emph{VS-Standard} achieves slightly higher peak diversity. This study confirms that probability manipulation in our method provides a practical and effective mechanism for fine-grained diversity control through prompting alone, with optimal parameter ranges varying by task.

% Across all probability settings, \ours significantly outperforms the Direct and Sequence baselines, with the performance gap being particularly pronounced in joke generation, requiring a broken y-axis visualization due to substantial diversity improvements.

% The results reveal consistent diversity tuning patterns across different model architectures, confirming the robustness of the probability manipulation approach. Task-specific optimal ranges emerge, with joke generation benefiting from moderate to low probability thresholds (0.05-0.5), while poem generation shows more nuanced patterns across the parameter space. This ablation study confirms that probability manipulation in \ours provides a practical mechanism for diversity control, offering users fine-grained control over output creativity through prompting alone.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/diversity_tuning/poem_diversity_tuning_comparison.pdf}
    \caption{
    \textbf{Diversity tuning results for Poem Continuation Task.}
    Comparison of diversity scores across probability
  tuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash
  (right). Notably, while \emph{VS-Multi} initially falls behind \emph{VS-Standard} at higher probability thresholds, its diversity improves more with diversity tuning. As the threshold decreases, \emph{VS-Multi}'s diversity score catches up to that for GPT-4.1 (left) or even surpasses \emph{VS-Standard} for Gemini-2.5-Flash (right), demonstrating the effectiveness of the tuning process. We attribute this trend to a reduced cognitive burden, which allows \emph{VS-Multi} to generate more diverse results with greater capability. Both \emph{VS-Standard} and \emph{VS-Multi} maintain a consistent performance advantage over the \emph{Direct} and \emph{Sequence} baselines, confirming that probability tuning provides effective diversity control across different models.
    }
    \label{fig:diversity_tuning_poem}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/diversity_tuning/book_diversity_tuning_comparison.pdf}
    \caption{
    \textbf{Diversity tuning results for Story Generation.}
    Comparison of diversity scores across probability
  tuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash
  (right). The continuous y-axis shows the full range of
  diversity values. VS-Standard and VS-Multi maintain consistent
  performance advantages over baselines while exhibiting
  complementary tuning behaviors. The results demonstrate that
  diversity tuning provides diversity control
  across different models, with optimal parameter
  ranges varying based on the specific creative task.
    }
    \label{fig:diversity_tuning_story}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/diversity_tuning/joke_diversity_tuning_comparison.pdf}
    \caption{
    \textbf{Diversity tuning results for Joke Writing.}
    Comparison of diversity scores across probability
  tuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash
  (right). The x-axis shows probability thresholds in descending
  order from 1.0 to 0.001. VS-Standard and VS-Multi consistently
  outperform Direct and Sequence baselines across all parameter
  settings. Both VS
  variants show controllable diversity curves,
  with VS-Standard achieving slightly higher peak diversity values.
    }
    \label{fig:diversity_tuning_joke}
\end{figure}

\newpage
\subsection{Ablation on Probability Tuning in VS on Open-Ended QA} \label{sec:diversity_tuning_open_ended_qa}
Following the probability manipulation experiments on the creativity tasks in \Cref{sec:ablation_diversity_tuning_creativity}, we conducted the same experiment on the Open-Ended QA task. Unlike creativity tasks, this task has a more constrained answer space, where probabilities can be more clearly interpreted. 

\paragraph{Experimental Setup.} We conduct systematic experiments across different probability tuning parameters $p \in \{1.0, 0.9, 0.5, 0.1, 0.05, 0.01\}$, where $p = 1.0$ indicates no diversity tuning is applied (standard VS prompt). 
We used the same prompting strategy, explicitly instructing the model to sample from the distribution such that the probability of each response $< p\%$, thereby controlling the probability thresholds in the verbalization process. 
We excluded thresholds below $0.01$, as such extremely tailed distributions often led the model to return empty outputs, becauseof the constrained answer space in Open-Ended QA.
Experiments were conducted on the full Open-Ended QA set with $N=40$ and $k=20$, using GPT-4.1 and Gemini-2.5-Flash.

\paragraph{Results and Analysis.} As shown in \Cref{fig:diversity_tuning_coverage_n}, VS-Standard and VS-Multi consistently outperform the sequence baseline. For GPT-4.1, Coverage-N improves as $p$ decreases, peaking near $p=0.1$ before slightly dropping at $p=0.01$. A similar trend is observed for Gemini-2.5-Flash, where coverage improves notably at moderate probability thresholds. These results suggest that moderate probability constraints encourage the model to explore a broader range of plausible answers, thereby enhancing diversity. However, extremely low thresholds ($p \leq 0.01$) lead to diminishing returns, as the distribution becomes overly tailed and unstable.


We use KL divergence from a uniform distribution to measure how well a model accesses its low-frequency, or ``long-tail,'' knowledge. 
% The pretraining distribution naturally exhibits frequency biases where common states dominate, but our goal is to ensure the model can access the full range of its knowledge, including rare items. 
The uniform distribution provides an ideal reference for this objective: lower divergence indicates better coverage of tail elements and more equitable access to low-frequency knowledge that would otherwise be neglected under standard prompting.
As shown in \Cref{fig:diversity_tuning_kl_divergence}, there is a general decreasing trend in KL Divergence as $p$ decreases, reflecting closer alignment with the uniform distribution.
Both GPT-4.1 and Gemini-2.5-Flash benefit from tuning, though GPT-4.1 spikes at $p=0.01$, which may indicate instability when sampling from very low-probability regions. Across models, VS-Standard and VS-Multi consistently achieve lower divergence than the sequence baseline.
However, this push for diversity directly impacts the precision.
As shown in \Cref{fig:diversity_tuning_precision}, we also observed a general trend for both models in precision: the precision will first peak at $p=0.9$, then gradually decrease as $p$ decreases. 
This also suggests that the optimal value for $p$ is application-dependent, determined by the required balance between response diversity and precision.
% \wyshi{do we still want to compare with the pretraining distribution or the uniform distribution?}. 

Together, these findings indicate that probability tuning enhances response diversity in Open-Ended QA, with the strongest gains observed at moderate thresholds (e.g., $p \leq 0.1$). While VS-Standard already provides consistent improvements, VS-Multi offers additional flexibility in exploring the answer space, though very small probability cutoffs can introduce instability.

% \wyshi{how about precision for this one?}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation/bias_metrics_tuning_coverage_n.pdf}
    \caption{\textbf{Diversity tuning results for Open-Ended QA on Coverage-N.} Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters. Coverage-N measures the proportion of ground truth covered in the response distribution (higher is better). Both VS-Standard and VS-Multi consistently outperform the sequence baseline, with coverage increasing as probability decreases until $\leq 0.1$, where the distribution becomes heavily tailed.
    }
    \label{fig:diversity_tuning_coverage_n}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation/bias_metrics_tuning_kl_divergence.pdf}
    \caption{\textbf{Diversity tuning results for Open-Ended QA on KL Divergence over uniform distribution.} Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters. VS-Standard and VS-Multi achieve consistently lower divergence than the sequence baseline. The overall trend shows decreasing KL Divergence as probability decreases, indicating closer alignment with uniform distribution.
    }
    \label{fig:diversity_tuning_kl_divergence}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation/bias_metrics_tuning_precision.pdf}
    \caption{\textbf{Diversity tuning results for Open-Ended QA on Precision.} Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters.
    }
    \label{fig:diversity_tuning_precision}
\end{figure}