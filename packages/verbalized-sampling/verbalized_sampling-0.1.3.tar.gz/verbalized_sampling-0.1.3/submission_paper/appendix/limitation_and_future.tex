\section{Contribution Statement}
\label{sec:contribution statement}
\textbf{Jiayi Zhang} and \textbf{Simon Yu} co-led the design and execution of experiments. 

\textbf{Jiayi Zhang} established the core proof of concept for the intuition on the dialogue simulation task important for the project, proposed tasks and ablations, contributed to the codebase, and conducted experiments on dialogue simulation, open-ended QA, commonsense reasoning, random number generation, probing the pretraining and verbalized distribution, synthetic data generation, and human study on creative writing.



\textbf{Simon Yu} implemented the core codebase, proposed tasks and ablations, refined the initial theoretical proof, validated the typicality bias on multiple preference datasets, conducted experiments on creative writing, synthetic data generation, safety evaluation, and ablation studies, and led the open source and packaged the codebase into a library.

%split work on the experiments (creative writing, dialogue simulation, synthetic data generation, open-ended QA, commonsense reasoning, safety evaluation, and ablation study). 

\textbf{Derek Chong} provided the core intuition of the project, proposed tasks, developed the theoretical proof on mode collapse in post-training alignment, conducted its empirical and statistical validation, helped with experimental design, and packaged the codebase into a library.
%generalize from there,  proof on mode collapse in post-training alignment, the empirical validation in the data, statistical evidence, 

\textbf{Anthony Sicilia} contributed to the discussions on the dialogue simulation tasks and collaborated with Derek Chong to refine the theoretical proof.

\textbf{Michael Tomz} and \textbf{Christopher Manning} provided funding for Derek Chong, steered the initial research direction, offered feedback across the project, and assisted with the review and proofreading of the manuscript.
% steer tje research riectionn

\textbf{Weiyan Shi} supervised the research, steered the project direction, provided funding support, gathered external feedback, polished the figures, and led the final comprehensive editing and review process.
% proof-editing (comprehensive review), project direction, supervision, gather external feedback, funding for all experiments. throughout the project,  provided funding support

All authors reviewed the manuscript and provided feedback. 

\section{Limitations}\label{sec:limitations}

We discuss the following limitations of our method. %Our method has the foll the proposed method has certain limitations that suggest avenues for future research.

\paragraph{Computational Cost and Latency.}
One major trade-off of \ours (VS) is an increased computational budget at inference time. Generating a distribution of $N$ candidates is more costly in terms of latency and token usage than generating a single response. In our experiments, we have controlled the total computing budget, but this limitation may still constrain its applicability in latency-sensitive or resource-constrained environments. 

\paragraph{Dependence on Model Scale and Capability.}
The performance gains from VS are positively correlated with model scale. Our results indicate that larger, more capable models can better handle the cognitive burden of the probability estimation and structured output. Conversely, less capable models may lack the reasoning and instruction-following abilities to fully benefit from VS, so they occasionally exhibit a degradation in output quality. The method's effectiveness is therefore contingent on a sufficient level of underlying model capability.

% \paragraph{Reliance on Automated Evaluation Metrics.}
% For subjective attributes such as creativity and response quality, our evaluation employed LLM-based judges. This approach, while necessary for scalability, serves as an imperfect proxy for human assessment. Automated evaluators possess their own biases and may not fully capture the nuanced qualities valued by human raters. A more definitive validation of output quality would require comprehensive human evaluation.

% \jiayicomment{Limitation with probability tuning on dialogue simulation}

% \jiayicomment{limitation on probability understanding}

\section{Future Directions}\label{sec:future_direction}

\paragraph{Mitigating Bias in Reward Models.} As we discussed in \Cref{sec:typicality}, the major cause of \emph{mode collapse} is the cognitive typicality biases embedded in the preference data and, therefore, affecting the reward models. These biases can cause the reward models to favor stereotypical  outputs or exhibit certain biases (e.g. towards length, style~\citep{liu2024rmbenchbenchmarkingrewardmodels}). To tackle this challenge, recent works have tried different calibration techniques that produce more balanced reward models. For example, \citet{huang2024posthocrewardcalibrationcase} introduced post-hoc calibration methods that specifically address length and stylistic biases. On the other hand, \citet{zhu2025charmcalibratingrewardmodels} took a different approach and used Chatbot Arena rankings collected from the public to calibrate their reward models. To reduce mode collapse, a promising future step is to mitigate reward model bias and achieve broader preference coverage through pluralistic alignment~\citep{sorensen2024roadmappluralisticalignment}. %Future work should focus on mitigating reward model bias and achieving broader preference coverage through pluralistic alignment~\citep{sorensen2024roadmappluralisticalignment}, which will be fundamental to reducing mode collapse.

\paragraph{Inference-time Scaling.}
\ours presents an alternative approach to inference-time scaling. Conventional methods~\citep{snell_scaling_2024,brown_large_2024} often rely on repeated sampling from a single prompt; however, as we have shown, this method can be vulnerable to mode collapse and suffer from limited output diversity~\citep{yang_how_2025}. By contrast, \ours elicits a broader distribution of responses that more faithfully represents the LLM's underlying generative capabilities. This enhanced diversity can be particularly promising for improving the action space exploration in RL training~\citep{cui2025entropymechanismreinforcementlearning,wang20258020rulehighentropyminority}. For instance, the diverse outputs from \ourslower
could enable exploration of less probable but potentially correct solutions, which can be reinforced during RL training to improve performance. This is a promising direction for future work.

% \section{The Use of Large Language Models}
% We used large language models (LLMs) only for language refinement tasks, such as grammar checking, phrasing adjustments, enhancing readability, and so on. \wyshi{@Derek fill in}
% We also used Deep Research~\citep{openai2025deepresearch} to assist with related work search.
% Besides these, all scientific ideas, experiments, analyses, and results are the sole contributions of the authors.

\section{Use of Large Language Models}

% In accordance with ICLR policy, 
We disclose our use of large language models (LLMs) in this work. We employed LLMs in two capacities:

\textbf{Paper Writing Assistance:} We used LLMs to improve the clarity and presentation of our work, including initial drafting of subsections, refinement of technical exposition, grammar and style improvements, and minor proof-editing tasks. We also used Deep Research~\citep{openai2025deepresearch} to assist with literature search and identifying relevant prior work.

\textbf{Research Assistance:} We utilized LLMs to help generate experimental code, assist in formalizing theoretical concepts, and support the implementation of our methods. All LLM-generated code and theoretical formulations were thoroughly reviewed, verified, and validated by the authors.

We emphasize that all core scientific contributions originate from the authors: LLM outputs were treated as preliminary drafts requiring substantial human oversight, verification, and modification. The authors take full responsibility for all content in this submission, including any text or code initially generated with LLM assistance.

% \derek{The above is aligned with https://blog.iclr.cc/2025/08/26/policies-on-large-language-model-usage-at-iclr-2026/}
