package pharia:skill@0.3.11;

@since(version = 0.3.0)
world skill {
    @since(version = 0.3.0)
    include csi;
    @since(version = 0.3.0)
    export skill-handler;
}

@since(version = 0.3.0)
interface skill-handler {
    /// The set of errors which may be raised by functions in this interface
    @since(version = 0.3.0)
    variant error {
        internal(string),
        invalid-input(string)
    }

    @since(version = 0.3.0)
    run: func(input: list<u8>) -> result<list<u8>, error>;

    @since(version = 0.3.0)
    record skill-metadata {
        description: option<string>,
        input-schema: list<u8>,
        output-schema: list<u8>,
    }

    @since(version = 0.3.0)
    metadata: func() -> skill-metadata;
}

@since(version = 0.3.6)
world message-stream-skill {
    @since(version = 0.3.6)
    include csi;
    @since(version = 0.3.6)
    export message-stream;
}

@since(version = 0.3.6)
interface message-stream {
    @since(version = 0.3.6)
    use streaming-output.{stream-output};

    /// The set of errors which may be raised by functions in this interface
    @since(version = 0.3.6)
    variant error {
        internal(string),
        invalid-input(string)
    }

    /// Run the skill. Output is streamed out of the stream-output resource.
    /// The skill is allowed to also terminate early with an error.
    @since(version = 0.3.6)
    run: func(input: list<u8>, output: stream-output) -> result<_, error>;
}

/// Provided host types for supporting running streaming skills.
@since(version = 0.3.6)
interface streaming-output {
    /// Payload for the beginning of a message.
    @since(version = 0.3.6)
    record begin-attributes {
        /// Optional role of the message. Currently unused. Setting it will have no effect. In case
        /// we would need it soon, it allows us to introduce it though without breaking changes to
        /// the wit world. Users consuming the SDK do not need to worry about this field.
        role: option<string>
    }

    @since(version = 0.3.6)
    variant message-item {
        /// Begin a new message. Cannot be sent before a previous message has ended.
        message-begin(begin-attributes),
        /// A chunk of the message text. If all appends are aggregated, it should produce the full message.
        /// Should be sent after a message-begin.
        message-append(string),
        /// Indicate the end of the message, with an optional payload.
        /// Needs to be preceded by at least a message-begin.
        message-end(option<list<u8>>),
    }

    @since(version = 0.3.6)
    resource stream-output {
        /// Puts part of a message into the output stream.
        @since(version = 0.3.6)
        write: func(item: message-item);
    }
}

// A WIT world dedicated to interacting with Large Language Models and other AI-related tasks.
@since(version = 0.3.0)
world csi {
    import chunking;
    import document-index;
    import inference;
    import language;
    import tool;
}

// The tool interface allows Skills to interact with the outside world.
@since(version = 0.3.7)
interface tool {
    @since(version = 0.3.11)
    record invoke-request {
        tool-name: string,
        arguments: list<argument>,
    }

    @since(version = 0.3.11)
    record argument {
        name: string,
        value: list<u8>
    }

    // A tool result is an arbitrary sized list of modalities.
    // See <https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result>.
    // At the moment, we only support text modalities from the Kernel.
    @since(version = 0.3.11)
    type tool-output = list<modality>;

    @since(version = 0.3.11)
    variant modality {
        text(string),
    }

    // See <https://github.com/bytecodealliance/wasm-tools/issues/2210> why a type alias is needed
    // as long as tool-output is not stable.
    @since(version = 0.3.11)
    type tool-result = result<tool-output, string>;

    @since(version = 0.3.11)
    invoke-tool: func(request: list<invoke-request>) -> list<tool-result>;

    @since(version = 0.3.11)
    record tool {
        name: string,
        description: string,
        input-schema: list<u8>,
    }

    // As long as we do not support tool calling in the inference, the prompt synthesis happens in the Skill code.
    // It could also happen in the Kernel, but we already have the logic in the SDK, and it seems like this will
    // move to inference soon anyway. Therefore, the Skill needs to know about the schema of the different tools.
    // While this could be achieved by querying for a list of tool names, and then getting a list of options in
    // the same order, simply listing all tools seems to be the simpler solution.
    @since(version = 0.3.11)
    list-tools: func() -> list<tool>;
}

@since(version = 0.3.0)
interface chunking {
    /// Chunking parameters
    @since(version = 0.3.0)
    record chunk-params {
        /// The name of the model the chunk is intended to be used for.
        /// This must be a known model.
        model: string,
        /// The maximum number of tokens that should be returned per chunk.
        max-tokens: u32,
        /// The amount of allowed overlap between chunks.
        /// overlap must be less than max-tokens.
        overlap: u32,
    }

    @since(version = 0.3.0)
    record chunk-request {
        text: string,
        params: chunk-params,
    }

    @since(version = 0.3.0)
    chunk: func(request: list<chunk-request>) -> list<list<string>>;

    @since(version = 0.3.3)
    record chunk-with-offset-request {
        text: string,
        params: chunk-params,
        /// Byte offsets will always be emitted, but if you work in a language that uses
        /// characters rather than bytes to index strings, you can ask for character
        /// offsets. If you do not need them, you should set this to `false` though,
        /// because it is extra work computing them.
        character-offsets: bool,
    }

    @since(version = 0.3.3)
    record chunk-with-offset {
        text: string,
        byte-offset: u64,
        character-offset: option<u64>,
    }

    @since(version = 0.3.3)
    chunk-with-offsets: func(request: list<chunk-with-offset-request>) -> list<list<chunk-with-offset>>;
}

@since(version = 0.3.0)
interface document-index {
    /// Which documents you want to search in, and which type of index should be used
    @since(version = 0.3.0)
    record index-path {
        /// The namespace the collection belongs to
        namespace: string,
        /// The collection you want to search in
        collection: string,
        /// The search index you want to use for the collection
        index: string,
    }

    @since(version = 0.3.0)
    record document-path {
        namespace: string,
        collection: string,
        name: string,
    }

    /// A position within a document. The cursor is always inclusive of the current position, in both start and end positions.
    @since(version = 0.3.0)
    record text-cursor {
        /// Index of the item in the document. A document is an array of text and image elements. These elements are referred to as items.
        item: u32,
        /// The character position the cursor can be found at within the string.
        position: u32,
    }

    /// The result for semantic document search. Part of an array of document names and content of the found documents in the given collection.
    @since(version = 0.3.0)
    record search-result {
        /// The path to a document. A path uniquely identifies a document among all managed documents.
        document-path: document-path,
        /// The text of the found section. As we do not support multi-modal, this is always a string.
        content: string,
        /// Search score of the found section, where a higher score indicates a closer match.
        /// Will be between -1 and 1. A score closer to -1 indicates the section opposes the query.
        /// A score close 0 suggests the section is unrelated to the query.
        /// A score close to 1 suggests the section is related to the query.
        /// The score depends on the index configuration, e.g. the score of a section differs for hybrid and non-hybrid indexes.
        /// For searches on hybrid indexes, the score can exceed the min_score of the query as the min_score only applies to the semantic similarity sub-query.
        score: f64,
        start: text-cursor,
        end: text-cursor,
    }

    @since(version = 0.3.0)
    record search-request {
        index-path: index-path,
        /// While the Document Index offers a list of multiple modality queries, as we do not support multi-modal search,
        /// we only support a single text modality query.
        query: string,
        /// Maximum number of found documents to return.
        max-results: u32,
        /// Filter out results with a cosine similarity score below this value.
        /// Scores range from -1 to 1. For searches on hybrid indexes, the Document Index applies the min_score to the semantic results before fusion of result sets.
        /// As fusion re-scores results, returned scores may exceed this value.
        min-score: option<f64>,
        /// A filter for search results that restricts the results to those document sections that match the filter criteria.
        /// The individual conditions of this array are AND-combined (i.e. all conditions must match).
        /// This can for example be used to restrict the returned sections based on their modality (i.e. image or text), or on their metadata.
        filters: list<search-filter>
    }

    @since(version = 0.3.0)
    search: func(requests: list<search-request>) -> list<list<search-result>>;

    @since(version = 0.3.0)
    document-metadata: func(requests: list<document-path>) -> list<option<list<u8>>>;

    @since(version = 0.3.0)
    record document {
        path: document-path,
        contents: list<modality>,
        metadata: option<list<u8>>,
    }

    /// A logical combination of filter conditions.
    @since(version = 0.3.0)
    variant search-filter {
        /// Logical conjunction of negations, i.e. forms the predicate "(NOT filterCondition1) AND (NOT filterCondition2) AND ..."
        without(list<metadata-filter>),
        /// Logical disjunction, i.e. forms the predicate "filterCondition1 OR filterCondition2 OR ..."
        with-one-of(list<metadata-filter>),
        /// Logical conjunction, i.e. forms the predicate "filterCondition1 AND filterCondition2 AND ..."
        with-all(list<metadata-filter>),
    }

    /// Matches sections whose metadata fields match the given condition. You must specify the field, and can only specify a single condition.
    @since(version = 0.3.0)
    record metadata-filter {
        /// The metadata field on which to filter search results.
        /// Field names must only contain alphanumeric characters, dashes and underscores.
        /// Nested fields can be specified using dot notation (e.g. 'a.b').
        /// Array-valued fields can either use a wildcard specifier (e.g. 'a[].b') or a specific index (e.g. 'a[1].b').
        /// The maximum length of the field name is 1000 characters.
        field: string,
        /// The condition to filter on.
        condition: metadata-filter-condition
    }

    @since(version = 0.3.0)
    variant metadata-filter-condition {
        greater-than(f64),
        greater-than-or-equal-to(f64),
        less-than(f64),
        less-than-or-equal-to(f64),
        after(string),
        at-or-after(string),
        before(string),
        at-or-before(string),
        equal-to(metadata-field-value),
        /// This condition matches all metadata fields with a value of null.
        is-null,
    }

    @since(version = 0.3.0)
    variant metadata-field-value {
        string-type(string),
        integer-type(s64),
        boolean-type(bool),
    }

    @since(version = 0.3.0)
    variant modality {
        text(string),
        /// We don't expose the image contents, as none of the models support multi-modal.
        image,
    }

    @since(version = 0.3.0)
    documents: func(requests: list<document-path>) -> list<document>;
}

@since(version = 0.3.0)
interface inference {
    /// Better understand the source of a completion, specifically on how much each section of a prompt impacts each token of the completion.
    @since(version = 0.3.1)
    explain: func(request: list<explanation-request>) -> list<list<text-score>>;

    /// A score for a text segment.
    @since(version = 0.3.1)
    record text-score {
        /// The start index of the text segment w.r.t. to characters in the prompt.
        start: u32,
        /// The length of the text segment w.r.t. to characters in the prompt.
        length: u32,
        /// The score of the text segment, higher means more relevant.
        score: f64,
    }

    /// At which granularity should the target be explained in terms of the prompt.
    /// If you choose, for example, [`granularity.sentence`] then we report the importance score of each
    /// sentence in the prompt towards generating the target output.
    /// The default is [`granularity.auto`] which means we will try to find the granularity that
    /// brings you closest to around 30 explanations. For large prompts, this would likely
    /// be sentences. For short prompts this might be individual words or even tokens.
    @since(version = 0.3.1)
    enum granularity {
        /// Let the system decide which granularity is most suitable for the given input.
        auto,
        word,
        sentence,
        paragraph,
    }

    @since(version = 0.3.1)
    record explanation-request {
        /// The prompt that typically was the input of a previous completion request
        prompt: string,
        /// The target string that should be explained. The influence of individual parts
        /// of the prompt for generating this target string will be indicated in the response.
        target: string,
        /// The model to use for the explanation.
        model: string,
        /// The granularity of the explanation.
        granularity: granularity,
    }

    /// The reason the model finished generating
    @since(version = 0.3.0)
    enum finish-reason {
        /// The model hit a natural stopping point or a provided stop sequence
        stop,
        /// The maximum number of tokens specified in the request was reached
        length,
        /// Content was omitted due to a flag from content filters
        content-filter,
    }

    @since(version = 0.3.0)
    record logprob {
        token: list<u8>,
        logprob: f64,
    }

    @since(version = 0.3.0)
    record distribution {
        /// Logarithmic probability of the token returned in the completion
        sampled: logprob,
        /// Logarithmic probabilities of the most probable tokens, filled if user has set
        /// variant `logprobs` to `top` in chat or completion request.
        top: list<logprob>,
    }

    @since(version = 0.3.0)
    record token-usage {
        /// Number of tokens in the prompt
        prompt: u32,
        /// Number of tokens in the generated completion
        completion: u32,
    }

    /// The result of a completion, including the text generated as well as
    /// why the model finished completing.
    @since(version = 0.3.0)
    record completion {
        /// The text generated by the model
        text: string,
        /// The reason the model finished generating
        finish-reason: finish-reason,
        /// Contains the logprobs for the sampled and top n tokens, given that
        /// `completion-request.params.logprobs` has been set to `sampled` or `top`.
        logprobs: list<distribution>,
        /// Usage statistics for the completion request.
        usage: token-usage,
    }

    @since(version = 0.3.0)
    variant logprobs {
        /// Do not return any logprobs
        no,
        /// Return only the logprob of the tokens which have actually been sampled into the completion.
        sampled,
        /// Request between 0 and 20 tokens
        top(u8),
    }

    /// Completion request parameters
    @since(version = 0.3.0)
    record completion-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The number of possible next tokens the model will choose from.
        top-k: option<u32>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// A list of sequences that, if encountered, the API will stop generating further tokens.
        stop: list<string>,
        /// Whether to include special tokens like `<|eot_id|>` in the completion
        return-special-tokens: bool,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
    }

    /// Completion request parameters
    /// Outdated, consumers should use `completion-request-v2` instead.
    /// Still supported for backwards compatibility.
    @since(version = 0.3.0)
    record completion-request {
        model: string,
        prompt: string,
        params: completion-params
    }

    /// Outdated, consumers should use `complete-v2` instead.
    /// Still supported for backwards compatibility.
    @since(version = 0.3.0)
    complete: func(requests: list<completion-request>) -> list<completion>;

    /// Completion request parameters
    @since(version = 0.3.8)
    record completion-params-v2 {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The number of possible next tokens the model will choose from.
        top-k: option<u32>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// A list of sequences that, if encountered, the API will stop generating further tokens.
        stop: list<string>,
        /// Whether to include special tokens like `<|eot_id|>` in the completion
        return-special-tokens: bool,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
        /// Echo the prompt in the completion. This may be especially helpful when log_probs is set
        /// to return logprobs for the prompt.
        echo: bool,
    }

    /// Completion request parameters
    /// Introduces support for `echo` without requiring a version bump.
    @since(version = 0.3.8)
    record completion-request-v2 {
        model: string,
        prompt: string,
        params: completion-params-v2
    }

    /// Introduces support for `echo` without requiring a version bump.
    @since(version = 0.3.8)
    complete-v2: func(requests: list<completion-request-v2>) -> list<completion>;

    /// A chunk of a completion returned by a completion stream.
    @since(version = 0.3.6)
    record completion-append {
        /// A chunk of the completion text.
        text: string,
        /// Corresponding log probabilities for each token in the completion.
        logprobs: list<distribution>,
    }

    /// An event emitted by a completion stream.
    @since(version = 0.3.6)
    variant completion-event {
        /// A chunk of a completion returned by a completion stream.
        append(completion-append),
        /// The reason the completion stream stopped.
        end(finish-reason),
        /// The usage generated by the completion stream.
        usage(token-usage),
    }

    /// Allows for streaming completion tokens as they are generated.
    @since(version = 0.3.6)
    resource completion-stream {
        /// Creates a new completion-stream resource for a given completion-request.
        @since(version = 0.3.6)
        constructor(init: completion-request);
        /// Returns the next completion-event from the completion-stream.
        /// Will return None if the stream has finished.
        @since(version = 0.3.6)
        next: func() -> option<completion-event>;
    }

    @since(version = 0.3.0)
    record message {
        role: string,
        content: string,
    }

    @since(version = 0.3.0)
    record chat-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
    }

    /// The result of a chat response, including the message generated as well as
    /// why the model finished completing.
    @since(version = 0.3.0)
    record chat-response {
        /// The message generated by the model
        message: message,
        /// The reason the model finished generating
        finish-reason: finish-reason,
        /// Contains the logprobs for the sampled and top n tokens, given that
        /// `chat-request.params.logprobs` has been set to `sampled` or `top`.
        logprobs: list<distribution>,
        /// Usage statistics for the chat request.
        usage: token-usage,
    }

    @since(version = 0.3.0)
    record chat-request {
        model: string,
        messages: list<message>,
        params: chat-params,
    }

    @since(version = 0.3.0)
    chat: func(requests: list<chat-request>) -> list<chat-response>;

    /// A chunk of a message generated by the model.
    @since(version = 0.3.6)
    record message-append {
        /// A chunk of the message content
        content: string,
        /// Corresponding log probabilities for each token in the message content
        logprobs: list<distribution>,
    }

    /// An event emitted by the chat-stream resource.
    @since(version = 0.3.6)
    variant chat-event {
        /// The start of a new message. It includes the role of the message.
        message-begin(string),
        /// A chunk of a message generated by the model.
        message-append(message-append),
        /// The end of a message. It includes the reason for the message end.
        message-end(finish-reason),
        /// The usage from the generated message
        usage(token-usage),
    }

    @since(version = 0.3.6)
    resource chat-stream {
        /// Creates a new chat-stream resource for a given chat-request.
        @since(version = 0.3.6)
        constructor(init: chat-request);
        /// Returns the next chat-event from the chat-stream.
        /// Will return None if the stream has finished.
        @since(version = 0.3.6)
        next: func() -> option<chat-event>;
    }
}

@since(version = 0.3.0)
interface language {
    /// Select the detected language for the provided input based on the list of possible languages.
    /// If no language matches, None is returned.
    ///
    /// text: Text input
    /// languages: All languages that should be considered during detection.
    @since(version = 0.3.0)
    record select-language-request {
        text: string,
        languages: list<string>,
    }

    /// Select most likely language from a list of supported ISO 639-3 language codes.
    ///
    /// Afrikaans - "afr",
    /// Arabic - "ara",
    /// Azerbaijani - "aze",
    /// Belarusian - "bel",
    /// Bengali - "ben",
    /// Bosnian - "bos",
    /// Bulgarian - "bul",
    /// Catalan - "cat",
    /// Czech - "ces",
    /// Welsh - "cym",
    /// Danish - "dan",
    /// German - "deu",
    /// Greek - "ell",
    /// English - "eng",
    /// Esperanto - "epo",
    /// Estonian - "est",
    /// Basque - "eus",
    /// Persian - "fas",
    /// Finnish - "fin",
    /// French - "fra",
    /// Irish - "gle",
    /// Gujarati - "guj",
    /// Hebrew - "heb",
    /// Hindi - "hin",
    /// Croatian - "hrv",
    /// Hungarian - "hun",
    /// Armenian - "hye",
    /// Indonesian - "ind",
    /// Icelandic - "isl",
    /// Italian - "ita",
    /// Japanese - "jpn",
    /// Georgian - "kat",
    /// Kazakh - "kaz",
    /// Korean - "kor",
    /// Latin - "lat",
    /// Latvian - "lav",
    /// Lithuanian - "lit",
    /// Ganda - "lug",
    /// Marathi - "mar",
    /// Macedonian - "mkd",
    /// Mongolian - "mon",
    /// Maori - "mri",
    /// Malay - "msa",
    /// Dutch - "nld",
    /// Norwegian Nynorsk - "nno",
    /// Norwegian Bokm√•l - "nob",
    /// Punjabi - "pan",
    /// Polish - "pol",
    /// Portuguese - "por",
    /// Romanian - "ron",
    /// Russian - "rus",
    /// Slovak - "slk",
    /// Slovene - "slv",
    /// Shona - "sna",
    /// Somali - "som",
    /// Sotho - "sot",
    /// Spanish - "spa",
    /// Serbian - "srp",
    /// Albanian - "sqi",
    /// Swahili - "swa",
    /// Swedish - "swe",
    /// Tamil - "tam",
    /// Telugu - "tel",
    /// Tagalog - "tgl",
    /// Thai - "tha",
    /// Tswana - "tsn",
    /// Tsonga - "tso",
    /// Turkish - "tur",
    /// Ukrainian - "ukr",
    /// Urdu - "urd",
    /// Vietnamese - "vie",
    /// Xhosa - "xho",
    /// Yoruba - "yor",
    /// Chinese - "zho",
    /// Zulu - "zul",
    @since(version = 0.3.0)
    select-language: func(request: list<select-language-request>) -> list<option<string>>;
}
