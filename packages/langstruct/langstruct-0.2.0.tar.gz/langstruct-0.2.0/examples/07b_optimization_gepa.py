#!/usr/bin/env python3
"""
LangStruct GEPA Optimization Example

Shows how to use GEPA optimizer for reflective prompt optimization with feedback.
GEPA is particularly effective for complex reasoning tasks where understanding
*why* extractions succeed or fail helps improve performance.

This example uses a two-model strategy:
- Gemini 2.5 Flash Lite: Fast model for actual extractions (cost-effective)
- Gemini 2.5 Flash: Stronger model for reflections (smart improvements)

Requirements:
    pip install langstruct

Environment:
    export GOOGLE_API_KEY="your-key"

Usage:
    python 07b_optimization_gepa.py
"""

import os
import warnings

from langstruct import LangStruct


def main():
    """Example showing GEPA optimization capabilities."""

    print("‚ö° LangStruct GEPA Optimization Example")
    print("=" * 50)

    try:
        # Step 1: Create extractor with GEPA optimizer
        print()
        print("1Ô∏è‚É£ Creating extractor with GEPA optimizer...")
        print("   ‚Ä¢ Main model: Gemini 2.5 Flash Lite (fast, efficient)")
        print("   ‚Ä¢ Reflection model: Gemini 2.5 Flash (stronger reasoning)")
        print()

        import dspy

        from langstruct.optimizers import GEPAOptimizer

        # Create extractor with Gemini 2.5 Flash Lite as main model
        # Use google/ prefix to route to Google AI Studio (uses GOOGLE_API_KEY)
        extractor = LangStruct(
            example={
                "person_name": "Dr. Sarah Johnson",
                "job_title": "cardiologist",
                "years_experience": 8,
                "specialization": "interventional cardiology",
            },
            model="gemini/gemini-2.5-flash-lite",  # Fast model for extractions
            optimizer="gepa",
        )

        # Configure GEPA with stronger reflection model
        # The reflection model does the "thinking" about how to improve prompts
        # Important: Reflection needs higher max_tokens for detailed analysis
        gepa_optimizer = GEPAOptimizer(
            auto="light",  # Use "heavy" for production
            reflection_lm=dspy.LM(
                "gemini/gemini-2.5-flash",
                max_tokens=32000,  # High limit for detailed reflections
                temperature=1.0,  # Higher temp for creative improvements
            ),
            num_threads=4,
            track_stats=True,
        )
        extractor.optimizer = gepa_optimizer

        print("‚úÖ Extractor ready with GEPA optimizer!")
        print("   ‚Ä¢ Gemini 2.5 Flash Lite: Handles actual extractions (fast)")
        print(
            "   ‚Ä¢ Gemini 2.5 Flash: Reflects on failures and proposes improvements (smart)"
        )
        print("   ‚Ä¢ Reflection model configured with:")
        print("     - max_tokens=32000 (needs space for detailed analysis)")
        print("     - temperature=1.0 (creativity for prompt improvements)")
        print()
        print("üìö About GEPA:")
        print("   ‚Ä¢ Uses reflective prompt evolution with textual feedback")
        print("   ‚Ä¢ The reflection model learns from *why* extractions succeed or fail")
        print("   ‚Ä¢ Best for complex reasoning tasks")
        print("   ‚Ä¢ Particularly effective when you have ground truth data")
        print("   ‚Ä¢ Requires high max_tokens for reflection LM (32000 recommended)")

        # Step 2: Initial extraction (baseline)
        print("\n2Ô∏è‚É£ Baseline extraction...")
        baseline_text = """
        Dr. Michael Chen is a senior software engineer with 12 years of experience
        specializing in machine learning and distributed systems. He currently leads
        the AI infrastructure team at TechCorp.
        """

        baseline_result = extractor.extract(baseline_text)
        print(f"   Baseline confidence: {baseline_result.confidence:.1%}")
        print(f"   Extracted: {baseline_result.entities}")

        # Step 3: Prepare training data for GEPA optimization
        print("\n3Ô∏è‚É£ Preparing training data for GEPA...")
        print("   Note: GEPA works best WITH expected results (ground truth)")

        # Training texts - varied examples from your domain
        training_texts = [
            "Dr. Lisa Wang is a pediatrician with 6 years of experience specializing in neonatal care.",
            "Prof. James Miller, an experienced biochemist (15+ years), focuses on protein structure research.",
            "Sarah Kim works as a data scientist for 4 years, specializing in natural language processing.",
            "Dr. Robert Taylor is a neurologist with 10 years experience in epilepsy treatment.",
            "Emily Rodriguez is a software architect with 8 years focusing on cloud infrastructure.",
        ]

        # Expected results - GEPA uses these for rich feedback
        expected_results = [
            {
                "person_name": "Dr. Lisa Wang",
                "job_title": "pediatrician",
                "years_experience": 6,
                "specialization": "neonatal care",
            },
            {
                "person_name": "Prof. James Miller",
                "job_title": "biochemist",
                "years_experience": 15,
                "specialization": "protein structure research",
            },
            {
                "person_name": "Sarah Kim",
                "job_title": "data scientist",
                "years_experience": 4,
                "specialization": "natural language processing",
            },
            {
                "person_name": "Dr. Robert Taylor",
                "job_title": "neurologist",
                "years_experience": 10,
                "specialization": "epilepsy treatment",
            },
            {
                "person_name": "Emily Rodriguez",
                "job_title": "software architect",
                "years_experience": 8,
                "specialization": "cloud infrastructure",
            },
        ]

        print(f"   üìö Training set: {len(training_texts)} examples")
        print(f"   üéØ With ground truth for feedback generation")

        # Step 4: Run GEPA optimization
        print("\n4Ô∏è‚É£ Running GEPA optimization...")
        print("   GEPA will:")
        print("   ‚Ä¢ Generate detailed feedback on extraction quality")
        print("   ‚Ä¢ Identify which fields were missed or incorrect")
        print("   ‚Ä¢ Suggest improvements to prompts")
        print("   ‚Ä¢ Evolve prompts based on feedback")
        print()

        # Define a small test set to evaluate before/after
        test_texts = [
            "Dr. Amanda Foster is a radiologist with 7 years of experience in diagnostic imaging.",
            "Mark Thompson works as a DevOps engineer for 5 years, specializing in Kubernetes orchestration.",
        ]
        expected_test_results = [
            {
                "person_name": "Dr. Amanda Foster",
                "job_title": "radiologist",
                "years_experience": 7,
                "specialization": "diagnostic imaging",
            },
            {
                "person_name": "Mark Thompson",
                "job_title": "DevOps engineer",
                "years_experience": 5,
                "specialization": "Kubernetes orchestration",
            },
        ]

        # Evaluate baseline performance
        baseline_scores = extractor.evaluate(test_texts, expected_test_results)
        print(
            f"   üìâ Baseline ‚Äî Accuracy: {baseline_scores.get('accuracy', 0):.1%}, "
            f"F1: {baseline_scores.get('f1', 0):.3f}"
        )

        did_optimize = False
        try:
            # Run GEPA optimization (requires API key)
            if (
                os.getenv("LANGSTRUCT_REAL_OPT")
                or os.getenv("GOOGLE_API_KEY")
                or os.getenv("OPENAI_API_KEY")
                or os.getenv("ANTHROPIC_API_KEY")
            ):
                print("   ‚è≥ GEPA is analyzing extractions and generating feedback...")
                print("   (This will take a few minutes)")

                extractor.optimize(
                    texts=training_texts,
                    expected_results=expected_results,
                )
                did_optimize = True
                print("   ‚úÖ GEPA optimization complete!")
                print("   üìà Prompts evolved based on extraction feedback")
            else:
                print("   ‚è≥ GEPA optimization (simulated for demo)")
                print("   ‚úÖ Optimization complete! (simulated)")
                print(
                    "   üìà In real use: GEPA evolves prompts based on detailed feedback"
                )
        except Exception as e:
            warnings.warn(f"Optimization failed or was skipped: {e}")

        # Evaluate post-optimization performance
        post_scores = extractor.evaluate(test_texts, expected_test_results)
        print(
            f"   üìà Post-GEPA ‚Äî Accuracy: {post_scores.get('accuracy', 0):.1%}, "
            f"F1: {post_scores.get('f1', 0):.3f}"
        )

        if did_optimize:
            delta_acc = post_scores.get("accuracy", 0) - baseline_scores.get(
                "accuracy", 0
            )
            delta_f1 = post_scores.get("f1", 0) - baseline_scores.get("f1", 0)
            print(f"   Œî Improvement ‚Äî Accuracy: {delta_acc:+.1%}, F1: {delta_f1:+.3f}")

        # Step 5: Test optimized performance
        print("\n5Ô∏è‚É£ Testing GEPA-optimized performance...")

        for i, test_text in enumerate(test_texts, 1):
            result = extractor.extract(test_text)
            print(f"\n   Test {i}:")
            print(f"   ‚îú‚îÄ Confidence: {result.confidence:.1%}")
            print(f"   ‚îú‚îÄ Name: {result.entities.get('person_name')}")
            print(f"   ‚îú‚îÄ Title: {result.entities.get('job_title')}")
            print(f"   ‚îú‚îÄ Experience: {result.entities.get('years_experience')} years")
            print(f"   ‚îî‚îÄ Specialization: {result.entities.get('specialization')}")

        # Step 6: GEPA vs MIPROv2 - When to use which?
        print("\n6Ô∏è‚É£ GEPA vs MIPROv2 - When to use which?")
        print()
        print("   Use GEPA when:")
        print("   ‚úì You have ground truth data for training")
        print("   ‚úì The task requires complex reasoning")
        print("   ‚úì Understanding *why* extractions fail is valuable")
        print("   ‚úì You want the optimizer to learn from detailed feedback")
        print("   ‚úì You need Pareto-optimal prompt evolution")
        print()
        print("   Use MIPROv2 when:")
        print("   ‚úì You want fast, general-purpose optimization")
        print("   ‚úì The task is relatively straightforward")
        print("   ‚úì You have limited training data")
        print("   ‚úì Joint instruction + example optimization is sufficient")
        print()
        print("   üí° Pro tip: Try both and see which works better for your use case!")

        # Step 7: GEPA-specific features
        print("\n7Ô∏è‚É£ GEPA-Specific Features:")
        print("   ‚Ä¢ Textual feedback generation (not just scores)")
        print("   ‚Ä¢ Reflective prompt evolution (learns from mistakes)")
        print("   ‚Ä¢ Pareto frontier candidate selection")
        print("   ‚Ä¢ Can merge successful program variants")
        print("   ‚Ä¢ Supports inference-time search")
        print("   ‚Ä¢ Detailed optimization statistics tracking")

        # Step 8: Best practices
        print("\n8Ô∏è‚É£ GEPA Optimization Best Practices:")
        print("   ‚Ä¢ Always provide expected results for best feedback")
        print("   ‚Ä¢ Use 'auto=heavy' for production (we used 'light' for demo)")
        print()
        print("   ‚Ä¢ Two-model strategy (like this example):")
        print("     - Fast model (Gemini 2.5 Flash Lite) for actual extractions")
        print("     - Strong model (Gemini 2.5 Flash) for reflections")
        print("     This balances cost & quality!")
        print()
        print("   ‚Ä¢ Example configuration:")
        print("     from langstruct.optimizers import GEPAOptimizer")
        print("     import dspy")
        print()
        print("     optimizer = GEPAOptimizer(")
        print("         reflection_lm=dspy.LM(")
        print("             'gemini/gemini-2.5-flash',")
        print("             max_tokens=32000,  # Important for detailed reflections!")
        print("             temperature=1.0    # Higher temp for creative improvements")
        print("         ),")
        print("         auto='heavy'")
        print("     )")
        print()
        print("   ‚Ä¢ Start with 20-50 diverse training examples")
        print("   ‚Ä¢ Monitor the feedback to understand what GEPA learns")
        print("   ‚Ä¢ Use track_stats=True to see optimization progress")

        print("\nüéâ GEPA optimization provides richer feedback-driven improvement!")
        print("   Reflective evolution = better prompts over time")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("\nüí° Common fixes:")
        print("   ‚Ä¢ Ensure GOOGLE_API_KEY is set: export GOOGLE_API_KEY='your-key'")
        print("   ‚Ä¢ Check that API key has sufficient quota")
        print("   ‚Ä¢ Verify Gemini 2.0 Flash models are available in your region")
        print("   ‚Ä¢ Check that training data format is correct")
        print("   ‚Ä¢ Verify network stability during optimization")


if __name__ == "__main__":
    main()
