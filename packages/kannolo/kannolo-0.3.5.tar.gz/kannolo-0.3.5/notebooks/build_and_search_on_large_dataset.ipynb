{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a96f48",
   "metadata": {},
   "source": [
    "# Build and Search on Large Dataset with f16 Memory Optimization\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading large f32 dense dataset in batches using mmap (memory mapping) to avoid filling the memory.\n",
    "2. Converting to f16/u16 format (halves the used memory).\n",
    "3. Building dense f16 HNSW index.\n",
    "4. Single query search and batch search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc16672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from kannolo import DensePlainHNSWf16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19439f7",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "data_folder = \"\"  # Path to your dataset folder\n",
    "dataset_path = os.path.join(data_folder, \"dataset.npy\") # 1D array of f32 of len num_vectors * dim\n",
    "queries_path = os.path.join(data_folder, \"queries.npy\") # 2D array of f32 of shape (num_queries, dim)\n",
    "\n",
    "# Load dataset info to get dimensions\n",
    "dataset_mmap_info = np.load(dataset_path, mmap_mode='r')\n",
    "num_vectors, dim = dataset_mmap_info.shape\n",
    "del dataset_mmap_info\n",
    "\n",
    "# Load queries info\n",
    "queries_info = np.load(queries_path, mmap_mode='r')\n",
    "num_queries = queries_info.shape[0]\n",
    "del queries_info\n",
    "\n",
    "print(f\"Dataset: {num_vectors:,} vectors of dimension {dim}\")\n",
    "print(f\"Queries: {num_queries:,} vectors\")\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "print(f\"Queries path: {queries_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c461228",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset in Batches and Convert to f16/u16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17609cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset using memory mapping to avoid loading all into memory\n",
    "dataset_mmap = np.load(dataset_path, mmap_mode='r')\n",
    "print(f\"Memory-mapped dataset shape: {dataset_mmap.shape}\")\n",
    "\n",
    "# Process in batches to convert f32 -> f16 -> u16\n",
    "batch_size = 10000\n",
    "num_batches = (num_vectors + batch_size - 1) // batch_size\n",
    "\n",
    "# Pre-allocate u16 array for the entire dataset\n",
    "dataset_u16 = np.empty((num_vectors, dim), dtype=np.uint16)\n",
    "\n",
    "print(f\"Processing {num_batches} batches of size {batch_size}\")\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, num_vectors)\n",
    "    \n",
    "    # Load batch from memory-mapped array\n",
    "    batch_f32 = dataset_mmap[start_idx:end_idx]\n",
    "    \n",
    "    # Convert f32 -> f16 -> u16 bit representation\n",
    "    batch_f16 = batch_f32.astype(np.float16)\n",
    "    batch_u16 = batch_f16.view(np.uint16)\n",
    "    \n",
    "    # Store in pre-allocated array\n",
    "    dataset_u16[start_idx:end_idx] = batch_u16\n",
    "    \n",
    "    if (i + 1) % 2 == 0 or i == num_batches - 1:\n",
    "        print(f\"Processed batch {i + 1}/{num_batches}\")\n",
    "\n",
    "print(f\"\\nOriginal f32 dataset: {dataset_mmap.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Converted u16 dataset: {dataset_u16.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Memory savings: {((dataset_mmap.nbytes - dataset_u16.nbytes) / dataset_mmap.nbytes) * 100:.1f}%\")\n",
    "\n",
    "# Close memory map\n",
    "del dataset_mmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4041b",
   "metadata": {},
   "source": [
    "## Step 3: Build Dense f16 HNSW Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd30e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the f16 index from u16 data\n",
    "print(\"Building HNSW index from f16 data...\")\n",
    "\n",
    "# Flatten the u16 array for the index builder\n",
    "data_flat = dataset_u16.flatten()\n",
    "\n",
    "# Build index\n",
    "index_f16 = DensePlainHNSWf16.build_from_array(\n",
    "    data_flat, # Flattened u16 data array\n",
    "    dim, # Dimension of the vectors\n",
    "    m=32, # Number of neighbors per node (doubled at ground layer). Higher values improve accuracy but increase memory and build time.\n",
    "    ef_construction=200, # Controls index quality/speed trade-off during construction. Higher values improve accuracy but increase build time and memory usage.\n",
    "    metric=\"ip\" # \"l2\" for Euclidean, \"ip\" for Inner Product \n",
    ")\n",
    "\n",
    "print(\"✅ HNSW f16 index built successfully\")\n",
    "print(f\"Index contains {num_vectors} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5c4a2",
   "metadata": {},
   "source": [
    "## Step 4: Load Queries and Perform Single Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries\n",
    "queries_f32 = np.load(queries_path)\n",
    "print(f\"Loaded {queries_f32.shape[0]} queries of dimension {queries_f32.shape[1]}\")\n",
    "\n",
    "# Single query search - query ID 15\n",
    "query_id = 15 # Take a random query ID\n",
    "single_query = queries_f32[query_id]\n",
    "\n",
    "print(f\"\\nPerforming single search for query ID {query_id}\")\n",
    "\n",
    "# Search with single query\n",
    "k = 10 # Number of results to retrieve\n",
    "ef_search = 100 # Controls accuracy/speed trade-off during search. Higher values improve accuracy but increase search time.\n",
    "\n",
    "distances, ids = index_f16.search(\n",
    "    single_query,\n",
    "    k,\n",
    "    ef_search\n",
    ")\n",
    "\n",
    "print(f\"\\nSingle query results (k={k}):\")\n",
    "print(f\"Distances: {distances}\")\n",
    "print(f\"Document IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0454c25",
   "metadata": {},
   "source": [
    "## Step 5: Batch Search with All Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10826f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch search with all queries\n",
    "print(f\"Performing batch search with {queries_f32.shape[0]} queries\")\n",
    "\n",
    "batch_distances, batch_ids = index_f16.search_batch(\n",
    "    queries_path,\n",
    "    k,\n",
    "    ef_search\n",
    ")\n",
    "\n",
    "print(f\"Result shape - distances: {batch_distances.shape}, ids: {batch_ids.shape}\")\n",
    "\n",
    "# Verify consistency between single and batch search for query 15\n",
    "batch_results_for_single_query = batch_ids[k * query_id : k *(query_id+1)]\n",
    "\n",
    "ids_match = np.array_equal(ids, batch_results_for_single_query)\n",
    "\n",
    "print(f\"\\nConsistency check for query {query_id}:\")\n",
    "print(f\"IDs match: {ids_match}\")\n",
    "\n",
    "if ids_match:\n",
    "    print(\"✅ Single and batch search results are consistent!\")\n",
    "else:\n",
    "    print(\"❌ Results differ between single and batch search\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
