{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import ir_datasets\n",
    "from kannolo import SparsePlainHNSWf16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MS MARCO SPLADE (sparse)\n",
    "queries_path = os.path.expanduser('~/base_path/datasets_numpy/queries/ms_marco_splade/')\n",
    "index_path = os.path.expanduser('~/base_path/indexes/kannolo/kannolo_sparse_efc_2000_m_32_metric_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "queries_components = np.load(queries_path + 'components.npy') # Query components\n",
    "queries_values = np.load(queries_path + 'values.npy') # Query values\n",
    "queries_offsets = np.load(queries_path + 'offsets.npy') # Query offsets\n",
    "index = SparsePlainHNSWf16.load(index_path) # Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a query\n",
    "query_id_1 = 1500\n",
    "query_id_2 = 5000\n",
    "query_components_1 = queries_components[queries_offsets[query_id_1]:queries_offsets[query_id_1 + 1]]\n",
    "query_values_1 = queries_values[queries_offsets[query_id_1]:queries_offsets[query_id_1 + 1]]\n",
    "query_components_2 = queries_components[queries_offsets[query_id_2]:queries_offsets[query_id_2 + 1]]\n",
    "query_values_2 = queries_values[queries_offsets[query_id_2]:queries_offsets[query_id_2 + 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set search parameters\n",
    "k = 10\n",
    "efSearch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform search\n",
    "dists_1, ids_1 = index.search(query_components_1, query_values_1, k, efSearch)\n",
    "dists_2, ids_2 = index.search(query_components_2, query_values_2, k, efSearch)\n",
    "dists_1 = dists_1.reshape(-1, 10)\n",
    "ids_1 = ids_1.reshape(-1, 10)\n",
    "dists_2 = dists_2.reshape(-1, 10)\n",
    "ids_2 = ids_2.reshape(-1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ir_dataset dataset string id\n",
    "ir_dataset_string = \"msmarco-passage/dev/small\"\n",
    "# Load the dataset\n",
    "dataset = ir_datasets.load(\"msmarco-passage/dev/small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_passage_1 = [query for query in dataset.queries_iter()][query_id_1].text\n",
    "query_passage_2 = [query for query in dataset.queries_iter()][query_id_2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_passages = dataset.docs_iter()[:]\n",
    "results_1 = [documents_passages[int(i)].text for i in ids_1[0]]\n",
    "results_2 = [documents_passages[int(i)].text for i in ids_2[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "ir_measure = ir_measures.parse_measure(\"MRR@10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remapping the query ids for metric evaluation\n",
    "real_query_id_1 = [query for query in dataset.queries_iter()][query_id_1].query_id\n",
    "real_query_id_2 = [query for query in dataset.queries_iter()][query_id_2].query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the results for metric evaluation\n",
    "results_for_metric_1 = []\n",
    "for dd, ii in zip(dists_1[0], ids_1[0]):\n",
    "    results_for_metric_1.append(ir_measures.ScoredDoc(real_query_id_1, str(ii), dd))\n",
    "\n",
    "results_for_metric_2 = []\n",
    "for dd, ii in zip(dists_2[0], ids_2[0]):\n",
    "    results_for_metric_2.append(ir_measures.ScoredDoc(real_query_id_2, str(ii), dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the qrels (relevance judgments) for the dataset\n",
    "qrels = dataset.qrels\n",
    "qrel_1 = [q for q in qrels if q.query_id == real_query_id_1]\n",
    "qrel_2 = [q for q in qrels if q.query_id == real_query_id_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MRR@10 metric\n",
    "print(\"Metric evaluation for query 1\", ir_measures.calc_aggregate([ir_measure], qrel_1, results_for_metric_1))\n",
    "print(\"Metric evaluation for query 2\", ir_measures.calc_aggregate([ir_measure], qrel_2, results_for_metric_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_passage_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sparse representation better capture the specific question thanks to word-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_passage_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sparse representations in this case are fooled by the matching word. \"kids\" is associated to \"children\" and the result is considered relevant. However, the relevant document containing a real definition of dignity is absent in the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
