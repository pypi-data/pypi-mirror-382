# âš¡ï¸ LexiDecay â€” The Adaptive Lexical Decay Classifier  
*By Mohammad Taha Gorji*

> **A blazing-fast, semi-supervised text classification algorithm** based on adaptive lexical weighting, frequency decay, and probabilistic scoring â€” all without any training or labeled dataset.
> **LexiDecay is a semi-supervised lexical weighting model for unstructured text. It classifies content by adaptive word-frequency decay and soft lexical scoring. Fast (O(nÂ·m)), language-flexible, and training-free â€” ideal for topic classification, semantic filtering, and intent detection.**

---

## ğŸŒŒ Algorithm Philosophy & Core Idea

**LexiDecay** is inspired by the way human cognition evaluates language â€” not by rigid statistical training, but by dynamically weighting words according to their contextual importance and rarity.  
Instead of â€œlearningâ€ through countless iterations, **LexiDecay** *understands* by **measuring the gravitational pull of words** within conceptual clusters.

The algorithm analyzes each categoryâ€™s text content, counts and weights its tokens, and applies a **decay function** that reduces the influence of overly common words (like â€œtheâ€, â€œofâ€, â€œandâ€).  
During classification, it computes soft lexical similarities using adaptive decay, inverse document frequency, and a softmax-based probability normalization.

> ğŸ§  *Philosophically, LexiDecay reflects a cognitive model of understanding â€” flexible, intuitive, and progressively self-balancing.*

---

## ğŸ§© Scientific Position

| Category | Description |
|-----------|--------------|
| **Learning Type** | Semi-supervised lexical weighting |
| **Data Type** | Unstructured free text |
| **Complexity** | O(n Ã— m) â€” *n = words in input, m = number of categories* |
| **Core Mechanism** | Adaptive word-frequency decay + soft lexical scoring |
| **Primary Fields** | NLP, cognitive AI, text understanding, knowledge extraction |

---

## ğŸš€ Real-World Applications

LexiDecay is suitable for a wide variety of language-intelligent systems:

- ğŸ—‚ **Topic classification** â€” Distinguish content across domains (e.g. science, art, politics).  
- ğŸ¯ **Intent detection** â€” Recognize user intentions from text queries or chatbot messages.  
- ğŸ§­ **Semantic filtering** â€” Filter or route information based on conceptual meaning.  
- ğŸª¶ **Keyword-based reasoning** â€” Identify thematic or conceptual similarity.  
- ğŸ§  **Cognitive AI prototypes** â€” For lightweight, reasoning-like models without deep networks.  

---

## âš–ï¸ Advantages Over Classical Models

| Feature | LexiDecay | Classical Models (Naive Bayes, TF-IDF, etc.) |
|----------|------------|-----------------------------------------------|
| **Training Required** | âŒ None â€” works instantly | âœ… Needs training |
| **Computation Speed** | âš¡ Extremely fast (O(nÂ·m)) | ğŸ¢ Often slower (training + inference) |
| **Flexibility** | ğŸ§© Add or remove categories freely | ğŸ”’ Fixed to trained dataset |
| **Data Requirements** | ğŸŒ± Works with few samples | ğŸ“Š Needs many labeled samples |
| **Common Word Handling** | ğŸª¶ Auto frequency decay & adaptive weighting | âš™ï¸ Manual stopword removal |
| **Language Support** | ğŸŒ Fully language-independent | âš ï¸ Usually language-specific |
| **Explainability** | ğŸ” Transparent lexical logic | ğŸ•³ Often black-box statistics |

> ğŸ’¡ **LexiDecay** combines the interpretability of lexical systems with the adaptability of probabilistic models â€” no training, no fine-tuning, no waiting.

---

## âš™ï¸ Installation

```bash
pip install LexiDecay
````

Thatâ€™s it! ğŸª„

---

## ğŸ§± Getting Started

Below is a full example of how to use **LexiDecay** from scratch.

```python
from LexiDecay import LexiDecayModel

# 1ï¸âƒ£ Create a model
m = LexiDecayModel()

# 2ï¸âƒ£ Add categories (each category can be a string or list of texts)
m.add_category("science", open("science.txt").read())
m.add_category("philosophy", open("philosophy.txt").read())

# 3ï¸âƒ£ Classify new input
text = "Quantum theories explore the probabilistic structure of the universe."
result = m.classify(text)

print(result["top"])        # ('science', score, probability)
print(result["probs"])      # Probabilities for all categories
```

---

## ğŸ§  Function Reference & API Details

### ğŸ”¹ `add_category(label, content)`

Adds or replaces a category.

* `label`: `str` â†’ name of the category
* `content`: `str` or `List[str]` â†’ text data belonging to that category

Automatically rebuilds the internal vocabulary and frequency statistics.

---

### ğŸ”¹ `classify(input_text, decay=0.5, use_idf=False, auto_common_reduce=True, common_decay=0.7, min_common_mult=0.05, ignore_input_repetitions=False)`

Performs text classification and returns a dictionary with:

```python
{
  "scores": {label: float, ...},
  "probs": {label: float, ...},
  "matches": {label: {matched words, stats...}},
  "top": (best_label, score, probability)
}
```

#### Parameters:

| Parameter                  | Type  | Default | Description                                                                     |
| -------------------------- | ----- | ------- | ------------------------------------------------------------------------------- |
| `decay`                    | float | 0.5     | Controls how strongly frequent words lose influence (0 = linear, 1 = no decay). |
| `use_idf`                  | bool  | False   | Applies inverse-document-frequency weighting.                                   |
| `auto_common_reduce`       | bool  | True    | Automatically detects common words and lowers their impact.                     |
| `common_decay`             | float | 0.7     | Strength of reduction for common words.                                         |
| `min_common_mult`          | float | 0.05    | Minimum multiplier applied to frequent words.                                   |
| `ignore_input_repetitions` | bool  | False   | If True, counts each unique input word only once.                               |

---

### ğŸ”¹ `save_model(path)`

Saves the entire model (categories + data) into a `.pkl` file.

```python
m.save_model("lexidecay.pkl")
```

---

### ğŸ”¹ `load_model(path)`

Loads a model from a `.pkl` file.

```python
m2 = LexiDecayModel.load_model("lexidecay.pkl")
```


---

## ğŸŒŸ Why LexiDecay Feels Different

* **Human-like text perception:** adaptive decay mimics cognitive salience.
* **Instant deployability:** no model training â€” just plug and classify.
* **Infinite extendability:** add categories anytime, instantly rebuilt.
* **Compact and dependency-light:** only requires NumPy.
* **Transparent math:** pure lexical weighting, fully explainable results.

---

## ğŸ§¬ Example: Multi-category Classification

```python
from LexiDecay import LexiDecayModel
m = LexiDecayModel()
m.add_category("tech", ["AI","Model","AI algorithms", "neural networks", "deep learning"])
m.add_category("art", ["painting", "music", "creativity", "aesthetic beauty"])
m.add_category("sports", ["football", "strength", "competition"])

res = m.classify("New AI model beats humans at creative painting tasks.")
print(res)
# Output â†’ ('art', score, probability)
```

---

## ğŸ§© Citation

If you use **LexiDecay** in academic work, please cite:

> Mohammad Taha Gorji, *LexiDecay: Semi-supervised Lexical Decay Model for Adaptive Text Classification (2025)*

---

## ğŸ”¹ Examples

You can see **LexiDecay Examples** for some examples:

> [See here some examples](https://github.com/mr-r0ot/LexiDecay/tree/main/Examples)

---

## ğŸª„ Author

**Mohammad Taha Gorji**
Creator of *LexiDecay*
AI Researcher & Cognitive Systems Developer

---

## ğŸ–¤ License

Apache2 License Â© 2025 â€” Mohammad Taha Gorji
Open for research, education, and innovation.

---

> â€œLexiDecay doesnâ€™t learn â€” it understands.â€ ğŸ§ âœ¨

```
