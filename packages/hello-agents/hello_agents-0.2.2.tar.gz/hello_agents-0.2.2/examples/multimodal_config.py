"""å¤šæ¨¡æ€é…ç½®æ–‡ä»¶ - æ”¯æŒçœŸå®çš„è§†è§‰å’Œè§†é¢‘ç†è§£API"""

import os
from typing import Dict, Any, Optional

class MultimodalConfig:
    """å¤šæ¨¡æ€é…ç½®ç±»"""
    
    def __init__(self):
        # è§†è§‰ç†è§£APIé…ç½®
        self.vision_config = {
            # OpenAI GPT-4V
            "openai_gpt4v": {
                "api_key": os.getenv("OPENAI_API_KEY"),
                "model": "gpt-4-vision-preview",
                "max_tokens": 1000,
                "enabled": bool(os.getenv("OPENAI_API_KEY"))
            },
            
            # Google Gemini Vision
            "google_gemini": {
                "api_key": os.getenv("GOOGLE_API_KEY"),
                "model": "gemini-pro-vision",
                "enabled": bool(os.getenv("GOOGLE_API_KEY"))
            },
            
            # Claude 3 Vision
            "claude_vision": {
                "api_key": os.getenv("ANTHROPIC_API_KEY"),
                "model": "claude-3-opus-20240229",
                "enabled": bool(os.getenv("ANTHROPIC_API_KEY"))
            },
            
            # æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœæœ‰GPUï¼‰
            "local_vision": {
                "model_path": "./models/blip2",
                "device": "cuda" if os.getenv("CUDA_AVAILABLE") else "cpu",
                "enabled": False  # éœ€è¦æ‰‹åŠ¨å¯ç”¨
            }
        }
        
        # è§†é¢‘ç†è§£APIé…ç½®
        self.video_config = {
            # OpenAI GPT-4V (é€šè¿‡å…³é”®å¸§)
            "openai_keyframes": {
                "api_key": os.getenv("OPENAI_API_KEY"),
                "model": "gpt-4-vision-preview",
                "frame_interval": 30,  # æ¯30ç§’æå–ä¸€å¸§
                "enabled": bool(os.getenv("OPENAI_API_KEY"))
            },
            
            # Google Video Intelligence API
            "google_video_ai": {
                "api_key": os.getenv("GOOGLE_API_KEY"),
                "enabled": bool(os.getenv("GOOGLE_API_KEY"))
            },
            
            # æœ¬åœ°è§†é¢‘åˆ†æ
            "local_video": {
                "model_path": "./models/video_analysis",
                "enabled": False
            }
        }
        
        # è¯­éŸ³è¯†åˆ«é…ç½®
        self.speech_config = {
            "openai_whisper": {
                "api_key": os.getenv("OPENAI_API_KEY"),
                "model": "whisper-1",
                "enabled": bool(os.getenv("OPENAI_API_KEY"))
            },
            
            "local_whisper": {
                "model_size": "base",
                "enabled": True  # Whisperå¯ä»¥æœ¬åœ°è¿è¡Œ
            }
        }
    
    def get_available_vision_provider(self) -> Optional[str]:
        """è·å–å¯ç”¨çš„è§†è§‰ç†è§£æä¾›å•†"""
        for provider, config in self.vision_config.items():
            if config.get("enabled", False):
                return provider
        return None
    
    def get_available_video_provider(self) -> Optional[str]:
        """è·å–å¯ç”¨çš„è§†é¢‘ç†è§£æä¾›å•†"""
        for provider, config in self.video_config.items():
            if config.get("enabled", False):
                return provider
        return None
    
    def get_available_speech_provider(self) -> Optional[str]:
        """è·å–å¯ç”¨çš„è¯­éŸ³è¯†åˆ«æä¾›å•†"""
        for provider, config in self.speech_config.items():
            if config.get("enabled", False):
                return provider
        return None

# ç¯å¢ƒå˜é‡è®¾ç½®æŒ‡å—
ENV_SETUP_GUIDE = """
ğŸ”§ ç¯å¢ƒå˜é‡è®¾ç½®æŒ‡å—

ä¸ºäº†ä½¿ç”¨çœŸå®çš„å¤šæ¨¡æ€APIï¼Œè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

1. OpenAI API (æ¨è)ï¼š
   export OPENAI_API_KEY="your-openai-api-key"

2. Google APIï¼š
   export GOOGLE_API_KEY="your-google-api-key"

3. Anthropic APIï¼š
   export ANTHROPIC_API_KEY="your-anthropic-api-key"

4. å¦‚æœæœ‰CUDA GPUï¼š
   export CUDA_AVAILABLE="true"

Windowsç”¨æˆ·è¯·ä½¿ç”¨ï¼š
set OPENAI_API_KEY=your-openai-api-key

æˆ–è€…åœ¨Pythonä¸­è®¾ç½®ï¼š
import os
os.environ["OPENAI_API_KEY"] = "your-api-key"
"""

def print_setup_guide():
    """æ‰“å°è®¾ç½®æŒ‡å—"""
    print(ENV_SETUP_GUIDE)

def check_multimodal_capabilities():
    """æ£€æŸ¥å¤šæ¨¡æ€èƒ½åŠ›"""
    config = MultimodalConfig()
    
    print("ğŸ” æ£€æŸ¥å¤šæ¨¡æ€èƒ½åŠ›...")
    print("=" * 50)
    
    # æ£€æŸ¥è§†è§‰ç†è§£
    vision_provider = config.get_available_vision_provider()
    if vision_provider:
        print(f"âœ… è§†è§‰ç†è§£: {vision_provider}")
    else:
        print("âŒ è§†è§‰ç†è§£: æœªé…ç½®")
    
    # æ£€æŸ¥è§†é¢‘ç†è§£
    video_provider = config.get_available_video_provider()
    if video_provider:
        print(f"âœ… è§†é¢‘ç†è§£: {video_provider}")
    else:
        print("âŒ è§†é¢‘ç†è§£: æœªé…ç½®")
    
    # æ£€æŸ¥è¯­éŸ³è¯†åˆ«
    speech_provider = config.get_available_speech_provider()
    if speech_provider:
        print(f"âœ… è¯­éŸ³è¯†åˆ«: {speech_provider}")
    else:
        print("âŒ è¯­éŸ³è¯†åˆ«: æœªé…ç½®")
    
    if not any([vision_provider, video_provider, speech_provider]):
        print("\nâš ï¸ æœªæ£€æµ‹åˆ°å¤šæ¨¡æ€APIé…ç½®")
        print("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼è¿›è¡Œæ¼”ç¤º")
        print_setup_guide()
    
    return {
        "vision": vision_provider,
        "video": video_provider, 
        "speech": speech_provider
    }

# çœŸå®APIè°ƒç”¨ç¤ºä¾‹ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
def call_real_vision_api(image_path: str, prompt: str = "æè¿°è¿™å¼ å›¾ç‰‡") -> str:
    """è°ƒç”¨çœŸå®çš„è§†è§‰API"""
    config = MultimodalConfig()
    provider = config.get_available_vision_provider()
    
    if not provider:
        return "æœªé…ç½®è§†è§‰APIï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ"
    
    try:
        if provider == "openai_gpt4v":
            return _call_openai_vision(image_path, prompt, config.vision_config[provider])
        elif provider == "google_gemini":
            return _call_google_vision(image_path, prompt, config.vision_config[provider])
        elif provider == "claude_vision":
            return _call_claude_vision(image_path, prompt, config.vision_config[provider])
        else:
            return "ä¸æ”¯æŒçš„è§†è§‰APIæä¾›å•†"
    except Exception as e:
        return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"

def _call_openai_vision(image_path: str, prompt: str, config: Dict[str, Any]) -> str:
    """è°ƒç”¨OpenAI GPT-4V API"""
    try:
        import openai
        import base64
        
        # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        client = openai.OpenAI(api_key=config["api_key"])
        
        response = client.chat.completions.create(
            model=config["model"],
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=config["max_tokens"]
        )
        
        return response.choices[0].message.content
        
    except ImportError:
        return "è¯·å®‰è£…openaiåº“: pip install openai"
    except Exception as e:
        return f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}"

def _call_google_vision(image_path: str, prompt: str, config: Dict[str, Any]) -> str:
    """è°ƒç”¨Google Gemini Vision API"""
    try:
        import google.generativeai as genai
        from PIL import Image
        
        genai.configure(api_key=config["api_key"])
        model = genai.GenerativeModel(config["model"])
        
        image = Image.open(image_path)
        response = model.generate_content([prompt, image])
        
        return response.text
        
    except ImportError:
        return "è¯·å®‰è£…google-generativeaiåº“: pip install google-generativeai"
    except Exception as e:
        return f"Google APIè°ƒç”¨å¤±è´¥: {str(e)}"

def _call_claude_vision(image_path: str, prompt: str, config: Dict[str, Any]) -> str:
    """è°ƒç”¨Claude Vision API"""
    try:
        import anthropic
        import base64
        
        # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        client = anthropic.Anthropic(api_key=config["api_key"])
        
        response = client.messages.create(
            model=config["model"],
            max_tokens=1000,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": base64_image
                            }
                        },
                        {
                            "type": "text",
                            "text": prompt
                        }
                    ]
                }
            ]
        )
        
        return response.content[0].text
        
    except ImportError:
        return "è¯·å®‰è£…anthropicåº“: pip install anthropic"
    except Exception as e:
        return f"Claude APIè°ƒç”¨å¤±è´¥: {str(e)}"

if __name__ == "__main__":
    check_multimodal_capabilities()
