#!/usr/bin/env python3
"""
ë¡œê·¸ ë¶„ì„ ë„êµ¬ ëª¨ë“ˆ
ë¡œê·¸ íŒŒì¼ ë¶„ì„ ë° í†µê³„ ìƒì„±
"""

import os
import json
import gzip
import re
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
from collections import defaultdict, Counter
import pandas as pd

@dataclass
class LogAnalysisResult:
    """ë¡œê·¸ ë¶„ì„ ê²°ê³¼"""
    total_logs: int
    time_range: Tuple[datetime, datetime]
    level_distribution: Dict[str, int]
    logger_distribution: Dict[str, int]
    error_patterns: List[Dict[str, Any]]
    performance_metrics: Dict[str, Any]
    trends: Dict[str, List[Tuple[datetime, int]]]

class LogAnalyzer:
    """ë¡œê·¸ ë¶„ì„ê¸°"""
    
    def __init__(self, log_dir: str):
        self.log_dir = Path(log_dir)
        self.log_files = self._find_log_files()
    
    def _find_log_files(self) -> List[Path]:
        """ë¡œê·¸ íŒŒì¼ ì°¾ê¸°"""
        try:
            log_files = []
            
            # ì¼ë°˜ ë¡œê·¸ íŒŒì¼
            for pattern in ["*.log", "*.log.gz"]:
                log_files.extend(self.log_dir.glob(pattern))
            
            return sorted(log_files, key=lambda x: x.stat().st_mtime)
            
        except Exception as e:
            print(f"ë¡œê·¸ íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def _read_log_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """ë¡œê·¸ íŒŒì¼ ì½ê¸°"""
        try:
            logs = []
            
            # íŒŒì¼ ì—´ê¸°
            if file_path.suffix == '.gz':
                with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                    lines = f.readlines()
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
            
            # JSON ë¡œê·¸ íŒŒì‹±
            for line_num, line in enumerate(lines, 1):
                try:
                    log_entry = json.loads(line.strip())
                    log_entry['file_path'] = str(file_path)
                    log_entry['line_number'] = line_num
                    logs.append(log_entry)
                except json.JSONDecodeError:
                    # JSONì´ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ íŒŒì‹±
                    logs.append({
                        'timestamp': datetime.now().isoformat(),
                        'level': 'UNKNOWN',
                        'message': line.strip(),
                        'file_path': str(file_path),
                        'line_number': line_num
                    })
            
            return logs
            
        except Exception as e:
            print(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path}: {e}")
            return []
    
    def analyze_logs(self, hours: int = 24) -> LogAnalysisResult:
        """ë¡œê·¸ ë¶„ì„"""
        try:
            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(hours=hours)
            
            all_logs = []
            
            # ëª¨ë“  ë¡œê·¸ íŒŒì¼ ì½ê¸°
            for log_file in self.log_files:
                logs = self._read_log_file(log_file)
                
                # ì‹œê°„ í•„í„°ë§
                filtered_logs = []
                for log in logs:
                    try:
                        log_time = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00'))
                        if start_time <= log_time <= end_time:
                            filtered_logs.append(log)
                    except (ValueError, KeyError):
                        # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨
                        filtered_logs.append(log)
                
                all_logs.extend(filtered_logs)
            
            if not all_logs:
                return LogAnalysisResult(
                    total_logs=0,
                    time_range=(start_time, end_time),
                    level_distribution={},
                    logger_distribution={},
                    error_patterns=[],
                    performance_metrics={},
                    trends={}
                )
            
            # ë¶„ì„ ìˆ˜í–‰
            level_dist = self._analyze_level_distribution(all_logs)
            logger_dist = self._analyze_logger_distribution(all_logs)
            error_patterns = self._analyze_error_patterns(all_logs)
            performance_metrics = self._analyze_performance_metrics(all_logs)
            trends = self._analyze_trends(all_logs, start_time, end_time)
            
            # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
            timestamps = [datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00')) 
                         for log in all_logs if 'timestamp' in log]
            actual_time_range = (min(timestamps), max(timestamps)) if timestamps else (start_time, end_time)
            
            return LogAnalysisResult(
                total_logs=len(all_logs),
                time_range=actual_time_range,
                level_distribution=level_dist,
                logger_distribution=logger_dist,
                error_patterns=error_patterns,
                performance_metrics=performance_metrics,
                trends=trends
            )
            
        except Exception as e:
            print(f"ë¡œê·¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return LogAnalysisResult(
                total_logs=0,
                time_range=(start_time, end_time),
                level_distribution={},
                logger_distribution={},
                error_patterns=[],
                performance_metrics={},
                trends={}
            )
    
    def _analyze_level_distribution(self, logs: List[Dict[str, Any]]) -> Dict[str, int]:
        """ë ˆë²¨ë³„ ë¶„í¬ ë¶„ì„"""
        try:
            level_counter = Counter()
            for log in logs:
                level = log.get('level', 'UNKNOWN')
                level_counter[level] += 1
            
            return dict(level_counter)
            
        except Exception as e:
            print(f"ë ˆë²¨ ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_logger_distribution(self, logs: List[Dict[str, Any]]) -> Dict[str, int]:
        """ë¡œê±°ë³„ ë¶„í¬ ë¶„ì„"""
        try:
            logger_counter = Counter()
            for log in logs:
                logger_name = log.get('logger_name', 'UNKNOWN')
                logger_counter[logger_name] += 1
            
            return dict(logger_counter)
            
        except Exception as e:
            print(f"ë¡œê±° ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_error_patterns(self, logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì—ëŸ¬ íŒ¨í„´ ë¶„ì„"""
        try:
            error_patterns = []
            error_logs = [log for log in logs if log.get('level') in ['ERROR', 'CRITICAL']]
            
            # ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„
            error_messages = [log.get('message', '') for log in error_logs]
            message_counter = Counter(error_messages)
            
            for message, count in message_counter.most_common(10):
                if count > 1:  # 2íšŒ ì´ìƒ ë°œìƒí•œ ì—ëŸ¬ë§Œ
                    error_patterns.append({
                        'message': message,
                        'count': count,
                        'percentage': count / len(error_logs) * 100
                    })
            
            return error_patterns
            
        except Exception as e:
            print(f"ì—ëŸ¬ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_performance_metrics(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„"""
        try:
            metrics = {
                'api_calls': 0,
                'avg_response_time': 0,
                'slow_requests': 0,
                'error_rate': 0
            }
            
            api_logs = [log for log in logs if 'api' in log.get('message', '').lower()]
            error_logs = [log for log in logs if log.get('level') in ['ERROR', 'CRITICAL']]
            
            metrics['api_calls'] = len(api_logs)
            metrics['error_rate'] = len(error_logs) / len(logs) * 100 if logs else 0
            
            # ì‘ë‹µ ì‹œê°„ ë¶„ì„
            response_times = []
            for log in api_logs:
                extra_data = log.get('extra_data', {})
                if 'response_time' in extra_data:
                    response_times.append(extra_data['response_time'])
            
            if response_times:
                metrics['avg_response_time'] = sum(response_times) / len(response_times)
                metrics['slow_requests'] = len([rt for rt in response_times if rt > 5.0])
            
            return metrics
            
        except Exception as e:
            print(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_trends(self, logs: List[Dict[str, Any]], 
                       start_time: datetime, end_time: datetime) -> Dict[str, List[Tuple[datetime, int]]]:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            trends = {
                'total_logs': [],
                'error_logs': [],
                'warning_logs': []
            }
            
            # ì‹œê°„ë³„ ê·¸ë£¹í™”
            time_groups = defaultdict(int)
            error_groups = defaultdict(int)
            warning_groups = defaultdict(int)
            
            for log in logs:
                try:
                    log_time = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00'))
                    hour_key = log_time.replace(minute=0, second=0, microsecond=0)
                    
                    time_groups[hour_key] += 1
                    
                    if log.get('level') in ['ERROR', 'CRITICAL']:
                        error_groups[hour_key] += 1
                    elif log.get('level') == 'WARNING':
                        warning_groups[hour_key] += 1
                        
                except (ValueError, KeyError):
                    continue
            
            # íŠ¸ë Œë“œ ë°ì´í„° ìƒì„±
            trends['total_logs'] = sorted(time_groups.items())
            trends['error_logs'] = sorted(error_groups.items())
            trends['warning_logs'] = sorted(warning_groups.items())
            
            return trends
            
        except Exception as e:
            print(f"íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def search_logs(self, query: str, level: Optional[str] = None, 
                   hours: int = 24) -> List[Dict[str, Any]]:
        """ë¡œê·¸ ê²€ìƒ‰"""
        try:
            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(hours=hours)
            
            results = []
            
            # ëª¨ë“  ë¡œê·¸ íŒŒì¼ ê²€ìƒ‰
            for log_file in self.log_files:
                logs = self._read_log_file(log_file)
                
                for log in logs:
                    # ì‹œê°„ í•„í„°ë§
                    try:
                        log_time = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00'))
                        if not (start_time <= log_time <= end_time):
                            continue
                    except (ValueError, KeyError):
                        continue
                    
                    # ë ˆë²¨ í•„í„°ë§
                    if level and log.get('level') != level:
                        continue
                    
                    # ë©”ì‹œì§€ ê²€ìƒ‰
                    message = log.get('message', '')
                    if query.lower() in message.lower():
                        results.append(log)
            
            # ì‹œê°„ìˆœ ì •ë ¬
            results.sort(key=lambda x: x.get('timestamp', ''))
            
            return results
            
        except Exception as e:
            print(f"ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_report(self, hours: int = 24) -> str:
        """ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        try:
            analysis = self.analyze_logs(hours)
            
            report = []
            report.append("=" * 80)
            report.append("ğŸ“Š ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ")
            report.append("=" * 80)
            
            # ê¸°ë³¸ ì •ë³´
            report.append(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {analysis.time_range[0]} ~ {analysis.time_range[1]}")
            report.append(f"ğŸ“ˆ ì´ ë¡œê·¸ ìˆ˜: {analysis.total_logs:,}ê°œ")
            report.append("")
            
            # ë ˆë²¨ë³„ ë¶„í¬
            if analysis.level_distribution:
                report.append("ğŸ“‹ ë ˆë²¨ë³„ ë¶„í¬:")
                for level, count in sorted(analysis.level_distribution.items()):
                    percentage = count / analysis.total_logs * 100
                    report.append(f"  {level}: {count:,}ê°œ ({percentage:.1f}%)")
                report.append("")
            
            # ë¡œê±°ë³„ ë¶„í¬
            if analysis.logger_distribution:
                report.append("ğŸ·ï¸ ë¡œê±°ë³„ ë¶„í¬:")
                for logger_name, count in sorted(analysis.logger_distribution.items()):
                    percentage = count / analysis.total_logs * 100
                    report.append(f"  {logger_name}: {count:,}ê°œ ({percentage:.1f}%)")
                report.append("")
            
            # ì—ëŸ¬ íŒ¨í„´
            if analysis.error_patterns:
                report.append("âŒ ì£¼ìš” ì—ëŸ¬ íŒ¨í„´:")
                for i, pattern in enumerate(analysis.error_patterns[:5], 1):
                    report.append(f"  {i}. {pattern['message'][:50]}...")
                    report.append(f"     ë°œìƒ íšŸìˆ˜: {pattern['count']}íšŒ ({pattern['percentage']:.1f}%)")
                report.append("")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            if analysis.performance_metrics:
                report.append("âš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
                metrics = analysis.performance_metrics
                report.append(f"  API í˜¸ì¶œ ìˆ˜: {metrics.get('api_calls', 0):,}íšŒ")
                report.append(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {metrics.get('avg_response_time', 0):.2f}ì´ˆ")
                report.append(f"  ëŠë¦° ìš”ì²­: {metrics.get('slow_requests', 0):,}íšŒ")
                report.append(f"  ì—ëŸ¬ìœ¨: {metrics.get('error_rate', 0):.2f}%")
                report.append("")
            
            # íŠ¸ë Œë“œ
            if analysis.trends.get('total_logs'):
                report.append("ğŸ“ˆ ì‹œê°„ë³„ íŠ¸ë Œë“œ (ìµœê·¼ 5ì‹œê°„):")
                recent_trends = analysis.trends['total_logs'][-5:]
                for time_point, count in recent_trends:
                    report.append(f"  {time_point.strftime('%H:%M')}: {count}ê°œ")
                report.append("")
            
            report.append("=" * 80)
            
            return "\n".join(report)
            
        except Exception as e:
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def export_to_csv(self, output_file: str, hours: int = 24):
        """ë¡œê·¸ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            analysis = self.analyze_logs(hours)
            
            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(hours=hours)
            
            all_logs = []
            
            # ëª¨ë“  ë¡œê·¸ íŒŒì¼ ì½ê¸°
            for log_file in self.log_files:
                logs = self._read_log_file(log_file)
                
                # ì‹œê°„ í•„í„°ë§
                filtered_logs = []
                for log in logs:
                    try:
                        log_time = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00'))
                        if start_time <= log_time <= end_time:
                            filtered_logs.append(log)
                    except (ValueError, KeyError):
                        filtered_logs.append(log)
                
                all_logs.extend(filtered_logs)
            
            if not all_logs:
                print("ë‚´ë³´ë‚¼ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(all_logs)
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            columns = ['timestamp', 'level', 'logger_name', 'message', 'module', 'function']
            available_columns = [col for col in columns if col in df.columns]
            df = df[available_columns]
            
            # CSV ì €ì¥
            df.to_csv(output_file, index=False, encoding='utf-8')
            print(f"ë¡œê·¸ê°€ CSVë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤: {output_file}")
            
        except Exception as e:
            print(f"CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ§ª ë¡œê·¸ ë¶„ì„ ë„êµ¬ í…ŒìŠ¤íŠ¸")
    
    # ë¡œê·¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = LogAnalyzer("test_logs")
    
    # ë¶„ì„ ìˆ˜í–‰
    print("1. ë¡œê·¸ ë¶„ì„")
    analysis = analyzer.analyze_logs(hours=24)
    print(f"ì´ ë¡œê·¸ ìˆ˜: {analysis.total_logs}")
    print(f"ë ˆë²¨ ë¶„í¬: {analysis.level_distribution}")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n2. ë¡œê·¸ ê²€ìƒ‰")
    results = analyzer.search_logs("error", level="ERROR", hours=24)
    print(f"ì—ëŸ¬ ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    
    # ë³´ê³ ì„œ ìƒì„±
    print("\n3. ë¶„ì„ ë³´ê³ ì„œ")
    report = analyzer.generate_report(hours=24)
    print(report)
