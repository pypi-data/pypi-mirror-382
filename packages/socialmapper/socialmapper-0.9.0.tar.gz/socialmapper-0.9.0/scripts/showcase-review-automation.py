#!/usr/bin/env python3
"""
SocialMapper Showcase Review Automation

This script helps automate the community showcase review process by:
1. Collecting submissions from GitHub Discussions "Show & Tell" category
2. Analyzing submission quality and completeness
3. Generating review summaries and recommendations
4. Creating publication-ready content for featured analyses

Usage:
    python scripts/showcase-review-automation.py --month "September 2025"
    python scripts/showcase-review-automation.py --review-all --output-dir showcase-review
"""

import argparse
import json
import os
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import re
from dataclasses import dataclass, asdict

@dataclass
class SubmissionAnalysis:
    """Data class for submission analysis results."""
    title: str
    author: str
    url: str
    created_at: str
    upvotes: int
    comments: int
    body: str
    
    # Quality scores (1-5 scale)
    analytical_rigor: float = 0.0
    presentation_quality: float = 0.0
    real_world_impact: float = 0.0
    educational_value: float = 0.0
    innovation_creativity: float = 0.0
    
    # Overall metrics
    overall_score: float = 0.0
    recommendation: str = ""
    featured_tier: str = ""
    
    # Content analysis
    has_methodology: bool = False
    has_visualizations: bool = False
    has_code_examples: bool = False
    has_data_sources: bool = False
    has_real_world_application: bool = False
    has_conclusions: bool = False
    
    # Technical details
    mentions_socialmapper_version: bool = False
    mentions_configuration: bool = False
    mentions_performance: bool = False
    technical_depth: str = ""
    
    # Feedback and recommendations
    strengths: List[str] = None
    areas_for_improvement: List[str] = None
    specific_feedback: str = ""

class ShowcaseAnalyzer:
    """Analyzes showcase submissions for quality and completeness."""
    
    def __init__(self):
        self.quality_keywords = {
            'methodology': [
                'methodology', 'approach', 'method', 'technique', 'process',
                'workflow', 'pipeline', 'procedure', 'analysis steps'
            ],
            'data_sources': [
                'data source', 'dataset', 'census', 'openstreetmap', 'osm',
                'data collection', 'data acquisition', 'API', 'database'
            ],
            'visualizations': [
                'map', 'chart', 'graph', 'visualization', 'figure', 'plot',
                'isochrone', 'heatmap', 'choropleth', 'scatter', 'bar chart'
            ],
            'technical_details': [
                'socialmapper', 'python', 'configuration', 'parameters',
                'settings', 'version', 'installation', 'performance'
            ],
            'impact': [
                'policy', 'planning', 'decision', 'recommendation', 'application',
                'implementation', 'outcome', 'result', 'finding', 'insight'
            ],
            'innovation': [
                'novel', 'innovative', 'creative', 'unique', 'original',
                'new approach', 'breakthrough', 'advanced', 'cutting-edge'
            ]
        }
        
        self.scoring_weights = {
            'analytical_rigor': 0.30,
            'presentation_quality': 0.25,
            'real_world_impact': 0.20,
            'educational_value': 0.15,
            'innovation_creativity': 0.10
        }
    
    def analyze_submission(self, discussion_data: Dict[str, Any]) -> SubmissionAnalysis:
        """Analyze a single submission for quality and completeness."""
        
        submission = SubmissionAnalysis(
            title=discussion_data['title'],
            author=discussion_data['author']['login'],
            url=discussion_data['url'] if 'url' in discussion_data else '',
            created_at=discussion_data['createdAt'],
            upvotes=discussion_data.get('upvoteCount', 0),
            comments=discussion_data.get('comments', {}).get('totalCount', 0),
            body=discussion_data['body'],
            strengths=[],
            areas_for_improvement=[]
        )
        
        # Analyze content elements
        self._analyze_content_elements(submission)
        
        # Score quality dimensions
        self._score_analytical_rigor(submission)
        self._score_presentation_quality(submission)
        self._score_real_world_impact(submission)
        self._score_educational_value(submission)
        self._score_innovation_creativity(submission)
        
        # Calculate overall score
        self._calculate_overall_score(submission)
        
        # Generate recommendation
        self._generate_recommendation(submission)
        
        return submission
    
    def _analyze_content_elements(self, submission: SubmissionAnalysis):
        """Analyze content for key elements."""
        body_lower = submission.body.lower()
        
        # Check for methodology description
        methodology_indicators = sum(1 for keyword in self.quality_keywords['methodology'] 
                                   if keyword in body_lower)
        submission.has_methodology = methodology_indicators >= 2
        
        # Check for data sources
        data_source_indicators = sum(1 for keyword in self.quality_keywords['data_sources']
                                   if keyword in body_lower)
        submission.has_data_sources = data_source_indicators >= 1
        
        # Check for visualizations (look for image markers, links, or explicit mentions)
        viz_indicators = (
            sum(1 for keyword in self.quality_keywords['visualizations'] if keyword in body_lower) +
            body_lower.count('![') +  # Markdown images
            body_lower.count('.png') + body_lower.count('.jpg') +  # Image files
            body_lower.count('figure') + body_lower.count('chart')
        )
        submission.has_visualizations = viz_indicators >= 2
        
        # Check for code examples
        code_indicators = (
            body_lower.count('```') +  # Code blocks
            body_lower.count('`') +    # Inline code
            body_lower.count('python') +
            body_lower.count('socialmapper')
        )
        submission.has_code_examples = code_indicators >= 3
        
        # Check for real-world application discussion
        impact_indicators = sum(1 for keyword in self.quality_keywords['impact']
                              if keyword in body_lower)
        submission.has_real_world_application = impact_indicators >= 2
        
        # Check for conclusions/findings
        conclusion_indicators = (
            body_lower.count('conclusion') + body_lower.count('findings') +
            body_lower.count('results') + body_lower.count('insights') +
            body_lower.count('discovered') + body_lower.count('found')
        )
        submission.has_conclusions = conclusion_indicators >= 2
        
        # Technical depth analysis
        technical_indicators = sum(1 for keyword in self.quality_keywords['technical_details']
                                 if keyword in body_lower)
        
        if technical_indicators >= 5:
            submission.technical_depth = "High"
        elif technical_indicators >= 3:
            submission.technical_depth = "Medium"
        else:
            submission.technical_depth = "Low"
        
        # Check for specific technical details
        submission.mentions_socialmapper_version = 'version' in body_lower and 'socialmapper' in body_lower
        submission.mentions_configuration = any(word in body_lower for word in ['config', 'setting', 'parameter'])
        submission.mentions_performance = any(word in body_lower for word in ['performance', 'speed', 'time', 'optimization'])
    
    def _score_analytical_rigor(self, submission: SubmissionAnalysis):
        """Score analytical rigor (1-5 scale)."""
        score = 1.0  # Base score
        
        # Methodology description (+2 points max)
        if submission.has_methodology:
            score += 1.5
            submission.strengths.append("Clear methodology description")
        else:
            submission.areas_for_improvement.append("Add detailed methodology section")
        
        # Data sources documented (+1 point)
        if submission.has_data_sources:
            score += 1.0
            submission.strengths.append("Data sources properly documented")
        else:
            submission.areas_for_improvement.append("Document data sources and acquisition process")
        
        # Technical details (+0.5 points)
        if submission.technical_depth == "High":
            score += 0.5
            submission.strengths.append("Excellent technical documentation")
        elif submission.technical_depth == "Medium":
            score += 0.25
        
        submission.analytical_rigor = min(5.0, score)
    
    def _score_presentation_quality(self, submission: SubmissionAnalysis):
        """Score presentation quality (1-5 scale)."""
        score = 1.0  # Base score
        
        # Visualizations (+2 points max)
        if submission.has_visualizations:
            score += 2.0
            submission.strengths.append("Good use of visualizations and graphics")
        else:
            submission.areas_for_improvement.append("Include maps, charts, or other visualizations")
        
        # Clear structure and conclusions (+1.5 points)
        if submission.has_conclusions:
            score += 1.5
            submission.strengths.append("Clear findings and conclusions presented")
        else:
            submission.areas_for_improvement.append("Add clear conclusions and key findings section")
        
        # Length and detail assessment
        word_count = len(submission.body.split())
        if word_count >= 500:
            score += 0.5
            submission.strengths.append("Comprehensive and detailed presentation")
        elif word_count < 200:
            score -= 0.5
            submission.areas_for_improvement.append("Expand description with more detail")
        
        submission.presentation_quality = min(5.0, score)
    
    def _score_real_world_impact(self, submission: SubmissionAnalysis):
        """Score real-world impact (1-5 scale)."""
        score = 1.0  # Base score
        
        # Real-world application discussion (+2 points)
        if submission.has_real_world_application:
            score += 2.0
            submission.strengths.append("Clear real-world applications described")
        else:
            submission.areas_for_improvement.append("Describe practical applications and impact")
        
        # Policy/planning relevance (bonus based on keywords)
        body_lower = submission.body.lower()
        policy_keywords = ['policy', 'government', 'planning', 'municipal', 'public health', 'equity']
        policy_relevance = sum(1 for keyword in policy_keywords if keyword in body_lower)
        
        if policy_relevance >= 3:
            score += 1.5
            submission.strengths.append("Strong policy and planning relevance")
        elif policy_relevance >= 1:
            score += 1.0
        
        # Community engagement (based on comments/upvotes)
        engagement_score = (submission.comments * 0.5) + (submission.upvotes * 0.3)
        if engagement_score >= 10:
            score += 0.5
            submission.strengths.append("High community engagement and interest")
        
        submission.real_world_impact = min(5.0, score)
    
    def _score_educational_value(self, submission: SubmissionAnalysis):
        """Score educational value (1-5 scale)."""
        score = 1.0  # Base score
        
        # Code examples and reproducibility (+2 points)
        if submission.has_code_examples:
            score += 2.0
            submission.strengths.append("Includes code examples and technical details")
        else:
            submission.areas_for_improvement.append("Include code snippets and configuration details")
        
        # Transferable methodology (+1.5 points)
        if submission.has_methodology and submission.technical_depth in ["High", "Medium"]:
            score += 1.5
            submission.strengths.append("Transferable methodology for similar analyses")
        
        # Learning resources mentioned
        learning_keywords = ['tutorial', 'documentation', 'example', 'guide', 'reference']
        learning_mentions = sum(1 for keyword in learning_keywords 
                              if keyword in submission.body.lower())
        if learning_mentions >= 2:
            score += 0.5
        
        submission.educational_value = min(5.0, score)
    
    def _score_innovation_creativity(self, submission: SubmissionAnalysis):
        """Score innovation and creativity (1-5 scale)."""
        score = 1.0  # Base score
        
        # Novel approaches or techniques
        innovation_indicators = sum(1 for keyword in self.quality_keywords['innovation']
                                  if keyword in submission.body.lower())
        
        if innovation_indicators >= 3:
            score += 2.5
            submission.strengths.append("Innovative approach and creative problem-solving")
        elif innovation_indicators >= 1:
            score += 1.5
            submission.strengths.append("Some innovative elements and creative approaches")
        
        # Integration with other tools
        integration_keywords = ['integrate', 'combine', 'qgis', 'arcgis', 'r', 'jupyter', 'api']
        integration_count = sum(1 for keyword in integration_keywords
                              if keyword in submission.body.lower())
        
        if integration_count >= 2:
            score += 1.0
            submission.strengths.append("Creative integration with other tools and platforms")
        
        # Unique use case or application domain
        domain_keywords = ['healthcare', 'education', 'environment', 'transportation', 'equity', 'justice']
        domain_diversity = sum(1 for keyword in domain_keywords
                             if keyword in submission.body.lower())
        
        if domain_diversity >= 2:
            score += 0.5
        
        submission.innovation_creativity = min(5.0, score)
    
    def _calculate_overall_score(self, submission: SubmissionAnalysis):
        """Calculate weighted overall score."""
        weighted_score = (
            submission.analytical_rigor * self.scoring_weights['analytical_rigor'] +
            submission.presentation_quality * self.scoring_weights['presentation_quality'] +
            submission.real_world_impact * self.scoring_weights['real_world_impact'] +
            submission.educational_value * self.scoring_weights['educational_value'] +
            submission.innovation_creativity * self.scoring_weights['innovation_creativity']
        )
        
        # Convert to 100-point scale
        submission.overall_score = (weighted_score / 5.0) * 100
    
    def _generate_recommendation(self, submission: SubmissionAnalysis):
        """Generate recommendation and featured tier assignment."""
        score = submission.overall_score
        
        if score >= 90:
            submission.featured_tier = "Tier 1 - Major Showcase"
            submission.recommendation = "FEATURE - Exceptional analysis suitable for newsletter lead feature, conference presentations, and major showcase"
        elif score >= 80:
            submission.featured_tier = "Tier 2 - Community Highlight"
            submission.recommendation = "HIGHLIGHT - Excellent analysis for newsletter highlights, meetup presentations, and website gallery"
        elif score >= 70:
            submission.featured_tier = "Tier 3 - Showcase Gallery"
            submission.recommendation = "INCLUDE - Good analysis for website gallery, GitHub examples, and tutorial references"
        elif score >= 60:
            submission.featured_tier = "Acknowledge"
            submission.recommendation = "ACKNOWLEDGE - Satisfactory analysis, acknowledge in community roundups with improvement suggestions"
        else:
            submission.featured_tier = "Needs Improvement"
            submission.recommendation = "FEEDBACK - Provide detailed feedback and encourage revision"
        
        # Generate specific feedback
        self._generate_specific_feedback(submission)
    
    def _generate_specific_feedback(self, submission: SubmissionAnalysis):
        """Generate specific feedback for the author."""
        feedback_parts = []
        
        # Positive feedback
        if submission.strengths:
            feedback_parts.append("Strengths:")
            for strength in submission.strengths:
                feedback_parts.append(f"- {strength}")
            feedback_parts.append("")
        
        # Improvement suggestions
        if submission.areas_for_improvement:
            feedback_parts.append("Areas for Enhancement:")
            for improvement in submission.areas_for_improvement:
                feedback_parts.append(f"- {improvement}")
            feedback_parts.append("")
        
        # Specific technical suggestions
        if submission.technical_depth == "Low":
            feedback_parts.append("Technical Enhancement Suggestions:")
            feedback_parts.append("- Include SocialMapper version and configuration details")
            feedback_parts.append("- Add code snippets showing key analysis steps")
            feedback_parts.append("- Describe performance characteristics and optimization choices")
            feedback_parts.append("")
        
        # Publication pathway
        if submission.overall_score >= 70:
            feedback_parts.append("Publication Opportunities:")
            if submission.overall_score >= 90:
                feedback_parts.append("- Eligible for newsletter lead feature and major showcase")
                feedback_parts.append("- Consider submitting to academic conferences or journals")
                feedback_parts.append("- Invitation to present at upcoming community meetups")
            elif submission.overall_score >= 80:
                feedback_parts.append("- Featured in monthly newsletter community highlights")
                feedback_parts.append("- Included in website showcase gallery")
                feedback_parts.append("- Opportunity to present at community meetups")
            else:
                feedback_parts.append("- Included in community showcase gallery")
                feedback_parts.append("- Referenced in tutorial and documentation examples")
        
        submission.specific_feedback = "\n".join(feedback_parts)

class GitHubShowcaseCollector:
    """Collects Show & Tell submissions from GitHub Discussions."""
    
    def __init__(self, repo: str = "mihiarc/socialmapper"):
        self.repo = repo
    
    def get_show_and_tell_submissions(self, days: int = 30) -> List[Dict[str, Any]]:
        """Get Show & Tell submissions from specified time period."""
        
        query = '''
        query($owner: String!, $name: String!) {
          repository(owner: $owner, name: $name) {
            discussions(first: 100, orderBy: {field: CREATED_AT, direction: DESC}, categorySlug: "show-and-tell") {
              nodes {
                title
                body
                createdAt
                updatedAt
                category { name }
                author { login }
                comments(first: 1) { totalCount }
                upvoteCount
                url
                id
                number
              }
            }
          }
        }
        '''
        
        try:
            # Use GitHub CLI to execute GraphQL query
            owner, name = self.repo.split('/')
            result = subprocess.run([
                'gh', 'api', 'graphql',
                '-f', f'query={query}',
                '-F', f'owner={owner}',
                '-F', f'name={name}'
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                discussions = data['data']['repository']['discussions']['nodes']
                
                # Filter by date
                cutoff_date = datetime.now() - timedelta(days=days)
                recent_submissions = []
                
                for discussion in discussions:
                    created_at = datetime.fromisoformat(discussion['createdAt'].replace('Z', '+00:00'))
                    if created_at >= cutoff_date.replace(tzinfo=created_at.tzinfo):
                        recent_submissions.append(discussion)
                
                return recent_submissions
            else:
                print(f"Error fetching discussions: {result.stderr}")
                return []
                
        except Exception as e:
            print(f"Error collecting submissions: {e}")
            return []

def generate_review_report(submissions: List[SubmissionAnalysis], output_path: Path) -> str:
    """Generate comprehensive review report."""
    
    # Sort submissions by overall score
    submissions.sort(key=lambda x: x.overall_score, reverse=True)
    
    # Calculate summary statistics
    total_submissions = len(submissions)
    avg_score = sum(s.overall_score for s in submissions) / total_submissions if total_submissions > 0 else 0
    
    tier_counts = {}
    for submission in submissions:
        tier = submission.featured_tier
        tier_counts[tier] = tier_counts.get(tier, 0) + 1
    
    # Generate report
    report = f"""# SocialMapper Showcase Review Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary Statistics
- **Total Submissions**: {total_submissions}
- **Average Quality Score**: {avg_score:.1f}/100
- **Submissions by Tier**:
"""
    
    for tier, count in tier_counts.items():
        percentage = (count / total_submissions * 100) if total_submissions > 0 else 0
        report += f"  - {tier}: {count} ({percentage:.1f}%)\n"
    
    report += "\n## Detailed Reviews\n\n"
    
    for i, submission in enumerate(submissions, 1):
        report += f"""### {i}. {submission.title}
- **Author**: {submission.author}
- **Created**: {submission.created_at[:10]}
- **Overall Score**: {submission.overall_score:.1f}/100
- **Recommendation**: {submission.featured_tier}
- **Community Engagement**: {submission.upvotes} upvotes, {submission.comments} comments

#### Quality Breakdown:
- Analytical Rigor: {submission.analytical_rigor:.1f}/5
- Presentation Quality: {submission.presentation_quality:.1f}/5
- Real-World Impact: {submission.real_world_impact:.1f}/5
- Educational Value: {submission.educational_value:.1f}/5
- Innovation/Creativity: {submission.innovation_creativity:.1f}/5

#### Content Analysis:
- Methodology: {'âœ…' if submission.has_methodology else 'âŒ'}
- Visualizations: {'âœ…' if submission.has_visualizations else 'âŒ'}
- Code Examples: {'âœ…' if submission.has_code_examples else 'âŒ'}
- Data Sources: {'âœ…' if submission.has_data_sources else 'âŒ'}
- Real-World Application: {'âœ…' if submission.has_real_world_application else 'âŒ'}
- Technical Depth: {submission.technical_depth}

#### Feedback:
{submission.specific_feedback}

---

"""
    
    # Save report
    report_file = output_path / "showcase_review_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    return str(report_file)

def create_publication_content(featured_submissions: List[SubmissionAnalysis], output_path: Path):
    """Create publication-ready content for featured analyses."""
    
    # Newsletter content
    newsletter_content = {
        'featured_analyses': [],
        'community_highlights': [],
        'gallery_additions': []
    }
    
    for submission in featured_submissions:
        if submission.overall_score >= 90:
            # Tier 1 - Newsletter feature
            newsletter_content['featured_analyses'].append({
                'title': submission.title,
                'author': submission.author,
                'score': submission.overall_score,
                'summary': submission.body[:300] + '...' if len(submission.body) > 300 else submission.body,
                'url': submission.url,
                'strengths': submission.strengths[:3],  # Top 3 strengths
                'publication_type': 'Lead Feature'
            })
        elif submission.overall_score >= 80:
            # Tier 2 - Community highlights
            newsletter_content['community_highlights'].append({
                'title': submission.title,
                'author': submission.author,
                'score': submission.overall_score,
                'summary': submission.body[:150] + '...' if len(submission.body) > 150 else submission.body,
                'url': submission.url,
                'engagement': f"{submission.upvotes} upvotes, {submission.comments} comments"
            })
        elif submission.overall_score >= 70:
            # Tier 3 - Gallery
            newsletter_content['gallery_additions'].append({
                'title': submission.title,
                'author': submission.author,
                'score': submission.overall_score,
                'url': submission.url,
                'categories': submission.technical_depth
            })
    
    # Save newsletter content
    with open(output_path / "publication_content.json", 'w') as f:
        json.dump(newsletter_content, f, indent=2)
    
    # Create individual author feedback files
    feedback_dir = output_path / "author_feedback"
    feedback_dir.mkdir(exist_ok=True)
    
    for submission in featured_submissions:
        feedback_file = feedback_dir / f"{submission.author}_{submission.title.replace(' ', '_')[:50]}.md"
        
        feedback_content = f"""# Review Feedback: {submission.title}

## Overall Assessment
- **Quality Score**: {submission.overall_score:.1f}/100
- **Recommendation**: {submission.featured_tier}
- **Community Engagement**: {submission.upvotes} upvotes, {submission.comments} comments

## Detailed Feedback
{submission.specific_feedback}

## Next Steps
{submission.recommendation}

Thank you for your valuable contribution to the SocialMapper community!

---
*This review was generated on {datetime.now().strftime('%Y-%m-%d')} as part of our monthly showcase review process.*
"""
        
        with open(feedback_file, 'w', encoding='utf-8') as f:
            f.write(feedback_content)

def main():
    """Main script execution."""
    parser = argparse.ArgumentParser(description="Automate SocialMapper showcase reviews")
    parser.add_argument("--month", help="Review submissions for specific month")
    parser.add_argument("--days", type=int, default=30, help="Number of days to look back for submissions")
    parser.add_argument("--output-dir", default="showcase-review", help="Output directory for review materials")
    parser.add_argument("--repo", default="mihiarc/socialmapper", help="GitHub repository")
    parser.add_argument("--review-all", action="store_true", help="Review all submissions regardless of date")
    
    args = parser.parse_args()
    
    # Create output directory
    output_path = Path(args.output_dir)
    output_path.mkdir(exist_ok=True)
    
    days_back = 365 if args.review_all else args.days
    period_desc = "all time" if args.review_all else f"past {days_back} days"
    
    print(f"ðŸ” Collecting Show & Tell submissions from {period_desc}...")
    
    # Initialize collectors and analyzers
    github_collector = GitHubShowcaseCollector(args.repo)
    analyzer = ShowcaseAnalyzer()
    
    try:
        # Collect submissions
        submissions_data = github_collector.get_show_and_tell_submissions(days_back)
        print(f"ðŸ“¥ Found {len(submissions_data)} submissions to review")
        
        if not submissions_data:
            print("No submissions found for the specified period.")
            return
        
        # Analyze submissions
        print("ðŸ”¬ Analyzing submissions...")
        analyzed_submissions = []
        
        for submission_data in submissions_data:
            try:
                analysis = analyzer.analyze_submission(submission_data)
                analyzed_submissions.append(analysis)
                print(f"   âœ… Analyzed: {analysis.title[:50]}... (Score: {analysis.overall_score:.1f})")
            except Exception as e:
                print(f"   âŒ Error analyzing {submission_data.get('title', 'Unknown')}: {e}")
        
        # Save raw analysis data
        analysis_data = [asdict(submission) for submission in analyzed_submissions]
        with open(output_path / "submission_analyses.json", 'w') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        print(f"ðŸ’¾ Analysis data saved to {output_path}/submission_analyses.json")
        
        # Generate review report
        print("ðŸ“„ Generating review report...")
        report_path = generate_review_report(analyzed_submissions, output_path)
        print(f"ðŸ“‹ Review report created: {report_path}")
        
        # Create publication content
        print("ðŸŽ¯ Creating publication content...")
        featured_submissions = [s for s in analyzed_submissions if s.overall_score >= 70]
        create_publication_content(featured_submissions, output_path)
        print(f"ðŸ“° Publication content created in {output_path}/")
        
        # Summary statistics
        tier_1_count = len([s for s in analyzed_submissions if s.overall_score >= 90])
        tier_2_count = len([s for s in analyzed_submissions if s.overall_score >= 80 and s.overall_score < 90])
        tier_3_count = len([s for s in analyzed_submissions if s.overall_score >= 70 and s.overall_score < 80])
        
        print("\nðŸŽ‰ Review process complete!")
        print(f"\nðŸ“Š Results Summary:")
        print(f"- Total submissions analyzed: {len(analyzed_submissions)}")
        print(f"- Tier 1 (Major Showcase): {tier_1_count}")
        print(f"- Tier 2 (Community Highlights): {tier_2_count}")
        print(f"- Tier 3 (Showcase Gallery): {tier_3_count}")
        print(f"- Average quality score: {sum(s.overall_score for s in analyzed_submissions) / len(analyzed_submissions):.1f}/100")
        
        print(f"\nðŸ“ Generated files:")
        print(f"- Review report: {report_path}")
        print(f"- Analysis data: {output_path}/submission_analyses.json")
        print(f"- Publication content: {output_path}/publication_content.json")
        print(f"- Author feedback: {output_path}/author_feedback/")
        
        print(f"\nðŸ“‹ Next steps:")
        print("1. Review the generated report and analysis")
        print("2. Send individual feedback to authors")
        print("3. Update newsletter content with featured analyses")
        print("4. Update website showcase gallery")
        print("5. Contact featured authors about presentation opportunities")
        
    except Exception as e:
        print(f"âŒ Error during review process: {e}")
        raise

if __name__ == "__main__":
    main()