import requests

class EvalsClient:
    """
    A client for interacting with Enkrypt AI Evals API endpoints.
    """

    def __init__(self, api_key, base_url="https://api.enkryptai.com"):
        """
        Initializes the client.

        Parameters:
        - api_key (str): Your API key for authenticating with the service.
        - base_url (str): Base URL of the API (default: "https://api.enkryptai.com").
        """
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()

    def _request(self, method, endpoint, headers=None, **kwargs):
        """
        Internal helper to send an HTTP request.

        Automatically adds the API key to headers.
        """
        url = self.base_url + endpoint
        headers = headers or {}
        if 'apikey' not in headers:
            headers['apikey'] = self.api_key
            
        try:
            response = self.session.request(method, url, headers=headers, **kwargs)
            response.raise_for_status()
            return response.json()
        
        except Exception as e:
            # print(e)
            return {"error": str(e)}

    # ----------------------------
    # Basic Evals Endpoints
    # ----------------------------

    def check_adherence(self, llm_answer, context):
        """
        Checks if the LLM's answer adheres to the provided context.

        Parameters:
        - llm_answer (str): The response generated by the LLM
        - context (str): The context against which to check the answer

        Returns:
        - JSON response from the API containing adherence analysis
        """
        payload = {
            "llm_answer": llm_answer,
            "context": context
        }
        try:
            return self._request("POST", "/guardrails/adherence", json=payload)
        except Exception as e:
            # print(e)
            return {"error": str(e)}

    def check_relevancy(self, question, llm_answer):
        """
        Checks if the LLM's answer is relevant to the asked question.

        Parameters:
        - question (str): The original question asked
        - llm_answer (str): The response generated by the LLM

        Returns:
        - JSON response from the API containing relevancy analysis
        """
        payload = {
            "question": question,
            "llm_answer": llm_answer
        }
        try:
            return self._request("POST", "/guardrails/relevancy", json=payload)
        except Exception as e:
            # print(e)
            return {"error": str(e)}