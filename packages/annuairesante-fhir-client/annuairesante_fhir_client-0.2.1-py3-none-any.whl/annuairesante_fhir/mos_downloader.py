"""T√©l√©chargeur et gestionnaire de r√©f√©rentiels MOS/NOS depuis esante.gouv.fr."""

import json
import os
import re
from datetime import datetime
from email.utils import parsedate_to_datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urljoin

import httpx
from bs4 import BeautifulSoup


class MOSDownloader:
    """
    T√©l√©chargeur de r√©f√©rentiels MOS/NOS depuis https://mos.esante.gouv.fr/NOS/

    G√®re le t√©l√©chargement incr√©mental bas√© sur les dates de modification.
    """

    BASE_URL = "https://mos.esante.gouv.fr/NOS/"

    def __init__(self, cache_dir: Optional[str] = None):
        """
        Args:
            cache_dir: R√©pertoire de cache (d√©faut: $ANNUAIRE_SANTE_CACHE_DIR/mos ou ~/.annuairesante_cache/mos/)
        """
        if cache_dir is None:
            # V√©rifier la variable d'environnement
            env_cache = os.getenv("ANNUAIRE_SANTE_CACHE_DIR")
            if env_cache:
                cache_dir = Path(env_cache) / "mos"
            else:
                cache_dir = Path.home() / ".annuairesante_cache" / "mos"

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Fichier de m√©tadonn√©es pour tracker les dates de mise √† jour
        self.metadata_file = self.cache_dir / "metadata.json"
        self.metadata = self._load_metadata()

        # Statistiques de t√©l√©chargement
        self.stats = {"downloaded": 0, "skipped": 0, "errors": 0, "total": 0}

    def _load_metadata(self) -> Dict:
        """Charge les m√©tadonn√©es de t√©l√©chargement."""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
        return {"last_full_sync": None, "terminologies": {}}

    def _save_metadata(self):
        """Sauvegarde les m√©tadonn√©es."""
        try:
            with open(self.metadata_file, "w", encoding="utf-8") as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur sauvegarde m√©tadonn√©es: {e}")

    def list_terminologies(self, force_refresh: bool = False) -> List[Dict[str, str]]:
        """
        Liste tous les r√©f√©rentiels disponibles (TRE_*, JDV_*, ASS_*).

        Args:
            force_refresh: Force le rafra√Æchissement de la liste

        Returns:
            Liste de dicts avec 'name', 'url', 'last_modified'
        """
        print("üìã R√©cup√©ration de la liste des r√©f√©rentiels...")

        try:
            response = httpx.get(self.BASE_URL, timeout=30.0, follow_redirects=True)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            terminologies = []

            for link in soup.find_all("a"):
                href = link.get("href", "")

                # Filtrer les r√©f√©rentiels TRE, JDV et ASS
                if any(href.startswith(prefix) for prefix in ["TRE_", "JDV_", "ASS_"]):
                    name = href.rstrip("/")
                    url = urljoin(self.BASE_URL, href)

                    # Extraire la date de modification si disponible
                    parent = link.parent
                    last_modified = None
                    if parent and parent.find_next_sibling():
                        date_text = parent.find_next_sibling().get_text(strip=True)
                        last_modified = self._parse_date(date_text)

                    terminologies.append({"name": name, "url": url, "last_modified": last_modified})

            print(f"‚úÖ {len(terminologies)} r√©f√©rentiels trouv√©s")
            return terminologies

        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration de la liste: {e}")
            return []

    def _parse_date(self, date_str: str) -> Optional[str]:
        """Parse une date du format Apache directory listing."""
        try:
            # Format: "2024-01-15 10:30"
            dt = datetime.strptime(date_str.strip(), "%Y-%m-%d %H:%M")
            return dt.isoformat()
        except Exception:
            return None

    def download_terminology(self, name: str, force: bool = False) -> bool:
        """
        T√©l√©charge un r√©f√©rentiel sp√©cifique.

        Args:
            name: Nom du r√©f√©rentiel (ex: "TRE_R48-DiplomeEtatFrancais")
            force: Force le t√©l√©chargement m√™me si d√©j√† √† jour

        Returns:
            True si t√©l√©charg√©, False si skipp√© ou erreur
        """
        # Construire l'URL du fichier JSON FHIR
        # Pattern: /NOS/{name}/FHIR/{normalized-name}/{name}-FHIR.json
        # Ex: TRE_R48-DiplomeEtatFrancais ‚Üí TRE-R48-DiplomeEtatFrancais
        normalized_name = name.replace("_", "-")

        base_url = urljoin(self.BASE_URL, f"{name}/FHIR/{normalized_name}/")
        json_url = urljoin(base_url, f"{name}-FHIR.json")

        # V√©rifier si mise √† jour n√©cessaire
        if not force and name in self.metadata["terminologies"]:
            # R√©cup√©rer la date de derni√®re modification sur le serveur
            try:
                head_response = httpx.head(json_url, timeout=10.0, follow_redirects=True)
                server_last_modified = head_response.headers.get("Last-Modified")

                if server_last_modified:
                    # Parser la date du serveur (format HTTP-date)
                    server_date = parsedate_to_datetime(server_last_modified)

                    # Comparer avec la date de t√©l√©chargement local
                    local_date_str = self.metadata["terminologies"][name].get("downloaded_at")
                    if local_date_str:
                        local_date = datetime.fromisoformat(local_date_str)

                        # Si le serveur n'a pas de version plus r√©cente, skip
                        if server_date.replace(tzinfo=None) <= local_date:
                            print(
                                f"‚è≠Ô∏è  {name}: √† jour (serveur: {server_date.strftime('%Y-%m-%d %H:%M')})"
                            )
                            self.stats["skipped"] += 1
                            return False
                        else:
                            print(
                                f"üîÑ {name}: mise √† jour disponible (serveur: {server_date.strftime('%Y-%m-%d %H:%M')})"
                            )
                    else:
                        print(f"‚è≠Ô∏è  {name}: d√©j√† en cache")
                        self.stats["skipped"] += 1
                        return False
                else:
                    # Pas de Last-Modified header, v√©rifier juste l'existence
                    print(f"‚è≠Ô∏è  {name}: d√©j√† en cache (pas de date serveur)")
                    self.stats["skipped"] += 1
                    return False

            except Exception as e:
                # En cas d'erreur HEAD, continuer avec le t√©l√©chargement
                print(f"‚ö†Ô∏è  {name}: impossible de v√©rifier la date ({e}), t√©l√©chargement...")

        try:
            # T√©l√©charger le fichier JSON FHIR
            print(f"‚¨áÔ∏è  T√©l√©chargement de {name}...")
            response = httpx.get(json_url, timeout=30.0, follow_redirects=True)
            response.raise_for_status()

            # Sauvegarder le fichier
            output_file = self.cache_dir / f"{name}-FHIR.json"
            output_file.write_bytes(response.content)

            # R√©cup√©rer la date de modification du serveur
            server_last_modified = response.headers.get("Last-Modified")
            server_date_iso = None
            if server_last_modified:
                try:
                    server_date = parsedate_to_datetime(server_last_modified)
                    server_date_iso = server_date.isoformat()
                except Exception:
                    pass

            # Mettre √† jour les m√©tadonn√©es
            self.metadata["terminologies"][name] = {
                "downloaded_at": datetime.now().isoformat(),
                "server_last_modified": server_date_iso,
                "file": str(output_file),
                "size": len(response.content),
            }
            self._save_metadata()

            print(f"‚úÖ {name}: t√©l√©charg√© ({len(response.content)} octets)")
            self.stats["downloaded"] += 1
            return True

        except Exception as e:
            print(f"‚ùå {name}: erreur - {e}")
            self.stats["errors"] += 1
            return False

    def parse_tabs_file(self, terminology_name: str) -> List[Dict[str, str]]:
        """
        Parse un fichier JSON FHIR et retourne les entr√©es.

        Args:
            terminology_name: Nom du r√©f√©rentiel

        Returns:
            Liste de dicts avec 'code', 'display', etc.
        """
        json_file = self.cache_dir / f"{terminology_name}-FHIR.json"

        if not json_file.exists():
            raise FileNotFoundError(f"Fichier non trouv√©: {json_file}")

        entries = []

        try:
            with open(json_file, encoding="utf-8") as f:
                data = json.load(f)

            resource_type = data.get("resourceType")

            # CodeSystem (TRE) : concept[] directement √† la racine
            if resource_type == "CodeSystem":
                concepts = data.get("concept", [])
                for concept in concepts:
                    entry = {
                        "code": concept.get("code"),
                        "display": concept.get("display"),
                        "definition": concept.get("definition"),
                        "property": concept.get("property", []),
                    }
                    entries.append(entry)

            # ValueSet (JDV, ASS) : compose.include[].concept[]
            elif resource_type == "ValueSet":
                compose = data.get("compose", {})
                includes = compose.get("include", [])

                for include in includes:
                    concepts = include.get("concept", [])
                    for concept in concepts:
                        entry = {
                            "code": concept.get("code"),
                            "display": concept.get("display"),
                            "designation": concept.get("designation", []),
                        }
                        entries.append(entry)

            return entries

        except Exception as e:
            print(f"‚ùå Erreur parsing {terminology_name}: {e}")
            return []

    def parse_fhir_json(self, terminology_name: str) -> List[Dict[str, str]]:
        """
        Alias pour parse_tabs_file (pour compatibilit√©).
        Parse un fichier JSON FHIR.
        """
        return self.parse_tabs_file(terminology_name)

    def download_all(
        self,
        force: bool = False,
        include_patterns: Optional[List[str]] = None,
        exclude_patterns: Optional[List[str]] = None,
    ) -> Dict[str, int]:
        """
        T√©l√©charge tous les r√©f√©rentiels.

        Args:
            force: Force le t√©l√©chargement m√™me si d√©j√† √† jour
            include_patterns: Liste de patterns √† inclure (ex: ["TRE_R*", "JDV_J1*"])
            exclude_patterns: Liste de patterns √† exclure

        Returns:
            Statistiques de t√©l√©chargement
        """
        print("=" * 70)
        print("üöÄ T√âL√âCHARGEMENT DES R√âF√âRENTIELS MOS/NOS")
        print("=" * 70)

        # R√©initialiser les stats
        self.stats = {"downloaded": 0, "skipped": 0, "errors": 0, "total": 0}

        # Lister les r√©f√©rentiels
        terminologies = self.list_terminologies()

        # Filtrer selon les patterns
        if include_patterns or exclude_patterns:
            terminologies = self._filter_terminologies(
                terminologies, include_patterns, exclude_patterns
            )

        self.stats["total"] = len(terminologies)

        print(f"\nüì¶ {len(terminologies)} r√©f√©rentiels √† traiter\n")

        # T√©l√©charger chaque r√©f√©rentiel
        for i, term in enumerate(terminologies, 1):
            print(f"[{i}/{len(terminologies)}] ", end="")
            self.download_terminology(term["name"], force=force)

        # Mettre √† jour la date de synchronisation compl√®te
        self.metadata["last_full_sync"] = datetime.now().isoformat()
        self._save_metadata()

        # Afficher le r√©sum√©
        print("\n" + "=" * 70)
        print("üìä R√âSUM√â")
        print("=" * 70)
        print(f"Total:        {self.stats['total']}")
        print(f"T√©l√©charg√©s:  {self.stats['downloaded']}")
        print(f"Skipp√©s:      {self.stats['skipped']}")
        print(f"Erreurs:      {self.stats['errors']}")
        print("=" * 70)

        return self.stats

    def _filter_terminologies(
        self,
        terminologies: List[Dict],
        include_patterns: Optional[List[str]],
        exclude_patterns: Optional[List[str]],
    ) -> List[Dict]:
        """Filtre les r√©f√©rentiels selon les patterns."""
        filtered = terminologies

        if include_patterns:
            filtered = [
                t
                for t in filtered
                if any(self._match_pattern(t["name"], p) for p in include_patterns)
            ]

        if exclude_patterns:
            filtered = [
                t
                for t in filtered
                if not any(self._match_pattern(t["name"], p) for p in exclude_patterns)
            ]

        return filtered

    def _match_pattern(self, name: str, pattern: str) -> bool:
        """V√©rifie si un nom correspond √† un pattern (avec wildcards)."""
        # Convertir le pattern en regex
        regex_pattern = pattern.replace("*", ".*").replace("?", ".")
        return bool(re.match(f"^{regex_pattern}$", name))

    def build_lookup_index(self) -> Dict[str, Dict[str, str]]:
        """
        Construit un index de recherche rapide pour tous les r√©f√©rentiels t√©l√©charg√©s.

        Returns:
            Dict[table_name, Dict[code, display]]
        """
        print("üî® Construction de l'index de recherche...")

        index = {}

        for term_name in self.metadata["terminologies"].keys():
            try:
                entries = self.parse_fhir_json(term_name)

                # Extraire le nom de table normalis√©
                table_name = self._extract_table_name(term_name)

                if table_name:
                    index[table_name] = {}

                    for entry in entries:
                        code = entry.get("code")
                        display = entry.get("display")

                        if code and display:
                            index[table_name][code] = display

                    print(f"  ‚úÖ {table_name}: {len(index[table_name])} codes")

            except Exception as e:
                print(f"  ‚ö†Ô∏è  {term_name}: {e}")

        # Sauvegarder l'index
        index_file = self.cache_dir / "lookup_index.json"
        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(index, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ Index sauvegard√©: {index_file}")
        print(f"üìä {len(index)} tables index√©es")

        return index

    def _extract_table_name(self, terminology_name: str) -> Optional[str]:
        """Extrait le nom de table normalis√©."""
        # TRE_R48-DiplomeEtatFrancais ‚Üí TRE-R48-DiplomeEtatFrancais
        # JDV_J01-XdsAuthorSpecialty-CISIS ‚Üí JDV-J01-XdsAuthorSpecialty-CISIS
        # ASS_A11-CorresModeleCDA ‚Üí ASS-A11-CorresModeleCDA

        match = re.match(r"^(TRE|JDV|ASS)_([A-Z]\d+)", terminology_name)
        if match:
            return f"{match.group(1)}-{match.group(2)}"

        return None

    def get_stats(self) -> Dict:
        """Retourne les statistiques de cache."""
        total_size = sum(
            Path(meta["file"]).stat().st_size
            for meta in self.metadata["terminologies"].values()
            if Path(meta["file"]).exists()
        )

        return {
            "terminologies_count": len(self.metadata["terminologies"]),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "last_sync": self.metadata.get("last_full_sync"),
            "cache_dir": str(self.cache_dir),
        }

    def check_updates(
        self,
        include_patterns: Optional[List[str]] = None,
        exclude_patterns: Optional[List[str]] = None,
    ) -> List[Dict[str, str]]:
        """
        V√©rifie les mises √† jour disponibles sans t√©l√©charger.

        Args:
            include_patterns: Patterns √† inclure
            exclude_patterns: Patterns √† exclure

        Returns:
            Liste des r√©f√©rentiels avec mises √† jour disponibles
        """
        print("üîç V√©rification des mises √† jour disponibles...\n")

        updates_available = []
        checked = 0

        for name, meta in self.metadata["terminologies"].items():
            # Filtrer selon les patterns
            if include_patterns and not any(self._match_pattern(name, p) for p in include_patterns):
                continue
            if exclude_patterns and any(self._match_pattern(name, p) for p in exclude_patterns):
                continue

            checked += 1

            try:
                # Construire l'URL du fichier JSON FHIR
                normalized_name = name.replace("_", "-")
                base_url = urljoin(self.BASE_URL, f"{name}/FHIR/{normalized_name}/")
                json_url = urljoin(base_url, f"{name}-FHIR.json")

                # HEAD request pour obtenir la date
                head_response = httpx.head(json_url, timeout=10.0, follow_redirects=True)
                server_last_modified = head_response.headers.get("Last-Modified")

                if server_last_modified:
                    server_date = parsedate_to_datetime(server_last_modified)
                    local_date_str = meta.get("downloaded_at")

                    if local_date_str:
                        local_date = datetime.fromisoformat(local_date_str)

                        if server_date.replace(tzinfo=None) > local_date:
                            updates_available.append(
                                {
                                    "name": name,
                                    "local_date": local_date.strftime("%Y-%m-%d %H:%M"),
                                    "server_date": server_date.strftime("%Y-%m-%d %H:%M"),
                                    "age_days": (datetime.now() - local_date).days,
                                }
                            )
                            print(f"üîÑ {name}: mise √† jour disponible")
                            print(f"   Local:  {local_date.strftime('%Y-%m-%d %H:%M')}")
                            print(f"   Serveur: {server_date.strftime('%Y-%m-%d %H:%M')}\n")
                        else:
                            print(f"‚úÖ {name}: √† jour")

            except Exception as e:
                print(f"‚ö†Ô∏è  {name}: erreur - {e}")

        print("\nüìä R√©sum√©:")
        print(f"   V√©rifi√©s: {checked}")
        print(f"   Mises √† jour disponibles: {len(updates_available)}")

        return updates_available


def download_mos_terminologies(
    force: bool = False, include: Optional[List[str]] = None, exclude: Optional[List[str]] = None
) -> Dict[str, int]:
    """
    Fonction helper pour t√©l√©charger les r√©f√©rentiels MOS/NOS.

    Args:
        force: Force le t√©l√©chargement m√™me si d√©j√† √† jour
        include: Patterns √† inclure (ex: ["TRE_R*"])
        exclude: Patterns √† exclure

    Returns:
        Statistiques de t√©l√©chargement

    Example:
        >>> from annuairesante_fhir.mos_downloader import download_mos_terminologies
        >>>
        >>> # T√©l√©charger tous les TRE
        >>> stats = download_mos_terminologies(include=["TRE_*"])
        >>>
        >>> # T√©l√©charger tous sauf les ASS
        >>> stats = download_mos_terminologies(exclude=["ASS_*"])
    """
    downloader = MOSDownloader()
    stats = downloader.download_all(force=force, include_patterns=include, exclude_patterns=exclude)

    # Construire l'index apr√®s t√©l√©chargement
    if stats["downloaded"] > 0:
        downloader.build_lookup_index()

    return stats
