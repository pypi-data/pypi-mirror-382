[core]
tracker = ["console", "api"]

[run]
environments = ['local']
# standalone config
fix_env_on_osx = True

# heartbeat timeout - 2 hours
heartbeat_timeout_s = 7200
heartbeat_interval_s = 5
heartbeat_sender_log_to_file = True


[tracking]
track_source_code = True

[run_log]
# this is common tracking and orchestration sections
# Logging level
level = INFO

# Logging format
formatter = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
formatter_simple = %%(asctime)s %%(levelname)s - %%(message)s
formatter_colorlog = [%%(asctime)s] %%(log_color)s%%(levelname)s %%(reset)s %%(task)-15s - %%(message)s

console_formatter_name = formatter_colorlog
file_formatter_name = formatter

sentry_url =

at_warn = azure.storage,flask_appbuilder


[local_machine_engine]
_type = dbnd_run.run_settings.engine.LocalMachineEngineConfig


[output]
path_task = {root}{sep}{env_label}{sep}{task_target_date}{sep}{task_name}{sep}{task_name}{task_class_version}_{task_signature}{sep}{output_name}{output_ext}
path_prod_immutable_task = {root}{sep}production{sep}{task_name}{task_class_version}{sep}{output_name}{output_ext}{sep}date={task_target_date}

target = csv
str = txt
object = pickle
List[object] = pickle
List[str] = csv
Dict[Any,DataFrame] = pickle
pandas_dataframe = csv
tensorflow_model = tfmodel
tensorflow_history = tfhistory

pandas_df_dict = hdf5
numpy_ndarray = numpy
matplotlib_figure = png
spark_dataframe = csv

hdf_format = fixed

validate_no_extra_params = disabled


[scheduler]
default_retries = 3
refresh_interval = 10
active_by_default = True
shell_cmd = True


[airflow]
enable_dbnd_context_vars = True
enable_windows_support = False

auto_add_versioned_dags = True
auto_add_scheduled_dags = True
auto_disable_scheduled_dags_load = True

optimize_airflow_db_access = True
disable_db_ping_on_connect = True
disable_dag_concurrency_rules = True
dbnd_pool = dbnd_pool

dbnd_dag_concurrency = 100000

webserver_url = http://localhost:8082

use_connections = False


# engines configuration


[docker]
_type = dbnd_docker.docker.docker_engine_config.DockerEngineConfig
network =
sql_alchemy_conn =


[kubernetes]
_type = dbnd_docker.kubernetes.kubernetes_engine_config.KubernetesEngineConfig

pod_error_cfg_source_dict = {
                            "255": {"retry_count": 3, "retry_delay": "3m"},
                            "err_image_pull": {"retry_count": 0, "retry_delay": "3m"},
                            }

submit_termination_grace_period = 30s

# environment configurations
[local]
root = ${DBND_HOME}/data
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = spark_local

[gcp]
_type = dbnd_gcp.env.GcpEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd

conn_id = google_cloud_default

spark_engine = dataproc

[aws]
_type = dbnd_aws.env.AwsEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = emr
docker_engine = aws_batch

[azure]
_type = dbnd_azure.env.AzureCloudConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd


# spark configurations
[spark]
_type = dbnd_spark.spark_config.SparkConfig

[livy]
_type = livy

[spark_local]
_type = dbnd_spark.local.local_spark_config.SparkLocalEngineConfig
conn_id = spark_default

[dataproc]
_type = dbnd_gcp.dataproc.dataproc_config.DataprocConfig

[databricks]
_type = dbnd_databricks.databricks_config.DatabricksConfig
conn_id = databricks_default


[qubole]
_type = dbnd_qubole.qubole_config.QuboleConfig

[databricks_azure]
local_dbfs_mount = /mnt/dbnd/

[emr]
_type = emr

