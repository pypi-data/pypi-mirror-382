"""Functions to load the data and parse the annotation file."""

import re
from dataclasses import dataclass

import numpy as np
import pandas as pd
from tqdm import tqdm

import omniplate.admin as admin
import omniplate.clogger as clogger
import omniplate.omerrors as errors
import omniplate.omgenutils as gu
from omniplate.parse_plate import parse_plate


@clogger.log
def load(
    self,
    d_names,
    a_names=None,
    experiment_type=None,
    plate_reader_type="Tecan",
    d_sheets=False,
    a_sheets=False,
    info=True,
    combine=False,
    combine_name=None,
    min_dt=0.05,
    plate_type=96,
):
    """
    Load raw data files.

    Two files are required: one generated by the plate reader and the other
    the corresponding annotation files - both assumed to be xlsx.

    Parameters
    ----------
    dnames: string or list of strings, optional
        The name of the file containing the data from the plate reader
        or a list of file names.
    anames: string or list of strings, optional
        The name of file containing the corresponding annotation or a list
        of file names.
    experimenttype: string or list of strings, optional
        If specified, creates a new experiment_type column in each
        dataframe.
    platereadertype: string or list of str
        The type of plate reader, currently either 'Tecan' or 'Sunrise'
        or 'tidy'. If you have data from different plate readers, say three,
        use a list, such as ["Tecan", "Tecan", "Sunrise"].
    dsheets: list of integers or strings, optional
        The relevant sheets of the Excel files storing the data.
    asheets: list of integers or strings, optional
        The relevant sheets of the corresponding Excel files for the
        annotation.
    info: boolean
        If True (default), display summary information on the data once
        loaded.
    combine: boolean
        If True (default is False), combine all the data loaded into one
        single experiment.
    combinename: str (optional)
        If specified, the name given to the combined experiment.
    min_dt: float, optional
        The minimum time interval taken by the plate reader between
        measurements. Used only when combine=True.
    platetype: int, optional
        The total number of wells in the plate, either 96 (default), 48,
        or 24.

    Examples
    -------
    >>> p.load('Data.xlsx', 'DataContents.xlsx')
    >>> p.load('Data.xlsx', 'DataContents.xlsx', info= False)
    >>> p.load(['HxtA.xls', 'HxtB.xlsx'], ['HxtAcontents.xlsx',
            'HxtBcontents.xlsx'], combine=True)
    >>> p.load(['HxtG.xls', 'HxtP.xlsx'],
            ['HxtGcontents.xlsx', 'HxtPcontents.xlsx'],
            ["glucose_pregrowth", "pyruvate_pregrowth"])
    """
    config = run_load_checks(
        d_names=d_names,
        a_names=a_names,
        experiment_type=experiment_type,
        plate_reader_type=plate_reader_type,
        d_sheets=d_sheets,
        a_sheets=a_sheets,
    )
    all_data = {}
    for i, (d_name, plate_reader) in enumerate(
        zip(config.d_names, config.plate_reader_type)
    ):
        # get dataframe for raw data
        res = load_data_files(
            plate_reader_type=plate_reader,
            datadir_path=self.datadir_path,
            d_name=d_name,
            d_sheet=config.d_sheets[i],
            a_name=config.a_names[i],
            a_sheet=config.a_sheets[i],
            plate_type=plate_type,
        )
        self.all_experiments.append(res.experiment)
        self.all_conditions[res.experiment] = res.all_conditions
        self.all_strains[res.experiment] = res.all_strains
        self.data_types[res.experiment] = res.data_types
        self.all_strains_conditions[res.experiment] = list(
            (res.rdf.strain + " in " + res.rdf.condition).dropna().unique()
        )
        all_data.update(res.all_data)
        # make r dataframe of raw data
        self.r = (
            pd.merge(self.r, res.rdf, how="outer")
            if hasattr(self, "r")
            else res.rdf
        )
        # update progress dictionary
        admin.initialise_progress(self, res.experiment)
    sort_attributes(self)
    # make sc dataframe for summary stats and corrections
    all_dfs = []
    for exp in all_data:
        strs, cons = [], []
        for cs in all_data[exp]:
            strs.append(cs.split(" in ")[0])
            cons.append(cs.split(" in ")[1])
        corrdict = {"experiment": exp, "strain": strs, "condition": cons}
        corrdict.update(
            {dtype + "_measured": True for dtype in self.data_types[exp]}
        )
        all_dfs.append(pd.DataFrame(corrdict))
    self.sc = pd.concat(all_dfs)
    self.sc = self.sc.reset_index(drop=True)
    # replace NaN with False for data_types_measured
    for column in self.sc.columns[self.sc.columns.str.contains("_measured")]:
        self.sc[column] = (
            self.sc[column].fillna(False).infer_objects(copy=False)
        )
    # make backup dataframe of original data
    self.origr = self.r.copy()
    # make dataframe for well contents
    self.wellsdf = admin.make_wells_df(self.r)
    # make s dataframe for summary data
    self.s = admin.make_s(self)
    # add experiment_type
    if experiment_type is not None:
        for df in [self.r, self.s, self.sc]:
            df["experiment_type"] = df.experiment.map(
                config.experiment_type_dict
            )
    # display info on experiment, conditions and strains
    if info:
        self.info
    print('\nWarning: wells with no strains have been changed to "Null".')
    # combine into one experiment
    if combine:
        if combine_name is not None and isinstance(combine_name, str):
            self.combined = combine_name
        combine_experiments(self, min_dt=min_dt)


def run_load_checks(
    d_names,
    a_names,
    experiment_type,
    plate_reader_type,
    d_sheets,
    a_sheets,
):
    """Fix inputs to loaddata."""

    @dataclass
    class LoadConfig:
        """Configuration data for loading experiments."""

        d_names: list[str]
        a_names: list[str]
        plate_reader_type: list[str]
        experiment_type_dict: dict[str, str] | None
        d_sheets: list[int]
        a_sheets: list[int]

    d_names = gu.make_list(d_names)
    if a_names is None:
        a_names = [
            d_name.split(".")[0] + "Contents.xlsx" for d_name in d_names
        ]
    else:
        a_names = gu.make_list(a_names)
    if experiment_type is not None:
        experiment_type = gu.make_list(experiment_type)
        experiment_type_dict = {
            d_name.split(".")[0]: e_type
            for d_name, e_type in zip(d_names, experiment_type)
        }
    else:
        experiment_type_dict = None
    if isinstance(plate_reader_type, str):
        plate_reader_type = [plate_reader_type for d_name in d_names]
    if not d_sheets:
        d_sheets = [0 for d_name in d_names]
    if not a_sheets:
        a_sheets = [0 for d_name in d_names]
    return LoadConfig(
        d_names=d_names,
        a_names=a_names,
        plate_reader_type=plate_reader_type,
        experiment_type_dict=experiment_type_dict,
        d_sheets=d_sheets,
        a_sheets=a_sheets,
    )


def load_data_files(
    plate_reader_type,
    datadir_path,
    d_name,
    d_sheet,
    a_name,
    a_sheet,
    plate_type,
):
    """Call functions to parse data and metadate from the input files."""

    @dataclass
    class res:
        rdf: pd.DataFrame
        all_conditions: list[str]
        all_strains: list[str]
        all_data: dict[str, str]
        experiment: str
        data_types: list[str]

    experiment = d_name.split(".")[0]
    # import and parse plate contents file
    well_analysis = analyse_contents_of_wells(
        datadir_path=datadir_path,
        experiment=experiment,
        a_name=a_name,
        a_sheet_number=a_sheet,
        plate_type=plate_type,
    )
    # import and parse data file created by plate reader
    try:
        print("Loading", d_name)
        rdf = parse_plate(d_name, plate_reader_type, datadir_path, d_sheet)
    except FileNotFoundError as exc:
        raise errors.FileNotFound(str(datadir_path / d_name)) from exc
    # get data_types
    cols = rdf.columns.to_list()
    data_types = [col for col in cols if col != "time" and col != "well"]
    # add condition and strain to dataframe
    rdf["experiment"] = experiment
    rdf["condition"] = rdf["well"].map(
        {
            well: well_analysis.well_contents[well][0]
            for well in well_analysis.well_contents
        }
    )
    rdf["strain"] = rdf["well"].map(
        {
            well: well_analysis.well_contents[well][1]
            for well in well_analysis.well_contents
        }
    )
    # drop wells with condition=strain=None
    rdf = rdf.drop(
        rdf[rdf.condition.isnull() & rdf.strain.isnull()].index
    ).reset_index(drop=True)
    return res(
        rdf=rdf,
        all_conditions=well_analysis.all_conditions,
        all_strains=well_analysis.all_strains,
        all_data=well_analysis.all_data,
        experiment=experiment,
        data_types=data_types,
    )


def analyse_contents_of_wells(
    datadir_path, experiment, a_name, a_sheet_number, plate_type
):
    """
    Load and parse ContentsofWells file.

    Return wellcontents, a dictionary with the contents of each well indexed
    by well, and alldata, a dictionary describing the contents of each well
    indexed by experiment.
    """

    @dataclass
    class WellAnalysis:
        """Results from analyzing well contents."""

        all_conditions: list[str]
        all_strains: list[str]
        all_data: dict[str, list[str]]
        well_contents: dict[str, list[str | None]]

    try:
        x_range, y_range = get_well_ranges(plate_type)
        all_data = {experiment: []}
        # import contents of the wells
        anno = pd.read_excel(
            str(datadir_path / a_name), index_col=0, sheet_name=a_sheet_number
        )
        well_contents = {}
        # run through and parse content of each well
        for x in x_range:
            for y in y_range:
                well = y + str(x)
                if (
                    isinstance(anno[x][y], str)
                    and anno[x][y] != "contaminated"
                ):
                    s, c = anno[x][y].split(" in ")
                    # standardise naming of wells with no strains
                    s = re.sub("(null|NULL)", "Null", s)
                    well_contents[well] = [c.strip(), s.strip()]
                    all_data[experiment].append(
                        well_contents[well][1]
                        + " in "
                        + well_contents[well][0]
                    )
                else:
                    well_contents[well] = [None, None]
        # create summary descriptions of the well contents
        all_data[experiment] = list(np.unique(all_data[experiment]))
        all_conditions = list(
            set(
                [
                    well_contents[well][0]
                    for well in well_contents
                    if well_contents[well][0] is not None
                ]
            )
        )
        all_strains = list(
            set(
                [
                    well_contents[well][1]
                    for well in well_contents
                    if well_contents[well][0] is not None
                ]
            )
        )
        return WellAnalysis(
            all_conditions=all_conditions,
            all_strains=all_strains,
            all_data=all_data,
            well_contents=well_contents,
        )
    except FileNotFoundError as exc:
        raise errors.FileNotFound(str(datadir_path / a_name)) from exc


def get_well_ranges(plate_type):
    """Find x and y ranges for a given number of wells in the plate."""
    if plate_type == 96:
        x_range = np.arange(1, 13)
        y_range = "ABCDEFGH"
    elif plate_type == 48:
        x_range = np.arange(1, 9)
        y_range = "ABCDEF"
    elif plate_type == 24:
        x_range = np.arange(1, 7)
        y_range = "ABCD"
    else:
        raise ValueError(f"A plate with {plate_type} wells is not expected.")
    return x_range, y_range


def combine_experiments(self, min_dt, dt=None):
    """
    Combine raw data from all loaded experiments into one experiment.

    Only to be run immediately after first loading the data.

    Wells are relabelled with a prefix denoting their original
    experiment, and p.experiment_map gives the mapping back to the
    original experiment names.
    """
    print("Combining experiments...")
    # to make new r dataframe
    rdict = {col: [] for col in list(self.r.columns)}
    # find data types present in all experiments
    data_types = self.r.select_dtypes(include="number").columns.tolist()
    df_indices = self.r.select_dtypes(include=["object"]).columns.tolist()
    # define a new universal time
    data_types.remove("time")
    unique_t = np.sort(self.r.time.unique())
    if dt is None:
        ut_values, ut_counts = np.unique(
            np.round(np.diff(unique_t), 2), return_counts=True
        )
        dt = ut_values[ut_values > min_dt][
            np.argmax(ut_counts[ut_values > min_dt])
        ]
    print(f" Using dt = {dt:.2f}.")
    nt = np.arange(0, unique_t.max(), dt)
    # find index for all wells
    wells_index = list(self.r.groupby(df_indices).mean().index)
    # interpolate data for each well to new universal time
    print(" Interpolating data to a universal time.")
    for wi in tqdm(wells_index):
        df = self.r.query(
            " and ".join(
                [
                    f"{column} == '{value}'"
                    for column, value in zip(df_indices, wi)
                ]
            )
        )
        # local universal time depending on length of experiments
        l_nt = nt[
            np.argmin((df.time.min() - nt) ** 2) : np.argmin(
                (df.time.max() - nt) ** 2
            )
            + 1
        ]
        x = df.time.values
        xs = np.sort(x)
        # interpolate data for this well to universal time
        new_y = {}
        for data_type in data_types:
            try:
                y = df[data_type].values
                ys = y[np.argsort(x)]
                new_y[data_type] = np.interp(l_nt, xs, ys)
            except KeyError:
                new_y[data_type] = np.nan * np.ones(l_nt.shape)
        # add to dict
        for i in range(l_nt.size):
            for column, value in zip(df_indices, wi):
                rdict[column].append(value)
            rdict["time"].append(l_nt[i])
            for data_type in data_types:
                rdict[data_type].append(new_y[data_type][i])
    # re-make r dataframe, now with universal time
    self.r = pd.DataFrame(data=rdict)
    # make dict mapping experiment number to original name
    experiment_map = {
        expt: str(i) for i, expt in enumerate(self.r.experiment.unique())
    }
    # rename wells
    self.r.well = (
        self.r.experiment.astype("string").replace(experiment_map)
        + "_"
        + self.r.well
    )
    self.r["well"] = self.r["well"].astype("object")
    # save names of original experiments
    self.r["original_experiment"] = self.r.experiment.copy()
    self.r["experiment_id"] = (
        self.r.experiment.astype("string").replace(experiment_map).copy()
    )
    self.r["experiment_id"] = self.r["experiment_id"].astype("object")
    # write over r.experiment and sort
    self.r.experiment = self.combined
    self.r = self.r.sort_values(by=["time", "well"]).reset_index(drop=True)
    # remake summary attributes
    self.all_experiments = [self.combined]
    for mattr in [
        "all_conditions",
        "all_strains",
        "all_strains_conditions",
        "data_types",
    ]:
        uniques = set(
            [
                lattr
                for e in getattr(self, mattr)
                for lattr in getattr(self, mattr)[e]
            ]
        )
        setattr(self, mattr, {self.combined: list(uniques)})
    sort_attributes(self)
    # reset self.progress
    for key in self.progress:
        self.progress[key] = {
            self.combined: self.progress[key][next(iter(self.progress[key]))]
        }
    # remake dataframe for well content
    self.wellsdf = admin.make_wells_df(self.r)
    # remake s dataframe
    self.s = admin.make_s(self)
    # add original experiments to sc dataframe
    self.sc["original_experiment"] = self.sc.experiment.copy()
    self.sc["experiment_id"] = (
        self.sc.experiment.astype("string").replace(experiment_map).copy()
    )
    self.sc.experiment = self.combined


def sort_attributes(self):
    """Alphabetically sort attributes describing experiments."""
    self.all_experiments = sorted(self.all_experiments)
    for e in self.all_conditions:
        self.all_conditions[e] = sorted(
            self.all_conditions[e], key=lambda x: (x.split()[-1], x.split()[0])
        )
    for e in self.all_strains:
        self.all_strains[e] = sorted(self.all_strains[e])
    for e in self.all_strains_conditions:
        self.all_strains_conditions[e] = sorted(
            self.all_strains_conditions[e],
            key=lambda x: (
                x.split(" in ")[0],
                x.split(" in ")[1].split()[-1],
                x.split(" in ")[1].split()[0],
            ),
        )
