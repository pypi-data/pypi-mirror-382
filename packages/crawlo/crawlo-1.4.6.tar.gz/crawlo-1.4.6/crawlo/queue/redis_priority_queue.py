import asyncio
import asyncio
import pickle
import time
import traceback
from typing import Optional, TYPE_CHECKING

import redis.asyncio as aioredis

# ä½¿ç”¨ TYPE_CHECKING é¿å…è¿è¡Œæ—¶å¾ªç¯å¯¼å…¥
if TYPE_CHECKING:
    from crawlo import Request

from crawlo.utils.error_handler import ErrorHandler
from crawlo.utils.log import get_logger
from crawlo.utils.redis_connection_pool import get_redis_pool, OptimizedRedisConnectionPool
from crawlo.utils.request_serializer import RequestSerializer

# å»¶è¿Ÿåˆå§‹åŒ–é¿å…å¾ªç¯ä¾èµ–
_logger = None
_error_handler = None


def get_module_logger():
    global _logger
    if _logger is None:
        _logger = get_logger(__name__)
    return _logger


def get_module_error_handler():
    global _error_handler
    if _error_handler is None:
        _error_handler = ErrorHandler(__name__)
    return _error_handler


class RedisPriorityQueue:
    """
    åŸºäº Redis çš„åˆ†å¸ƒå¼å¼‚æ­¥ä¼˜å…ˆçº§é˜Ÿåˆ—
    """

    def __init__(
            self,
            redis_url: str = None,
            queue_name: str = None,  # ä¿®æ”¹é»˜è®¤å€¼ä¸º None
            processing_queue: str = None,  # ä¿®æ”¹é»˜è®¤å€¼ä¸º None
            failed_queue: str = None,  # ä¿®æ”¹é»˜è®¤å€¼ä¸º None
            max_retries: int = 3,
            timeout: int = 300,  # ä»»åŠ¡å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_connections: int = 10,  # è¿æ¥æ± å¤§å°
            module_name: str = "default"  # æ·»åŠ  module_name å‚æ•°
    ):
        # ç§»é™¤ç›´æ¥ä½¿ç”¨ os.getenv()ï¼Œè¦æ±‚é€šè¿‡å‚æ•°ä¼ é€’ redis_url
        if redis_url is None:
            # å¦‚æœæ²¡æœ‰æä¾› redis_urlï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ï¼Œè¦æ±‚åœ¨ settings ä¸­é…ç½®
            raise ValueError("redis_url must be provided. Configure it in settings instead of using os.getenv()")

        self.redis_url = redis_url
        self.module_name = module_name  # ä¿å­˜ module_name

        # å¦‚æœæœªæä¾› queue_nameï¼Œåˆ™æ ¹æ® module_name è‡ªåŠ¨ç”Ÿæˆ
        if queue_name is None:
            self.queue_name = f"crawlo:{module_name}:queue:requests"
        else:
            # å¤„ç†å¤šé‡ crawlo å‰ç¼€ï¼Œè§„èŒƒåŒ–é˜Ÿåˆ—åç§°
            self.queue_name = self._normalize_queue_name(queue_name)

        # å¦‚æœæœªæä¾› processing_queueï¼Œåˆ™æ ¹æ® queue_name è‡ªåŠ¨ç”Ÿæˆ
        if processing_queue is None:
            if ":queue:requests" in self.queue_name:
                self.processing_queue = self.queue_name.replace(":queue:requests", ":queue:processing")
            else:
                self.processing_queue = f"{self.queue_name}:processing"
        else:
            self.processing_queue = processing_queue

        # å¦‚æœæœªæä¾› failed_queueï¼Œåˆ™æ ¹æ® queue_name è‡ªåŠ¨ç”Ÿæˆ
        if failed_queue is None:
            if ":queue:requests" in self.queue_name:
                self.failed_queue = self.queue_name.replace(":queue:requests", ":queue:failed")
            else:
                self.failed_queue = f"{self.queue_name}:failed"
        else:
            self.failed_queue = failed_queue

        self.max_retries = max_retries
        self.timeout = timeout
        self.max_connections = max_connections
        self._redis_pool: Optional[OptimizedRedisConnectionPool] = None
        self._redis: Optional[aioredis.Redis] = None
        self._lock = asyncio.Lock()  # ç”¨äºè¿æ¥åˆå§‹åŒ–çš„é”
        self.request_serializer = RequestSerializer()  # å¤„ç†åºåˆ—åŒ–

    def _normalize_queue_name(self, queue_name: str) -> str:
        """
        è§„èŒƒåŒ–é˜Ÿåˆ—åç§°ï¼Œå¤„ç†å¤šé‡ crawlo å‰ç¼€
        
        :param queue_name: åŸå§‹é˜Ÿåˆ—åç§°
        :return: è§„èŒƒåŒ–åçš„é˜Ÿåˆ—åç§°
        """
        # å¦‚æœé˜Ÿåˆ—åç§°å·²ç»ç¬¦åˆè§„èŒƒï¼ˆä»¥ crawlo: å¼€å¤´ä¸”ä¸æ˜¯ crawlo:crawlo:ï¼‰ï¼Œåˆ™ä¿æŒä¸å˜
        if queue_name.startswith("crawlo:") and not queue_name.startswith("crawlo:crawlo:"):
            return queue_name
            
        # å¤„ç†ä¸‰é‡ crawlo å‰ç¼€ï¼Œç®€åŒ–ä¸ºæ ‡å‡†æ ¼å¼
        if queue_name.startswith("crawlo:crawlo:crawlo:"):
            # ä¸‰é‡ crawlo å‰ç¼€ï¼Œç®€åŒ–ä¸ºæ ‡å‡† crawlo: æ ¼å¼
            remaining = queue_name[21:]  # å»æ‰ "crawlo:crawlo:crawlo:" å‰ç¼€
            if remaining:
                return f"crawlo:{remaining}"
            else:
                return "crawlo:requests"  # é»˜è®¤åç§°
                
        # å¤„ç†åŒé‡ crawlo å‰ç¼€
        elif queue_name.startswith("crawlo:crawlo:"):
            # åŒé‡ crawlo å‰ç¼€ï¼Œç®€åŒ–ä¸ºæ ‡å‡† crawlo: æ ¼å¼
            remaining = queue_name[14:]  # å»æ‰ "crawlo:crawlo:" å‰ç¼€
            if remaining:
                return f"crawlo:{remaining}"
            else:
                return "crawlo:requests"  # é»˜è®¤åç§°
                
        # å¤„ç†æ—  crawlo å‰ç¼€çš„æƒ…å†µ
        elif not queue_name.startswith("crawlo:"):
            # æ—  crawlo å‰ç¼€ï¼Œæ·»åŠ  crawlo: å‰ç¼€
            if queue_name:
                return f"crawlo:{queue_name}"
            else:
                return "crawlo:requests"  # é»˜è®¤åç§°
                
        # å…¶ä»–æƒ…å†µï¼Œä¿æŒä¸å˜
        else:
            return queue_name

    async def connect(self, max_retries=3, delay=1):
        """å¼‚æ­¥è¿æ¥ Redisï¼Œæ”¯æŒé‡è¯•"""
        async with self._lock:
            if self._redis is not None:
                # å¦‚æœå·²ç»è¿æ¥ï¼Œæµ‹è¯•è¿æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                try:
                    await self._redis.ping()
                    return self._redis
                except Exception:
                    # è¿æ¥å¤±æ•ˆï¼Œé‡æ–°è¿æ¥
                    self._redis = None

            for attempt in range(max_retries):
                try:
                    # ä½¿ç”¨ä¼˜åŒ–çš„è¿æ¥æ± ï¼Œç¡®ä¿ decode_responses=False ä»¥é¿å…ç¼–ç é—®é¢˜
                    self._redis_pool = get_redis_pool(
                        self.redis_url,
                        max_connections=self.max_connections,
                        socket_connect_timeout=5,
                        socket_timeout=30,
                        health_check_interval=30,
                        retry_on_timeout=True,
                        decode_responses=False,  # ç¡®ä¿ä¸è‡ªåŠ¨è§£ç å“åº”
                        encoding='utf-8'
                    )

                    self._redis = await self._redis_pool.get_connection()

                    # æµ‹è¯•è¿æ¥
                    await self._redis.ping()
                    # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¾“å‡ºè¯¦ç»†è¿æ¥ä¿¡æ¯
                    # get_module_logger().debug(f"Redis è¿æ¥æˆåŠŸ (Module: {self.module_name})")  # æ³¨é‡Šæ‰é‡å¤çš„æ—¥å¿—
                    return self._redis
                except Exception as e:
                    error_msg = f"Redis è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}, Module: {self.module_name}): {e}"
                    get_module_logger().warning(error_msg)
                    get_module_logger().debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                    else:
                        raise ConnectionError(f"æ— æ³•è¿æ¥ Redis (Module: {self.module_name}): {e}")

    async def _ensure_connection(self):
        """ç¡®ä¿è¿æ¥æœ‰æ•ˆ"""
        if self._redis is None:
            await self.connect()
        try:
            await self._redis.ping()
        except Exception as e:
            get_module_logger().warning(f"Redis è¿æ¥å¤±æ•ˆ (Module: {self.module_name})ï¼Œå°è¯•é‡è¿...: {e}")
            self._redis = None
            await self.connect()

    async def put(self, request, priority: int = 0) -> bool:
        """æ”¾å…¥è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        try:
            await self._ensure_connection()
            # ä¿®å¤ä¼˜å…ˆçº§è¡Œä¸ºä¸€è‡´æ€§é—®é¢˜
            # åŸæ¥: score = -priority ï¼ˆå¯¼è‡´priorityå¤§çš„å…ˆå‡ºé˜Ÿï¼‰
            # ç°åœ¨: score = priority ï¼ˆç¡®ä¿priorityå°çš„å…ˆå‡ºé˜Ÿï¼Œä¸å†…å­˜é˜Ÿåˆ—ä¸€è‡´ï¼‰
            score = priority
            key = self._get_request_key(request)

            # ğŸ”¥ ä½¿ç”¨ä¸“ç”¨çš„åºåˆ—åŒ–å·¥å…·æ¸…ç† Request
            clean_request = self.request_serializer.prepare_for_serialization(request)

            # ç¡®ä¿åºåˆ—åŒ–åçš„æ•°æ®å¯ä»¥è¢«æ­£ç¡®ååºåˆ—åŒ–
            try:
                serialized = pickle.dumps(clean_request)
                # éªŒè¯åºåˆ—åŒ–æ•°æ®å¯ä»¥è¢«ååºåˆ—åŒ–
                pickle.loads(serialized)
            except Exception as serialize_error:
                get_module_logger().error(f"è¯·æ±‚åºåˆ—åŒ–éªŒè¯å¤±è´¥ (Module: {self.module_name}): {serialize_error}")
                return False

            pipe = self._redis.pipeline()
            pipe.zadd(self.queue_name, {key: score})
            pipe.hset(f"{self.queue_name}:data", key, serialized)
            result = await pipe.execute()

            if result[0] > 0:
                get_module_logger().debug(f"æˆåŠŸå…¥é˜Ÿ (Module: {self.module_name}): {request.url}")  # æ³¨é‡Šæ‰é‡å¤çš„æ—¥å¿—
            return result[0] > 0
        except Exception as e:
            get_module_error_handler().handle_error(
                e,
                context=f"æ”¾å…¥é˜Ÿåˆ—å¤±è´¥ (Module: {self.module_name})",
                raise_error=False
            )
            return False

    async def get(self, timeout: float = 5.0):
        """
        è·å–è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶ï¼‰
        :param timeout: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…æ— é™è½®è¯¢
        """
        try:
            await self._ensure_connection()
            start_time = asyncio.get_event_loop().time()

            while True:
                # å°è¯•è·å–ä»»åŠ¡
                result = await self._redis.zpopmin(self.queue_name, count=1)
                if result:
                    key, score = result[0]
                    serialized = await self._redis.hget(f"{self.queue_name}:data", key)
                    if not serialized:
                        continue

                    # ç§»åŠ¨åˆ° processing
                    processing_key = f"{key}:{int(time.time())}"
                    pipe = self._redis.pipeline()
                    pipe.zadd(self.processing_queue, {processing_key: time.time() + self.timeout})
                    pipe.hset(f"{self.processing_queue}:data", processing_key, serialized)
                    pipe.hdel(f"{self.queue_name}:data", key)
                    await pipe.execute()

                    # æ›´å®‰å…¨çš„ååºåˆ—åŒ–æ–¹å¼
                    try:
                        # é¦–å…ˆå°è¯•æ ‡å‡†çš„ pickle ååºåˆ—åŒ–
                        request = pickle.loads(serialized)
                        return request
                    except UnicodeDecodeError:
                        # å¦‚æœå‡ºç°ç¼–ç é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ latin1 è§£ç 
                        request = pickle.loads(serialized, encoding='latin1')
                        return request
                    except Exception as pickle_error:
                        # å¦‚æœpickleååºåˆ—åŒ–å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡è¿™ä¸ªä»»åŠ¡
                        get_module_logger().error(f"æ— æ³•ååºåˆ—åŒ–è¯·æ±‚æ•°æ® (Module: {self.module_name}): {pickle_error}")
                        # ä»processingé˜Ÿåˆ—ä¸­ç§»é™¤è¿™ä¸ªæ— æ•ˆçš„ä»»åŠ¡
                        await self._redis.zrem(self.processing_queue, processing_key)
                        await self._redis.hdel(f"{self.processing_queue}:data", processing_key)
                        # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªä»»åŠ¡
                        continue

                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                if asyncio.get_event_loop().time() - start_time > timeout:
                    return None

                # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…ç©ºè½®è¯¢ï¼Œä½†å‡å°‘ç­‰å¾…æ—¶é—´ä»¥æé«˜å“åº”é€Ÿåº¦
                await asyncio.sleep(0.001)  # ä»0.01å‡å°‘åˆ°0.001

        except Exception as e:
            get_module_error_handler().handle_error(
                e,
                context=f"è·å–é˜Ÿåˆ—ä»»åŠ¡å¤±è´¥ (Module: {self.module_name})",
                raise_error=False
            )
            return None

    async def ack(self, request: "Request"):
        """ç¡®è®¤ä»»åŠ¡å®Œæˆ"""
        try:
            await self._ensure_connection()
            key = self._get_request_key(request)
            cursor = 0
            while True:
                cursor, keys = await self._redis.zscan(self.processing_queue, cursor, match=f"{key}:*")
                if keys:
                    pipe = self._redis.pipeline()
                    for k in keys:
                        pipe.zrem(self.processing_queue, k)
                        pipe.hdel(f"{self.processing_queue}:data", k)
                    await pipe.execute()
                if cursor == 0:
                    break
        except Exception as e:
            get_module_error_handler().handle_error(
                e,
                context=f"ç¡®è®¤ä»»åŠ¡å®Œæˆå¤±è´¥ (Module: {self.module_name})",
                raise_error=False
            )

    async def fail(self, request: "Request", reason: str = ""):
        """æ ‡è®°ä»»åŠ¡å¤±è´¥"""
        try:
            await self._ensure_connection()
            key = self._get_request_key(request)
            await self.ack(request)

            retry_key = f"{self.failed_queue}:retries:{key}"
            retries = await self._redis.incr(retry_key)
            await self._redis.expire(retry_key, 86400)

            if retries <= self.max_retries:
                await self.put(request, priority=request.priority + 1)
                get_module_logger().info(
                    f"ä»»åŠ¡é‡è¯• [{retries}/{self.max_retries}] (Module: {self.module_name}): {request.url}")
            else:
                failed_data = {
                    "url": request.url,
                    "reason": reason,
                    "retries": retries,
                    "failed_at": time.time(),
                    "request_pickle": pickle.dumps(request).hex(),  # å¯é€‰ï¼šä¿å­˜å®Œæ•´è¯·æ±‚
                }
                await self._redis.lpush(self.failed_queue, pickle.dumps(failed_data))
                get_module_logger().error(f"ä»»åŠ¡å½»åº•å¤±è´¥ [{retries}æ¬¡] (Module: {self.module_name}): {request.url}")
        except Exception as e:
            get_module_error_handler().handle_error(
                e,
                context=f"æ ‡è®°ä»»åŠ¡å¤±è´¥å¤±è´¥ (Module: {self.module_name})",
                raise_error=False
            )

    def _get_request_key(self, request) -> str:
        """ç”Ÿæˆè¯·æ±‚å”¯ä¸€é”®"""
        return f"{self.module_name}:url:{hash(request.url) & 0x7FFFFFFF}"  # ç¡®ä¿æ­£æ•°

    async def qsize(self) -> int:
        """Get queue size"""
        try:
            await self._ensure_connection()
            return await self._redis.zcard(self.queue_name)
        except Exception as e:
            get_module_error_handler().handle_error(
                e,
                context=f"Failed to get queue size (Module: {self.module_name})",
                raise_error=False
            )
            return 0

    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            # è¿æ¥æ± ä¼šè‡ªåŠ¨ç®¡ç†è¿æ¥ï¼Œè¿™é‡Œä¸éœ€è¦æ˜¾å¼å…³é—­å•ä¸ªè¿æ¥
            self._redis = None
            self._redis_pool = None
            get_module_logger().debug(f"Redis è¿æ¥å·²é‡Šæ”¾ (Module: {self.module_name})")
        except Exception as e:
            get_module_error_handler().handle_error(
                e,
                context=f"é‡Šæ”¾ Redis è¿æ¥å¤±è´¥ (Module: {self.module_name})",
                raise_error=False
            )
