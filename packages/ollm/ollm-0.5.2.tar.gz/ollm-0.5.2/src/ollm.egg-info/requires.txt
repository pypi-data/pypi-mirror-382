numpy
torch>2.6.0
transformers>=4.57.0
accelerate
flash-attn
flash-linear-attention
