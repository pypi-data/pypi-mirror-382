{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "72cf0370-2350-4eb3-9e76-5850adce7ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zia-sdk-python[databricks] in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (0.0.13)\nRequirement already satisfied: httpx<0.29.0,>=0.28.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (0.28.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.11.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (2.11.7)\nRequirement already satisfied: pytest<9.0.0,>=8.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (8.4.1)\nRequirement already satisfied: pytest-asyncio<2.0.0,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (1.1.0)\nRequirement already satisfied: pytest-cov<7.0.0,>=6.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (6.2.1)\nRequirement already satisfied: python-dotenv<2.0.0,>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (1.1.1)\nRequirement already satisfied: pandas<3.0.0,>=2.2.3 in /databricks/python3/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (2.2.3)\nRequirement already satisfied: pyspark<4.0.0,>=3.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from zia-sdk-python[databricks]) (3.5.6)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<0.29.0,>=0.28.1->zia-sdk-python[databricks]) (4.6.2)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx<0.29.0,>=0.28.1->zia-sdk-python[databricks]) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<0.29.0,>=0.28.1->zia-sdk-python[databricks]) (1.0.2)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.12/site-packages (from httpx<0.29.0,>=0.28.1->zia-sdk-python[databricks]) (3.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.1->zia-sdk-python[databricks]) (0.14.0)\nRequirement already satisfied: numpy>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.2.3->zia-sdk-python[databricks]) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.2.3->zia-sdk-python[databricks]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.2.3->zia-sdk-python[databricks]) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.2.3->zia-sdk-python[databricks]) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.11.7->zia-sdk-python[databricks]) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.11.7->zia-sdk-python[databricks]) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.11.7->zia-sdk-python[databricks]) (4.12.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.11.7->zia-sdk-python[databricks]) (0.4.1)\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from pyspark<4.0.0,>=3.4.0->zia-sdk-python[databricks]) (0.10.9.7)\nRequirement already satisfied: iniconfig>=1 in /databricks/python3/lib/python3.12/site-packages (from pytest<9.0.0,>=8.4.1->zia-sdk-python[databricks]) (1.1.1)\nRequirement already satisfied: packaging>=20 in /databricks/python3/lib/python3.12/site-packages (from pytest<9.0.0,>=8.4.1->zia-sdk-python[databricks]) (24.2)\nRequirement already satisfied: pluggy<2,>=1.5 in /databricks/python3/lib/python3.12/site-packages (from pytest<9.0.0,>=8.4.1->zia-sdk-python[databricks]) (1.5.0)\nRequirement already satisfied: pygments>=2.7.2 in /databricks/python3/lib/python3.12/site-packages (from pytest<9.0.0,>=8.4.1->zia-sdk-python[databricks]) (2.15.1)\nRequirement already satisfied: coverage>=7.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a00eff79-7c94-4362-9889-9aa07d757b03/lib/python3.12/site-packages (from coverage[toml]>=7.5->pytest-cov<7.0.0,>=6.2.1->zia-sdk-python[databricks]) (7.10.6)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.3->zia-sdk-python[databricks]) (1.16.0)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio->httpx<0.29.0,>=0.28.1->zia-sdk-python[databricks]) (1.3.0)\n\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Zia Neurolabs SDK Python  \n",
    "!pip install --upgrade zia-sdk-python[databricks] \n",
    "# Use below if you're installing updates to the package \n",
    "# %restart_python \n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2048ecbb-1f06-4926-ab51-f5e223936709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Base \n",
    "from typing import List, Any\n",
    "import datetime\n",
    "\n",
    "# Import data handling dependencies \n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import zia-sdk depdendencies \n",
    "from neurolabszia import Zia, NLIRResult \n",
    "from neurolabszia.utils import ir_results_to_dataframe, get_spark_schema_from_dataframe, to_spark_dataframe\n",
    "\n",
    "# 1. Get API key securely from Databricks secrets \n",
    "try:\n",
    "    api_key = dbutils.secrets.get(scope=\"neurolabs-api\", key=\"demo_abi\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to retrieve API key from Databricks secrets. Make sure the secret scope and key are set up.\") from e\n",
    "\n",
    "# Helper method to get paginated results using Neurolabs SDK \n",
    "async def get_paginated_results(\n",
    "    client: Zia, task_uuid: str, batch_size: int = 10, max_iter: int = 5\n",
    ") -> list[NLIRResult]:\n",
    "    \"\"\"\n",
    "    Get all results from a task using pagination.\n",
    "\n",
    "    Args:\n",
    "        client: Zia client instance\n",
    "        task_uuid: The UUID of the task\n",
    "        batch_size: Number of results to fetch per request (default: 10)\n",
    "        max_iter: Maximum number of batches to fetch results for (default: 5)\n",
    "    Returns:\n",
    "        List of all NLIRResult objects\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    offset = 0\n",
    "\n",
    "    print(f\"üîç Fetching paginated results for task: {task_uuid}\")\n",
    "    print(f\"üì¶ Batch size: {batch_size}\")\n",
    "\n",
    "    while len(all_results) < (batch_size * max_iter):\n",
    "        print(f\"\\nüìÑ Fetching batch at offset {offset}...\")\n",
    "\n",
    "        # Get a batch of results \n",
    "        batch = await client.result_management.get_task_results(\n",
    "            task_uuid=task_uuid, limit=batch_size, offset=offset\n",
    "        )\n",
    "\n",
    "        if not batch:\n",
    "            print(f\"‚úÖ No more results found at offset {offset}\")\n",
    "            break\n",
    "\n",
    "        print(f\"‚úÖ Retrieved {len(batch)} results\")\n",
    "        all_results.extend(batch)\n",
    "\n",
    "        # If we got fewer results than requested, we've reached the end\n",
    "        if len(batch) < batch_size:  \n",
    "            print(f\"‚úÖ Reached end of results (got {len(batch)} < {batch_size})\")\n",
    "            break\n",
    "\n",
    "        offset += batch_size\n",
    "\n",
    "    print(f\"\\nüéâ Total results retrieved: {len(all_results)}\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ef4522-0639-44aa-84cc-c6cc84308ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: eyJh*********************************************************************************************************************************************************************************************************************************************************************************************-RdI\n"
     ]
    }
   ],
   "source": [
    "# Databricks redacts secrets by default, print the first and last 4 characters of the API key to make sure you've got what's required \n",
    "print(f\"API Key: {api_key[:4]}{'*' * (len(api_key) - 8)}{api_key[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910834f6-528e-45f6-af62-792387f7b9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run `get_paginated_results` when you want to get new data via Neurolabs API \n",
    "task_uuid = \"ec01e5c7-51c9-4889-8136-19a4ab7168c1\"\n",
    "\n",
    "async def get_all(max_iter):\n",
    "    \"\"\"Main function demonstrating paginated results usage.\"\"\"\n",
    "\n",
    "    print(\"üöÄ Zia SDK - Pull Batched IR Results - Example\")\n",
    "    print(\"=\" * 60)\n",
    "    all_results = []\n",
    "    # Initialize client once and reuse it\n",
    "    async with Zia(api_key) as client:\n",
    "        # Example 1: Get all results with pagination\n",
    "        print(\"\\n1Ô∏è‚É£ Getting all paginated results:\")\n",
    "        try:\n",
    "            all_results = await get_paginated_results(client, task_uuid, batch_size=20, max_iter=max_iter)\n",
    "            print(f\"‚úÖ Successfully retrieved {len(all_results)} total results\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting all results: {e}\")\n",
    "\n",
    "    return all_results \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ Paginated results example complete!\")\n",
    "\n",
    "\n",
    "#all_results = []\n",
    "#for results in results_path: \n",
    "#    data = load_json(results)\n",
    "    # Parse into our NLB\n",
    "    # results = [NLIRResult.model_validate(result) for result in data[\"items\"]]\n",
    "    # all_results.extend(results)\n",
    "\n",
    "#print(f\"Total results retrieved: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8489760d-16ae-4ce2-a205-448a868c5c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Zia SDK - Pull Batched IR Results - Example\n============================================================\n\n1Ô∏è‚É£ Getting all paginated results:\nüîç Fetching paginated results for task: ec01e5c7-51c9-4889-8136-19a4ab7168c1\nüì¶ Batch size: 20\n\nüìÑ Fetching batch at offset 0...\n‚úÖ Retrieved 20 results\n\nüìÑ Fetching batch at offset 20...\n‚úÖ Retrieved 20 results\n\nüéâ Total results retrieved: 40\n‚úÖ Successfully retrieved 40 total results\n"
     ]
    }
   ],
   "source": [
    "all_results = await get_all(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706af220-bcd2-4d98-8b60-c9939a5cf9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f56d39-42b7-402f-b08b-57f2748be922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started at: 2025-09-02 21:55:22.010910\nSuccessfully wrote 1480 records to table neurolabs_ir_results_demo.ir_results_sample.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert IR Results into Spark Dataframe and Upload to Unity Catalog \n",
    "\n",
    "print(\"Execution started at:\", datetime.now())\n",
    "\n",
    "# 1. Create Spark session\n",
    "spark = SparkSession.builder.appName(\"NLIRResultsIngestion\").getOrCreate()\n",
    "\n",
    "# 2. Create Catalog, Schema & Table \n",
    "catalog_name = \"catalog_integration\"\n",
    "schema_name = \"neurolabs_ir_results_demo\" \n",
    "table_name = \"ir_results_sample\"\n",
    "\n",
    "# 3. Convert NLIRResults -> pd.Dataframe -> Spark Dataframe\n",
    "#pdf = ir_results_to_dataframe(all_results)\n",
    "#ir_results_schema = get_spark_schema_from_dataframe(pdf)\n",
    "df_spark = to_spark_dataframe(all_results, spark)\n",
    "# df_spark.head(2)\n",
    "\n",
    "# 4. Write to Databricks Delta table\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "\n",
    "print(f\"Successfully wrote {df_spark.count()} records to table {schema_name}.{table_name}.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162588cb-8656-44eb-b1b2-a4dba31d0743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deprecated - helpers to facilitate IRREsults -> Dataframe -> Spark conversion now in zia.utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a61838ba-5082-45d2-97df-0ef9fd457ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Used for debugging \n",
    "pdf = ir_results_to_dataframe(all_results)\n",
    "pdf.head(2)\n",
    "# Replace NaN values with None for Spark compatibility\n",
    "pdf = pdf.where(pd.notnull(pdf), None)\n",
    "#schema = get_dynamic_spark_schema(pdf)\n",
    "schema = []\n",
    "\n",
    "# 1. Create Spark session\n",
    "spark_session = SparkSession.builder.appName(\"NLIRResultsIngestion\").getOrCreate()\n",
    "\n",
    "# Convert to Spark DataFrame using records\n",
    "df_spark = spark_session.createDataFrame(pdf, schema=schema)\n",
    "# 4. Write to Databricks Delta table\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "\n",
    "print(f\"Successfully wrote {df_spark.count()} records to table {schema_name}.{table_name}.\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3546fc75-86b5-4ca4-8a8e-4177a9247563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helpers to call API, get paginated IRResults & connvert NLIRResults into a Dataframe \n",
    "# TODO: Include the NLIRResults -> Spark Dataframe in the zia/utils in the next iteration of the SDK \n",
    "\n",
    "def get_dynamic_spark_schema(df: pd.DataFrame) -> 'StructType':\n",
    "    \"\"\"\n",
    "    Dynamically generate a Spark schema based on the actual DataFrame structure.\n",
    "    \n",
    "    This function analyzes the pandas DataFrame and creates a matching Spark schema,\n",
    "    ensuring no mismatches between the DataFrame columns and schema fields.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame created by ir_results_to_dataframe()\n",
    "        \n",
    "    Returns:\n",
    "        pyspark.sql.types.StructType schema that matches the DataFrame exactly\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import (\n",
    "        StructType, FloatType, TimestampType, BooleanType,\n",
    "        ArrayType, StructType as SparkStructType\n",
    "    )\n",
    "    \n",
    "    fields = []\n",
    "    \n",
    "    for column_name, dtype in df.dtypes.items():\n",
    "        # Handle different pandas dtypes\n",
    "        if dtype == 'object':\n",
    "            # Check if it's a datetime column\n",
    "            if column_name in ['result_created_at', 'result_updated_at']:\n",
    "                spark_type = TimestampType()\n",
    "            # Check if it's the alternative_predictions column (list of dicts)\n",
    "            elif column_name == 'alternative_predictions':\n",
    "                # Define schema for alternative prediction items\n",
    "                alt_pred_schema = SparkStructType([\n",
    "                    StructField(\"category_id\", IntegerType(), True),\n",
    "                    StructField(\"category_name\", StringType(), True),\n",
    "                    StructField(\"score\", FloatType(), True),\n",
    "                ])\n",
    "                spark_type = ArrayType(alt_pred_schema)\n",
    "            else:\n",
    "                spark_type = StringType()\n",
    "        elif dtype == 'int64':\n",
    "            spark_type = IntegerType()\n",
    "        elif dtype == 'float64':\n",
    "            spark_type = FloatType()\n",
    "        elif dtype == 'bool':\n",
    "            spark_type = BooleanType()\n",
    "        elif dtype == 'datetime64[ns]':\n",
    "            spark_type = TimestampType()\n",
    "        else:\n",
    "            # Default to string for unknown types\n",
    "            spark_type = StringType()\n",
    "        \n",
    "        fields.append(StructField(column_name, spark_type, True))\n",
    "    \n",
    "    return StructType(fields)\n",
    "\n",
    "\n",
    "def get_spark_schema_from_dataframe(df: pd.DataFrame) -> 'StructType':\n",
    "    \"\"\"\n",
    "    Generate Spark schema directly from the DataFrame structure.\n",
    "    \n",
    "    This is the recommended approach to ensure perfect schema matching.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame created by ir_results_to_dataframe()\n",
    "        \n",
    "    Returns:\n",
    "        pyspark.sql.types.StructType schema that matches the DataFrame exactly\n",
    "    \"\"\"\n",
    "    return get_dynamic_spark_schema(df)\n",
    "\n",
    "def ir_results_to_dataframe(\n",
    "    results: List[Any],\n",
    "    include_bbox: bool = True,\n",
    "    include_alternative_predictions: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a list of NLIRResult objects to a pandas DataFrame.\n",
    "    \n",
    "    This function matches categories with annotations using the category_id\n",
    "    and creates a flat DataFrame with all attributes for each detected item.\n",
    "    \n",
    "    Args:\n",
    "        results: List of NLIRResult objects (from zia.models)\n",
    "        include_bbox: Whether to include bounding box coordinates as separate columns\n",
    "        include_alternative_predictions: Whether to include alternative predictions\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with one row per detected item\n",
    "        \n",
    "    Example:\n",
    "        >>> from zia import ir_results_to_dataframe\n",
    "        >>> results = await client.image_recognition.get_all_task_results(task_uuid)\n",
    "        >>> df = ir_results_to_dataframe(results)\n",
    "        >>> print(df.head())\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for result in results:\n",
    "        if not result.coco or result.status.value != \"PROCESSED\":\n",
    "            continue\n",
    "\n",
    "        # Create a mapping of category_id to category for quick lookup\n",
    "        category_map = {cat.id: cat for cat in result.coco.categories}\n",
    "\n",
    "        for annotation in result.coco.annotations:\n",
    "            # Get the corresponding category\n",
    "            category = category_map.get(annotation.category_id)\n",
    "            if not category:\n",
    "                continue\n",
    "\n",
    "            # Base row with result-level information\n",
    "            row = {\n",
    "                # Result-level information\n",
    "                \"result_uuid\": result.uuid,\n",
    "                \"task_uuid\": result.task_uuid,\n",
    "                \"image_url\": result.image_url,\n",
    "                \"result_status\": result.status.value,\n",
    "                \"result_duration\": result.duration,\n",
    "                \"result_created_at\": result.created_at,\n",
    "                \"result_updated_at\": result.updated_at,\n",
    "                \"confidence_score\": result.confidence_score,\n",
    "                # Image information\n",
    "                \"image_id\": annotation.image_id,\n",
    "                \"image_width\": next(\n",
    "                    (\n",
    "                        img.width\n",
    "                        for img in result.coco.images\n",
    "                        if img.id == annotation.image_id\n",
    "                    ),\n",
    "                    None,\n",
    "                ),\n",
    "                \"image_height\": next(\n",
    "                    (\n",
    "                        img.height\n",
    "                        for img in result.coco.images\n",
    "                        if img.id == annotation.image_id\n",
    "                    ),\n",
    "                    None,\n",
    "                ),\n",
    "                \"image_filename\": next(\n",
    "                    (\n",
    "                        img.file_name\n",
    "                        for img in result.coco.images\n",
    "                        if img.id == annotation.image_id\n",
    "                    ),\n",
    "                    None,\n",
    "                ),\n",
    "                # Annotation information\n",
    "                \"annotation_id\": annotation.id,\n",
    "                \"category_id\": annotation.category_id,\n",
    "                \"area\": annotation.area,\n",
    "                \"iscrowd\": annotation.iscrowd,\n",
    "                \"detection_score\": annotation.neurolabs.score,\n",
    "                # Category information\n",
    "                \"category_name\": category.name,\n",
    "                \"category_supercategory\": category.supercategory,\n",
    "            }\n",
    "\n",
    "            # Add bounding box coordinates if requested\n",
    "            if include_bbox and annotation.bbox:\n",
    "                row.update(\n",
    "                    {\n",
    "                        \"bbox_x\": annotation.bbox[0],\n",
    "                        \"bbox_y\": annotation.bbox[1],\n",
    "                        \"bbox_width\": annotation.bbox[2],\n",
    "                        \"bbox_height\": annotation.bbox[3],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Add Neurolabs category information\n",
    "            if category.neurolabs:\n",
    "                row.update(\n",
    "                    {\n",
    "                        \"product_uuid\": category.neurolabs.productUuid,\n",
    "                        \"product_name\": category.neurolabs.name,\n",
    "                        \"product_brand\": category.neurolabs.brand,\n",
    "                        \"product_barcode\": category.neurolabs.barcode,\n",
    "                        \"product_custom_id\": category.neurolabs.customId,\n",
    "                        \"product_label\": category.neurolabs.label,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Add alternative predictions if requested\n",
    "            if (\n",
    "                include_alternative_predictions\n",
    "                and annotation.neurolabs.alternative_predictions\n",
    "            ):\n",
    "                alt_predictions = []\n",
    "                for alt_pred in annotation.neurolabs.alternative_predictions:\n",
    "                    alt_category = category_map.get(alt_pred.category_id)\n",
    "                    alt_predictions.append(\n",
    "                        {\n",
    "                            \"category_id\": alt_pred.category_id,\n",
    "                            \"category_name\": alt_category.name\n",
    "                            if alt_category\n",
    "                            else f\"Unknown_{alt_pred.category_id}\",\n",
    "                            \"score\": alt_pred.score,\n",
    "                        }\n",
    "                    )\n",
    "                row[\"alternative_predictions\"] = alt_predictions\n",
    "\n",
    "            # Add modalities if present\n",
    "            if annotation.neurolabs.modalities:\n",
    "                for (\n",
    "                    modality_name,\n",
    "                    modality_value,\n",
    "                ) in annotation.neurolabs.modalities.items():\n",
    "                    row[f\"modality_{modality_name}\"] = modality_value\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Convert datetime columns\n",
    "    datetime_columns = [\"result_created_at\", \"result_updated_at\"]\n",
    "    for col in datetime_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "async def get_paginated_results(client: Zia, task_uuid: str, \n",
    "                                batch_size: int = 10, max_offset: int = 100) -> List[NLIRResult]:\n",
    "    \"\"\"\n",
    "    Get all results from a task using pagination.\n",
    "    \n",
    "    Args:\n",
    "        client: Zia client instance\n",
    "        task_uuid: The UUID of the task\n",
    "        batch_size: Number of results to fetch per request (default: 10)\n",
    "        max_offset: Maximum number of images to fetch results for (default: 100)\n",
    "    Returns:\n",
    "        List of all NLIRResult objects\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    offset = 0\n",
    "    \n",
    "    print(f\"üîç Fetching paginated results for task: {task_uuid}\")\n",
    "    print(f\"üì¶ Batch size: {batch_size}\")\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\nüìÑ Fetching batch at offset {offset}...\")\n",
    "        \n",
    "        # Get a batch of results\n",
    "        batch = await client.result_management.get_task_results(\n",
    "            task_uuid=task_uuid,\n",
    "            limit=batch_size,\n",
    "            offset=offset\n",
    "        )\n",
    "        \n",
    "        if not batch:\n",
    "            print(f\"‚úÖ No more results found at offset {offset}\")\n",
    "            break\n",
    "            \n",
    "        print(f\"‚úÖ Retrieved {len(batch)} results\")\n",
    "        all_results.extend(batch)\n",
    "        \n",
    "        # If we got fewer results than requested, we've reached the end\n",
    "        if len(batch) < batch_size or offset >= max_offset:\n",
    "            print(f\"‚úÖ Reached end of results (got {len(batch)} < {batch_size})\")\n",
    "            break\n",
    "            \n",
    "        offset += batch_size\n",
    "    \n",
    "    print(f\"\\nüéâ Total results retrieved: {len(all_results)}\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "async def get_paginated_results_with_status_filter(\n",
    "    client: Zia,\n",
    "    task_uuid: str, \n",
    "    status_filter: str = \"PROCESSED\",\n",
    "    batch_size: int = 10,\n",
    "    max_offset: int = 100\n",
    ") -> List[NLIRResult]:\n",
    "    \"\"\"\n",
    "    Get paginated results with status filtering.\n",
    "    \n",
    "    Args:\n",
    "        client: Zia client instance\n",
    "        task_uuid: The UUID of the task\n",
    "        status_filter: Status to filter by (default: \"PROCESSED\")\n",
    "        batch_size: Number of results to fetch per request\n",
    "        \n",
    "    Returns:\n",
    "        List of filtered NLIRResult objects\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    offset = 0\n",
    "    \n",
    "    print(f\"üîç Fetching {status_filter} results for task: {task_uuid}\")\n",
    "    print(f\"üì¶ Batch size: {batch_size}\")\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\nüìÑ Fetching batch at offset {offset}...\")\n",
    "        \n",
    "        # Get a batch of results\n",
    "        batch = await client.result_management.get_task_results(\n",
    "            task_uuid=task_uuid,\n",
    "            limit=batch_size,\n",
    "            offset=offset\n",
    "        )\n",
    "        \n",
    "        if not batch:\n",
    "            print(f\"‚úÖ No more results found at offset {offset}\")\n",
    "            break\n",
    "            \n",
    "        # Filter by status\n",
    "        filtered_batch = [result for result in batch if result.status.value == status_filter]\n",
    "        \n",
    "        print(f\"‚úÖ Retrieved {len(batch)} results, {len(filtered_batch)} with status '{status_filter}'\")\n",
    "        all_results.extend(filtered_batch)\n",
    "        \n",
    "        # If we got fewer results than requested, we've reached the end\n",
    "        if len(batch) < batch_size or offset >= max_offset:\n",
    "            print(f\"‚úÖ Reached end of results (got {len(batch)} < {batch_size})\")\n",
    "            break\n",
    "            \n",
    "        offset += batch_size\n",
    "    \n",
    "    print(f\"\\nüéâ Total {status_filter} results: {len(all_results)}\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e5b4b3d8-016e-482f-80bb-6960a54c621b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   uuid  ...                  updated_at\n0  a264509a-8533-4943-bd48-3f1333ebd052  ...  2025-03-11T13:28:49.695921\n1  6b59bb4b-76ff-4159-b5a8-715f96d3f45d  ...  2025-01-23T12:00:59.785723\n\n[2 rows x 16 columns]\nSuccessfully wrote 328 records to table neurolabs_catalog.agbarr_catalog.\n"
     ]
    }
   ],
   "source": [
    "# 2. Fetch all catalog items with pagination\n",
    "def fetch_all_catalog_items(api_key, base_url, limit=50):\n",
    "    headers = {\"accept\": \"application/json\", \"X-API-Key\": api_key}\n",
    "    offset = 0\n",
    "    all_items = []\n",
    "    while True:\n",
    "        url = f\"{base_url}?limit={limit}&offset={offset}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_items.extend(items)\n",
    "        if len(items) < limit:\n",
    "            break\n",
    "        offset += limit\n",
    "    return all_items\n",
    "\n",
    "base_url = \"https://api.neurolabs.ai/v2/catalog-items\"\n",
    "items = fetch_all_catalog_items(api_key, base_url, limit=10)\n",
    "\n",
    "# 3. Convert to Spark DataFrame\n",
    "# TODO - remove the need for converting to pandas\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCatalogIngestion\").getOrCreate()\n",
    "pdf = pd.DataFrame(items)\n",
    "spark_df = spark.createDataFrame(pdf)\n",
    "\n",
    "# 4. Write to Databricks Delta table\n",
    "schema = \"neurolabs_catalog\" \n",
    "# Cast string columns to appropriate types\n",
    "spark_df = spark_df.withColumn(\"height\", col(\"height\").cast(\"double\"))\n",
    "spark_df = spark_df.withColumn(\"width\", col(\"width\").cast(\"double\"))\n",
    "spark_df = spark_df.withColumn(\"depth\", col(\"depth\").cast(\"double\"))\n",
    "spark_df = spark_df.withColumn(\"created_at\", col(\"created_at\").cast(\"timestamp\"))\n",
    "spark_df = spark_df.withColumn(\"updated_at\", col(\"updated_at\").cast(\"timestamp\"))\n",
    "\n",
    "# Write data to a managed table in a custom database\n",
    "table_name = \"agbarr_catalog\"\n",
    "print(pdf.head(2))\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema}.{table_name}\")\n",
    "\n",
    "print(f\"Successfully wrote {spark_df.count()} records to table {schema}.{table_name}.\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IR Results Neurolabs Ingestion [DB Integration]",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}