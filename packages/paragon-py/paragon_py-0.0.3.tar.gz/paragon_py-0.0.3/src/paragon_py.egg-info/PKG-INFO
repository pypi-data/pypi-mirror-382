Metadata-Version: 2.4
Name: paragon-py
Version: 0.0.3
Summary: Paragon AI â€” Python bindings (ctypes) with embedded cross-vendor GPU backends
Author: OpenFluke / Samuel Watson
License-Expression: Apache-2.0
Project-URL: Homepage, https://github.com/openfluke/paragon-py
Project-URL: Source, https://github.com/openfluke/paragon-py
Project-URL: Issues, https://github.com/openfluke/paragon-py/issues
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: Implementation :: CPython
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Dynamic: license-file

# ğŸ§  Paragon-Py

**Paragon-Py** is the official Python binding for the [Paragon AI Framework](https://github.com/openfluke/paragon).  
It provides a lightweight `ctypes` interface to Paragonâ€™s **GPU-agnostic runtime** â€” built in Go and powered by **WebGPU / Vulkan / Metal / DirectX 12**.

> Run the same AI models across NVIDIA, AMD, Intel, Apple, and Qualcomm GPUs â€” identical outputs, zero conversions.

---

## âœ¨ Features

- ğŸ§© **Cross-vendor GPU acceleration** â€” Vulkan, Metal, GL, and DX12 backends.
- ğŸ§  **Unified C-ABI layer** via [Teleport](https://github.com/openfluke/teleport): same interface across languages.
- âš™ï¸ **Pure Python bindings** â€” no build step, binaries included via Git LFS.
- ğŸ”¬ **Deterministic inference** â€” identical numerical results across vendors.
- ğŸŒ **Portable** â€” works in Jupyter, scripts, or servers (Linux / macOS / Windows).

---

## ğŸ“¦ Installation

Paragon-Py is distributed on **PyPI**:

```bash
pip install paragon-py
```

If youâ€™re building from source (e.g., this repo):

```bash
git clone https://github.com/openfluke/paragon-py.git
cd paragon-py
python -m build
pip install dist/paragon_py-*.whl
```

Binaries for Linux, macOS (Intel/ARM), and Windows are embedded in the wheel.  
No compiler or Go toolchain required.

---

## ğŸ§° Quick Start

```python
import paragon_py as paragon

# Create a small 3-layer network: input â†’ hidden â†’ output
# Each layer uses ReLU activation and is trainable.
h = paragon.new_network(
    shapes=[(4, 8), (8, 8), (8, 2)],     # width x height per layer
    activations=["relu", "relu", "relu"],
    trainable=[True, True, True],
    use_gpu=True
)

# Initialize GPU backend (optional but faster)
paragon.initialize_gpu(h)

# Dummy forward pass
sample_input = [[0.1, 0.5, 0.3, 0.7]]
paragon.forward(h, sample_input)

# Extract and print the output
out = paragon.extract_output(h)
print("Network output:", out)

# Cleanup GPU resources
paragon.cleanup_gpu(h)
```

---

## ğŸ§© Functions

| Function                                                    | Description                              |
| ----------------------------------------------------------- | ---------------------------------------- |
| `new_network(shapes, activations, trainable, use_gpu)`      | Create a new network and return a handle |
| `forward(handle, image_2d)`                                 | Run forward propagation                  |
| `extract_output(handle)`                                    | Get output tensor as a list              |
| `initialize_gpu(handle)`                                    | Initialize GPU context manually          |
| `train(handle, inputs, targets, epochs, lr, shuffle=False)` | Run training loop                        |
| `cleanup_gpu(handle)`                                       | Free GPU buffers                         |
| `get_phase_methods_json()`                                  | Introspect available backend methods     |

---

## ğŸ§± Repo Structure

```
paragon-py/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ MANIFEST.in
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ src/paragon_py/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ utils.py
    â”œâ”€â”€ linux_amd64/
    â”œâ”€â”€ darwin_amd64/
    â”œâ”€â”€ darwin_arm64/
    â”œâ”€â”€ darwin_universal/
    â””â”€â”€ windows_amd64/
```

Each platform folder includes a **Teleport** binary (`teleport_*.so/.dylib/.dll`)  
built from [openfluke/teleport](https://github.com/openfluke/teleport).

---

## ğŸ”— Related Projects

- ğŸ§  [Paragon](https://github.com/openfluke/paragon) â€” The core AI framework (Go + WebGPU)
- ğŸ§¬ [Teleport](https://github.com/openfluke/teleport) â€” C-ABI bridge enabling language-agnostic bindings
- ğŸª [OpenFluke](https://github.com/openfluke) â€” Unified ecosystem for reproducible, cross-vendor AI

---

## ğŸ§¾ License

Licensed under the **Apache 2.0 License**.  
See [`LICENSE`](./LICENSE) for details.

---

## ğŸ§‘â€ğŸ’» Contributing

Pull requests are welcome!  
If youâ€™re adding a new backend or updating native binaries:

1. Build the new `teleport` binaries from [openfluke/teleport](https://github.com/openfluke/teleport/releases)
2. Drop them into the matching platform folder under `src/paragon_py/`
3. Commit with Git LFS enabled
4. Bump version in `pyproject.toml` and tag (e.g. `v0.0.4`)

---

## ğŸ§  About Paragon

Paragon is a next-generation, GPU-agnostic AI runtime built in Go with WebGPU acceleration.  
Itâ€™s designed for _deterministic reproducibility_ and _cross-platform portability_,  
running natively across CPU, GPU, browser, and embedded systems â€” without model conversion.

---

**Author:** [Samuel Watson](https://github.com/openfluke)  
**Organization:** [OpenFluke](https://openfluke.com)

> â€œOne model, any device.â€
