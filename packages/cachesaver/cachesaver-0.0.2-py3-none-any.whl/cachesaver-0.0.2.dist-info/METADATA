Metadata-Version: 2.4
Name: cachesaver
Version: 0.0.2
Summary: A caching library built with LLM experiments in mind
Author-email: Lars Klein <lars.klein@epfl.ch>
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: deepdiff
Requires-Dist: diskcache
Requires-Dist: openai
Requires-Dist: together
Provides-Extra: test
Requires-Dist: pytest>=7.0.0; extra == "test"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "test"
Requires-Dist: pytest-cov>=4.1.0; extra == "test"
Requires-Dist: deepdiff; extra == "test"
Requires-Dist: diskcache; extra == "test"
Dynamic: license-file

# Cachesaver
Cachesaver is a high-efficiency caching library for experiments with large language models (LLMs), designed to minimize costs, improve reproducibility, and streamline debugging. The library enables caching of multiple responses per query, tracks usage for unique sampling across runs, and ensures statistical integrity. Built on Pythonâ€™s `asyncio`, Cachesaver supports asynchronous execution, request batching, prompt deduplication, and race-condition prevention.

```python
from cachesaver.models.openai import AsyncOpenAI

client = AsyncOpenAI(batch_size=2)

resp = await client.chat.completions.create(
    model="gpt-4.1-nano",
    messages=[
        {"role": "user", "content": "What's the capital of France?"}
    ]
)
```
