#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep 29 17:52:45 2025

Code tests generated by ChatGPT-5. Most seem reasonable. Some of the values 
tested, B, tol_p, and difference thresholds had to be changed significantly.
Removed rng seeds.

@author: Jon Paul Lundquist
"""

# tests/test_basic.py
import math
import numpy as np
import pytest
import sys
from ltest import ltest


def _finite_tuple(res):
    """Helper: ensure returned tuple has only finite floats where expected."""
    l_p, l_p_err, l_shift, shift_boot, shift_err, l_stat = res
    for v in (l_p, l_p_err, l_shift, shift_boot, shift_err, l_stat):
        assert isinstance(v, (float, np.floating))
        assert math.isfinite(v)
    # p-values in [0,1]
    assert 0.0 <= l_p  <= 1.0
    return res


def test_accepts_python_lists_and_arrays():
    rng = np.random.default_rng()
    x_list = list(rng.normal(size=50))
    y_arr  = rng.normal(size=60)

    res = ltest(x_list, y_arr, B=200, tol_p=0.1, workers=1)
    _finite_tuple(res)


def test_nan_inf_are_dropped_independently():
    # Clean baseline
    rng = np.random.default_rng()
    x0 = rng.normal(size=500)
    y0 = rng.normal(size=510)

    # Pollute with NaN/Inf at random positions (independent between x and y)
    x_bad = x0.copy()
    y_bad = y0.copy()
    x_bad[3]  = np.nan
    x_bad[17] = np.inf
    y_bad[5]  = -np.inf
    y_bad[12] = np.nan

    res_clean = ltest(x0, y0, B=4000, tol_p=0.005, workers=1)
    res_bad   = ltest(x_bad, y_bad, B=4000, tol_p=0.005, workers=1)

    # Outputs should be finite and very close (dropping the few points barely changes result)
    _finite_tuple(res_clean)
    _finite_tuple(res_bad)
    
    print(abs(res_clean[0] - res_bad[0]))
    # l_stat and l_p should be close; allow small numerical tolerance
    assert abs(res_clean[5] - res_bad[5]) < 0.01      # l_stat
    assert abs(res_clean[0] - res_bad[0]) < 0.1       # l_p evidently this can be fairly large...

    # If this test fails, double-check ltest() uses x = x[np.isfinite(x)] (not ~np.isfinite)


def test_shift_invariance_global_additive():
    rng = np.random.default_rng()
    x = rng.normal(loc=0.0, scale=1.0, size=120)
    y = rng.normal(loc=0.3, scale=1.4, size=130)  # deliberately different shape/loc

    c = 5.7
    res1 = ltest(x, y, B=4000, tol_p=0.005)
    res2 = ltest(x + c, y + c, B=4000, tol_p=0.005)

    _finite_tuple(res1); _finite_tuple(res2)

    # l_stat and l_p should be close (shift-invariant)
    assert abs(res1[5] - res2[5]) < 0.01  # l_stat
    assert abs(res1[0] - res2[0]) < 3e-3  # l_p

    # Estimated shift between distributions should not change much when adding same c to both
    assert abs(res1[2] - res2[2]) < 1e-1  # l_shift


def test_swap_samples_flips_shift_sign():
    rng = np.random.default_rng()
    # Create a clear location difference so sign is stable
    x = rng.normal(loc=0.0, scale=1.0, size=100)
    y = rng.normal(loc=0.8, scale=1.0, size=110)

    res_xy = ltest(x, y, B=4000, tol_p=0.005)
    res_yx = ltest(y, x, B=4000, tol_p=0.005)

    _finite_tuple(res_xy); _finite_tuple(res_yx)

    # l_shift should flip sign when swapping arguments
    assert abs(res_xy[2] + res_yx[2]) < 7e-3  # l_shift_x_y â‰ˆ - l_shift_y_x

    # l_stat is symmetric; p-values should be very close
    assert abs(res_xy[5] - res_yx[5]) < 0.01  # l_stat
    assert abs(res_xy[0] - res_yx[0]) < 1e-2   # l_p (allow MC noise)


def test_ties_and_discrete_data_run_and_bound_pvalues():
    # Many ties: discrete values
    x = np.array([0, 0, 1, 1, 2, 2, 2, 3], dtype=float)
    y = np.array([0, 1, 1, 1, 2, 3, 3, 3, 4], dtype=float)

    res = ltest(x, y, B=2000, tol_p=0.01)
    l_p, l_p_err, l_shift, shift_boot, shift_err, l_stat = _finite_tuple(res)

    # Midranks with ties should still yield valid statistics; p-values within [0,1]
    assert 0.0 <= l_p  <= 1.0


def test_identical_samples_high_pvalues_reasonable_shift():
    rng = np.random.default_rng()
    x = rng.normal(size=150)
    y = x.copy()  # identical

    res = ltest(x, y, B=2000, tol_p=0.01)
    l_p, l_p_err, l_shift, shift_boot, shift_err, l_stat = _finite_tuple(res)

    # Identical samples -> minimal statistic, p-values should be on the higher side
    assert l_p  >= 0.2     # allow MC noise; should not be tiny
    assert abs(l_shift) < 1e-10
    # Uncertainty small-ish (but nonzero due to MC)
    assert shift_err >= 0.0


def test_small_samples_do_not_crash_and_return_finite():
    # Smallest allowable sizes (n=m=2)
    x = np.array([0.0, 1.0])
    y = np.array([0.5, 1.5])

    res = ltest(x, y, B=200, tol_p=0.1)
    _finite_tuple(res)


def test_value_error_on_too_short_samples():
    with pytest.raises(ValueError):
        ltest([1.0], [1.0, 2.0], B=50, workers=1)
    with pytest.raises(ValueError):
        ltest([1.0, 2.0], [np.nan], B=50, workers=1)

# def test_parallel_smoke():
#     rng = np.random.default_rng(0)
#     x = rng.normal(size=200); y = rng.normal(size=220)
#     # small B so startup cost stays tiny
#     res = ltest(x, y, B=50, tol_p=0.5, workers=4)
#     # just check it runs and returns sane values
#     assert 0.0 <= res[0] <= 1.0
        
if __name__ == "__main__":
    sys.exit(pytest.main([__file__, "-q"]))
