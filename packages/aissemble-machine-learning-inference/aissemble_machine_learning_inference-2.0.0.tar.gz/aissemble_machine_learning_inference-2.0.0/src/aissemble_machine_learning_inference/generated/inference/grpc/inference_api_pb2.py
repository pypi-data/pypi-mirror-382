# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: aissemble_machine_learning_inference/generated/inference/grpc/inference_api.proto
# Protobuf Python Version: 6.31.1
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    6,
    31,
    1,
    '',
    'aissemble_machine_learning_inference/generated/inference/grpc/inference_api.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from aissemble_machine_learning_inference.generated.inference.grpc.generated import inference_payload_definition_pb2 as aissemble__machine__learning__inference_dot_generated_dot_inference_dot_grpc_dot_generated_dot_inference__payload__definition__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nQaissemble_machine_learning_inference/generated/inference/grpc/inference_api.proto\x1ajaissemble_machine_learning_inference/generated/inference/grpc/generated/inference_payload_definition.proto\")\n\x10InferenceRequest\x12\x15\n\x04\x64\x61ta\x18\x01 \x01(\x0b\x32\x07.Record\"/\n\x11InferenceResponse\x12\x1a\n\x06result\x18\x01 \x01(\x0b\x32\n.Inference\"B\n\x15\x42\x61tchInferenceRequest\x12\x12\n\nrow_id_key\x18\x01 \x01(\t\x12\x15\n\x04\x64\x61ta\x18\x02 \x03(\x0b\x32\x07.Record\"\xa9\x01\n\x16\x42\x61tchInferenceResponse\x12\x44\n\x07results\x18\x01 \x03(\x0b\x32\x33.BatchInferenceResponse.RecordRowIdAndInferencePair\x1aI\n\x1bRecordRowIdAndInferencePair\x12\x0e\n\x06row_id\x18\x01 \x01(\t\x12\x1a\n\x06result\x18\x02 \x01(\x0b\x32\n.Inference2\x89\x01\n\x10InferenceService\x12\x32\n\x07\x41nalyze\x12\x11.InferenceRequest\x1a\x12.InferenceResponse\"\x00\x12\x41\n\x0c\x41nalyzeBatch\x12\x16.BatchInferenceRequest\x1a\x17.BatchInferenceResponse\"\x00\x62\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'aissemble_machine_learning_inference.generated.inference.grpc.inference_api_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  DESCRIPTOR._loaded_options = None
  _globals['_INFERENCEREQUEST']._serialized_start=193
  _globals['_INFERENCEREQUEST']._serialized_end=234
  _globals['_INFERENCERESPONSE']._serialized_start=236
  _globals['_INFERENCERESPONSE']._serialized_end=283
  _globals['_BATCHINFERENCEREQUEST']._serialized_start=285
  _globals['_BATCHINFERENCEREQUEST']._serialized_end=351
  _globals['_BATCHINFERENCERESPONSE']._serialized_start=354
  _globals['_BATCHINFERENCERESPONSE']._serialized_end=523
  _globals['_BATCHINFERENCERESPONSE_RECORDROWIDANDINFERENCEPAIR']._serialized_start=450
  _globals['_BATCHINFERENCERESPONSE_RECORDROWIDANDINFERENCEPAIR']._serialized_end=523
  _globals['_INFERENCESERVICE']._serialized_start=526
  _globals['_INFERENCESERVICE']._serialized_end=663
# @@protoc_insertion_point(module_scope)
