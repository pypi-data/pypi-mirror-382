{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HCR-NN end-to-end demo (uses **hcr_nn** package)\n",
    "\n",
    "This notebook demonstrates a full pipeline **using your real library**:\n",
    "\n",
    "- empirical CDF normalization via `hcr_nn.layers.CDFNorm`,\n",
    "- conditional model `hcr_nn.models.HCRCond2D` (via `build_hcr_cond2d`),\n",
    "- basis functions from `hcr_nn.basis`,\n",
    "- conditional densities and expectations from `hcr_nn.density`,\n",
    "- (mini demo) `hcr_nn.neuron.HCRNeuron`.\n",
    "\n",
    "We train `HCRCond2D` by minimizing MSE between the predicted **E[u1|u2]** and the empirical **u1** (both in [0,1]).\n",
    "\n",
    "**Run from repo root or from `examples/`** – the path tool below will make `import hcr_nn` work either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & path setup -------------------------------------------------\n",
    "import os, sys, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "\n",
    "np.random.seed(42); random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# Make sure we can import hcr_nn whether we run from repo root or examples/\n",
    "CWD = os.path.abspath(os.getcwd())\n",
    "REPO_ROOT = os.path.abspath(os.path.join(CWD, '..')) if os.path.basename(CWD) == 'examples' else CWD\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "from hcr_nn.layers import CDFNorm\n",
    "from hcr_nn.models import build_hcr_cond2d, HCRCond2D\n",
    "from hcr_nn.basis import select_basis\n",
    "from hcr_nn.density import conditional_density, expected_u1_given_u2\n",
    "from hcr_nn.neuron import HCRNeuron\n",
    "\n",
    "print('hcr_nn imported. Repo root =', REPO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or generate data (x,y), then normalize to [0,1] with empirical CDF\n",
    "\n",
    "We try to load any CSV under `data/` that has columns `x,y`. If none is found, we generate a small correlated dataset and save it so the notebook is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(REPO_ROOT, 'data')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def find_xy_csv(dirpath):\n",
    "    for fn in os.listdir(dirpath):\n",
    "        if fn.lower().endswith('.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(dirpath, fn))\n",
    "                if {'x','y'}.issubset({c.strip().lower() for c in df.columns}):\n",
    "                    return os.path.join(dirpath, fn)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "csv_path = find_xy_csv(DATA_DIR)\n",
    "if csv_path is None:\n",
    "    n = 1000\n",
    "    x = np.random.normal(0, 1, n)\n",
    "    y = 0.6 * x + 0.4 * np.random.normal(0, 1, n)\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    csv_path = os.path.join(DATA_DIR, '_generated_end_to_end.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print('[INFO] generated dataset at', os.path.relpath(csv_path, REPO_ROOT))\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)[['x','y']]\n",
    "    print('[OK] using dataset', os.path.relpath(csv_path, REPO_ROOT))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **empirical CDF** from `hcr_nn.layers.CDFNorm(method='empirical')` to get quantiles `u∈[0,1]` for each column independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = CDFNorm(method='empirical', affine=False, track_running_stats=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_t = torch.tensor(df['x'].to_numpy(), dtype=torch.float32)\n",
    "    y_t = torch.tensor(df['y'].to_numpy(), dtype=torch.float32)\n",
    "    u1 = cdf(x_t)  # quantiles for x\n",
    "    u2 = cdf(y_t)  # quantiles for y\n",
    "\n",
    "u = torch.stack([u1, u2], dim=1)  # (N,2) in [0,1]\n",
    "u[:5], u.min().item(), u.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the conditional model (`HCRCond2D`) with polynomial basis\n",
    "\n",
    "We pass a **quantile function** so the model can consume **raw** inputs \\(x,y\\) and internally map them to \\(u\\in[0,1]\\). Here the quantile function just calls `CDFNorm` column-wise (empirical CDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_fn_raw_to_u(x_raw: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x_raw: (B,2) tensor of raw values\n",
    "    returns: (B,2) quantiles in [0,1]\n",
    "    \"\"\"\n",
    "    # apply EDF per column independently\n",
    "    # For batch calls we reuse the CDFNorm defined above, which is stateless in 'empirical' mode.\n",
    "    col1 = cdf(x_raw[:,0])\n",
    "    col2 = cdf(x_raw[:,1])\n",
    "    return torch.stack([col1, col2], dim=1)\n",
    "\n",
    "# Build the model\n",
    "model: HCRCond2D = build_hcr_cond2d(\n",
    "    degree=8,\n",
    "    basis='polynomial',\n",
    "    grid_size=256,\n",
    "    coeff_init='xavier',\n",
    "    quantile_fn=quantile_fn_raw_to_u,\n",
    "    dtype=torch.float32,\n",
    "    device=None,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: minimize MSE between predicted **E[u1|u2]** and empirical **u1**\n",
    "\n",
    "We'll train only the coefficient matrix `a` inside `HCRCond2D` (parameter `model.coeffs`). Input is raw `[x,y]`, target is `u1` (empirical CDF of `x`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = torch.tensor(df[['x','y']].to_numpy(), dtype=torch.float32)\n",
    "y_target = u[:,0]  # u1\n",
    "\n",
    "# simple train/val split\n",
    "n = X_raw.shape[0]\n",
    "idx = torch.randperm(n)\n",
    "n_train = int(0.8*n)\n",
    "tr, va = idx[:n_train], idx[n_train:]\n",
    "Xtr, Xva = X_raw[tr], X_raw[va]\n",
    "ytr, yva = y_target[tr], y_target[va]\n",
    "\n",
    "opt = optim.Adam([model.coeffs], lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "EPOCHS = 200\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    pred = model(Xtr).reshape(-1)\n",
    "    loss = loss_fn(pred, ytr)\n",
    "    loss.backward(); opt.step()\n",
    "\n",
    "    if ep % 40 == 0 or ep == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val = model(Xva).reshape(-1)\n",
    "            vloss = loss_fn(val, yva).item()\n",
    "        print(f\"epoch {ep:3d} | train MSE = {loss.item():.5f} | val MSE = {vloss:.5f}\")\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize **E[u1|u2]** curve vs. normalized data (u2,u1)\n",
    "\n",
    "We compute the conditional curve using `model.conditional_curve(u2_grid)` and overlay it on the scatter of `(u2,u1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    u2_grid = torch.linspace(0,1,201)\n",
    "    curve = model.conditional_curve(u2_grid)\n",
    "\n",
    "plt.figure(figsize=(6.0,4.0))\n",
    "plt.scatter(u[:,1].numpy(), u[:,0].numpy(), s=8, alpha=0.3, label='data (u2 vs u1)')\n",
    "plt.plot(u2_grid.numpy(), curve.numpy(), lw=2, label='E[u1|u2] (model)')\n",
    "plt.xlabel('u2'); plt.ylabel('u1'); plt.title('Conditional expectation E[u1|u2]')\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-check with `hcr_nn.density`\n",
    "We independently compute conditional densities using your **density utilities** and compare a few expected values with the model curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # use the same basis as model\n",
    "    basis = model.basis  # already configured (polynomial degree)\n",
    "    deg = model.basis_dim - 1\n",
    "    coeffs = model.coeffs.detach()\n",
    "    grid = torch.linspace(0,1,201)\n",
    "    picks = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    approx_e = []\n",
    "    for u2s in picks:\n",
    "        # density-based expectation\n",
    "        e = expected_u1_given_u2(u2s, coeffs, basis, deg, grid_size=201)\n",
    "        approx_e.append(e)\n",
    "    approx_e = np.array(approx_e)\n",
    "    # model's own conditional curve\n",
    "    curve_e = model.conditional_curve(torch.tensor(picks, dtype=torch.float32)).numpy()\n",
    "\n",
    "pd.DataFrame({'u2': picks, 'E_u1_density_py': approx_e, 'E_u1_model_curve': curve_e})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small differences are expected due to grid resolution / numerical details. They should be broadly aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Mini) Using `HCRNeuron` directly\n",
    "A quick taste of your `neuron.HCRNeuron` with a 1D basis (e.g. polynomial with degree=4). We apply it to `u2` just as a demonstration of feature expansion + linear map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcr_nn.basis import PolynomialBasis\n",
    "\n",
    "basis1d = PolynomialBasis(degree=4)\n",
    "neuron = HCRNeuron(basis=basis1d, out_features=2)\n",
    "with torch.no_grad():\n",
    "    demo_out = neuron(u[:,1])  # feed u2\n",
    "demo_out.shape, demo_out[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save artifacts (for CI or to inspect later)\n",
    "We store the trained coefficient matrix and a small JSON with metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_DIR = os.path.join(REPO_ROOT, 'paper_models')\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "np.save(os.path.join(ART_DIR, 'hcr_cond2d_coeffs.npy'), model.coeffs.detach().cpu().numpy())\n",
    "with torch.no_grad():\n",
    "    pred_va = model(Xva).reshape(-1)\n",
    "    val_mse = nn.functional.mse_loss(pred_va, yva).item()\n",
    "with open(os.path.join(ART_DIR, 'hcr_cond2d_metrics.txt'), 'w') as f:\n",
    "    f.write(f'val_mse={val_mse:.6f}\\n')\n",
    "print('Saved: paper_models/hcr_cond2d_coeffs.npy and hcr_cond2d_metrics.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytroch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
