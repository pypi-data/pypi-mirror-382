# Copyright (C) 2024, 2025 Oracle and/or its affiliates.
#
# This software is under the Universal Permissive License
# (UPL) 1.0 (LICENSE-UPL or https://oss.oracle.com/licenses/upl) or Apache License
# 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0), at your option.

"""This module defines several Agent Spec components."""

from typing import ClassVar, List

from pydantic import SerializeAsAny

from pyagentspec.flows.node import Node
from pyagentspec.llms.llmconfig import LlmConfig
from pyagentspec.property import Property, StringProperty
from pyagentspec.templating import get_placeholder_properties_from_string


class LlmNode(Node):
    """
    Execute a prompt template with a given LLM.

    This node is intended to be a part of a Flow.

    - **Inputs**
        One per placeholder in the prompt template.
    - **Outputs**
        The output text generated by the LLM.
    - **Branches**
        One, the default next.

    Example
    -------
    >>> from pyagentspec.property import Property
    >>> from pyagentspec.flows.flow import Flow
    >>> from pyagentspec.flows.edges import ControlFlowEdge, DataFlowEdge
    >>> from pyagentspec.flows.nodes import LlmNode, StartNode, EndNode
    >>> country_property = Property(
    ...     json_schema={"title": "country", "type": "string"}
    ... )
    >>> capital_property = Property(
    ...     json_schema={"title": "capital", "type": "string"}
    ... )
    >>> start_node = StartNode(name="start", inputs=[country_property])
    >>> end_node = EndNode(name="end", outputs=[capital_property])
    >>> llm_node = LlmNode(
    ...     name="simple llm node",
    ...     llm_config=llm_config,
    ...     prompt_template="What is the capital of {{ country }}?",
    ...     inputs=[country_property],
    ...     outputs=[capital_property],
    ... )
    >>> flow = Flow(
    ...     name="Get the country's capital flow",
    ...     start_node=start_node,
    ...     nodes=[start_node, llm_node, end_node],
    ...     control_flow_connections=[
    ...         ControlFlowEdge(name="start_to_llm", from_node=start_node, to_node=llm_node),
    ...         ControlFlowEdge(name="llm_to_end", from_node=llm_node, to_node=end_node),
    ...     ],
    ...     data_flow_connections=[
    ...         DataFlowEdge(
    ...             name="country_edge",
    ...             source_node=start_node,
    ...             source_output="country",
    ...             destination_node=llm_node,
    ...             destination_input="country",
    ...         ),
    ...         DataFlowEdge(
    ...             name="capital_edge",
    ...             source_node=llm_node,
    ...             source_output="capital",
    ...             destination_node=end_node,
    ...             destination_input="capital"
    ...         ),
    ...     ],
    ... )

    """

    llm_config: SerializeAsAny[LlmConfig]
    """The LLM to use for generation in this node."""
    prompt_template: str
    """Defines the prompt sent to the model. Allows placeholders, which can define inputs."""

    DEFAULT_OUTPUT: ClassVar[str] = "generated_text"
    """str: Raw text generated by the LLM."""

    def _get_inferred_inputs(self) -> List[Property]:
        return get_placeholder_properties_from_string(getattr(self, "prompt_template", ""))

    def _get_inferred_outputs(self) -> List[Property]:
        output_title = self.outputs[0].title if self.outputs else self.DEFAULT_OUTPUT
        return [StringProperty(title=output_title, description="Raw text generated by the LLM")]
