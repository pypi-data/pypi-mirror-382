"""Standalone Discovery server for DataBeak using FastMCP server composition.

This module provides a complete Discovery server implementation following DataBeak's modular server
architecture pattern. It focuses on data exploration, profiling, pattern detection, and outlier
analysis with specialized algorithms for data insights.
"""

from __future__ import annotations

import logging
from typing import Annotated, Any, Literal, cast

import numpy as np
import pandas as pd
from fastmcp import Context, FastMCP
from pydantic import BaseModel, Field

from databeak.core.session import get_session_data
from databeak.exceptions import ColumnNotFoundError, InvalidParameterError

# Import session management and data models from the main package
from databeak.models import DataPreview
from databeak.models.tool_responses import (
    BaseToolResponse,
    CellLocation,
    CsvCellValue,
    DataTypeInfo,
    MissingDataInfo,
)

# Import data operations function directly to avoid dependency issues
from databeak.services.data_operations import create_data_preview_with_indices

logger = logging.getLogger(__name__)

# ============================================================================
# PYDANTIC MODELS FOR DISCOVERY OPERATIONS
# ============================================================================


class OutlierInfo(BaseModel):
    """Information about a detected outlier."""

    row_index: int = Field(description="Row index where outlier was detected")
    value: float = Field(description="Outlier value found")
    z_score: float | None = Field(default=None, description="Z-score if using z-score method")
    iqr_score: float | None = Field(default=None, description="IQR score if using IQR method")


class ProfileInfo(BaseModel):
    """Data profiling information for a column."""

    column_name: str = Field(description="Name of the profiled column")
    data_type: str = Field(description="Pandas data type of the column")
    null_count: int = Field(description="Number of null/missing values")
    null_percentage: float = Field(description="Percentage of null values (0-100)")
    unique_count: int = Field(description="Number of unique values")
    unique_percentage: float = Field(description="Percentage of unique values (0-100)")
    most_frequent: CsvCellValue = Field(None, description="Most frequently occurring value")
    frequency: int | None = Field(None, description="Frequency count of most common value")


class GroupStatistics(BaseModel):
    """Statistics for a grouped data segment."""

    count: int = Field(description="Number of records in this group")
    mean: float | None = Field(default=None, description="Mean value for numeric columns")
    sum: float | None = Field(default=None, description="Sum of values for numeric columns")
    min: float | None = Field(default=None, description="Minimum value in the group")
    max: float | None = Field(default=None, description="Maximum value in the group")
    std: float | None = Field(default=None, description="Standard deviation for numeric columns")


class OutliersResult(BaseToolResponse):
    """Response model for outlier detection analysis."""

    outliers_found: int = Field(description="Total number of outliers detected")
    outliers_by_column: dict[str, list[OutlierInfo]] = Field(
        description="Outliers grouped by column name",
    )
    method: Literal["zscore", "iqr", "isolation_forest"] = Field(
        description="Detection method used",
    )
    threshold: float = Field(description="Threshold value used for detection")


class ProfileResult(BaseToolResponse):
    """Response model for comprehensive data profiling."""

    profile: dict[str, ProfileInfo]
    total_rows: int
    total_columns: int
    memory_usage_mb: float
    include_correlations: bool = True
    include_outliers: bool = True


class GroupAggregateResult(BaseToolResponse):
    """Response model for group aggregation operations."""

    groups: dict[str, GroupStatistics]
    group_by_columns: list[str]
    aggregated_columns: list[str]
    total_groups: int


class DataSummaryResult(BaseToolResponse):
    """Response model for data overview and summary."""

    coordinate_system: dict[str, str]
    shape: dict[str, int]
    columns: dict[str, DataTypeInfo]
    data_types: dict[str, list[str]]
    missing_data: MissingDataInfo
    memory_usage_mb: float
    preview: DataPreview | None = None


class FindCellsResult(BaseToolResponse):
    """Response model for cell value search operations."""

    search_value: CsvCellValue
    matches_found: int
    coordinates: list[CellLocation]
    search_column: str | None = None
    exact_match: bool


class InspectDataResult(BaseToolResponse):
    """Response model for contextual data inspection."""

    center_coordinates: dict[str, Any]
    surrounding_data: DataPreview
    radius: int


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


# ============================================================================
# DISCOVERY OPERATIONS LOGIC
# ============================================================================


async def detect_outliers(
    ctx: Annotated[Context, Field(description="FastMCP context for session access")],
    columns: Annotated[
        list[str] | None,
        Field(description="List of numerical columns to analyze for outliers (None = all numeric)"),
    ] = None,
    method: Annotated[
        str,
        Field(description="Detection algorithm: zscore, iqr, or isolation_forest"),
    ] = "iqr",
    threshold: Annotated[
        float,
        Field(description="Sensitivity threshold (higher = less sensitive)"),
    ] = 1.5,
) -> OutliersResult:
    """Detect outliers in numerical columns using various algorithms.

    Identifies data points that deviate significantly from the normal pattern
    using statistical and machine learning methods. Essential for data quality
    assessment and anomaly detection in analytical workflows.

    Returns:
        Detailed outlier analysis with locations and severity scores

    Detection Methods:
        📊 Z-Score: Statistical method based on standard deviations
        📈 IQR: Interquartile range method (robust to distribution)
        🤖 Isolation Forest: ML-based method for high-dimensional data

    Examples:
        # Basic outlier detection
        outliers = await detect_outliers(ctx, ["price", "quantity"])

        # Use IQR method with custom threshold
        outliers = await detect_outliers(ctx, ["sales"],
                                        method="iqr", threshold=2.5)

    AI Workflow Integration:
        1. Data quality assessment and cleaning
        2. Anomaly detection for fraud/error identification
        3. Data preprocessing for machine learning
        4. Understanding data distribution characteristics

    """
    # Get session_id from FastMCP context
    session_id = ctx.session_id
    _session, df = get_session_data(session_id)

    # Select numeric columns
    if columns:
        missing_cols = [col for col in columns if col not in df.columns]
        if missing_cols:
            raise ColumnNotFoundError(missing_cols[0], df.columns.tolist())
        numeric_df = df[columns].select_dtypes(include=[np.number])
    else:
        numeric_df = df.select_dtypes(include=[np.number])

    if numeric_df.empty:
        raise InvalidParameterError(
            "columns",  # noqa: EM101
            columns if columns else "auto-detected",
            "at least one numeric column",
        )

    outliers_by_column = {}
    total_outliers_count = 0

    if method == "iqr":
        for col in numeric_df.columns:
            q1 = numeric_df[col].quantile(0.25)
            q3 = numeric_df[col].quantile(0.75)
            iqr = q3 - q1

            lower_bound = q1 - threshold * iqr
            upper_bound = q3 + threshold * iqr

            outlier_mask = (numeric_df[col] < lower_bound) | (numeric_df[col] > upper_bound)
            outlier_indices = df.index[outlier_mask]

            # Create OutlierInfo objects for each outlier
            outlier_infos = []
            for idx in outlier_indices[:100]:  # Limit to first 100
                raw_value = df.loc[idx, col]
                try:
                    value = float(cast("Any", raw_value))
                except (ValueError, TypeError):
                    continue  # Skip non-numeric values

                # Calculate IQR score (distance from nearest bound relative to IQR)
                if value < lower_bound:
                    iqr_score = float((lower_bound - value) / iqr) if iqr > 0 else 0.0
                else:
                    iqr_score = float((value - upper_bound) / iqr) if iqr > 0 else 0.0

                outlier_infos.append(
                    OutlierInfo(row_index=int(idx), value=value, iqr_score=iqr_score),
                )

            outliers_by_column[col] = outlier_infos
            total_outliers_count += len(outlier_indices)

    elif method == "zscore":
        for col in numeric_df.columns:
            col_mean = numeric_df[col].mean()
            col_std = numeric_df[col].std()
            z_scores = np.abs((numeric_df[col] - col_mean) / col_std)
            outlier_mask = z_scores > threshold
            outlier_indices = df.index[outlier_mask]

            # Create OutlierInfo objects for each outlier
            outlier_infos = []
            for idx in outlier_indices[:100]:  # Limit to first 100
                raw_value = df.loc[idx, col]
                try:
                    value = float(cast("Any", raw_value))
                except (ValueError, TypeError):
                    continue  # Skip non-numeric values

                z_score = float(abs((value - col_mean) / col_std)) if col_std > 0 else 0.0

                outlier_infos.append(
                    OutlierInfo(row_index=int(idx), value=value, z_score=z_score),
                )

            outliers_by_column[col] = outlier_infos
            total_outliers_count += len(outlier_indices)

    else:
        raise InvalidParameterError(
            "method",  # noqa: EM101
            method,
            "zscore, iqr, or isolation_forest",
        )

    # Map method names to match Pydantic model expectations
    if method == "zscore":
        pydantic_method = "zscore"
    elif method == "iqr":
        pydantic_method = "iqr"
    else:
        pydantic_method = "isolation_forest"

    return OutliersResult(
        outliers_found=total_outliers_count,
        outliers_by_column=outliers_by_column,
        method=cast("Literal['zscore', 'iqr', 'isolation_forest']", pydantic_method),
        threshold=threshold,
    )


async def profile_data(
    ctx: Annotated[Context, Field(description="FastMCP context for session access")],
) -> ProfileResult:
    """Generate comprehensive data profile with statistical insights.

    Creates a complete analytical profile of the dataset including column
    characteristics, data types, null patterns, and statistical summaries.
    Provides holistic data understanding for analytical workflows.

    Returns:
        Comprehensive data profile with multi-dimensional analysis

    Profile Components:
        📊 Column Profiles: Data types, null patterns, uniqueness
        📈 Statistical Summaries: Numerical column characteristics
        🔗 Correlations: Inter-variable relationships (optional)
        🎯 Outliers: Anomaly detection across columns (optional)
        💾 Memory Usage: Resource consumption analysis

    Examples:
        # Full data profile
        profile = await profile_data(ctx)

        # Quick profile without expensive computations
        profile = await profile_data(ctx,
                                   include_correlations=False,
                                   include_outliers=False)

    AI Workflow Integration:
        1. Initial data exploration and understanding
        2. Automated data quality reporting
        3. Feature engineering guidance
        4. Data preprocessing strategy development

    """
    # Get session_id from FastMCP context
    session_id = ctx.session_id
    _session, df = get_session_data(session_id)

    # Create ProfileInfo for each column (simplified to match model)
    profile_dict = {}

    for col in df.columns:
        col_data = df[col]

        # Get the most frequent value and its frequency
        value_counts = col_data.value_counts(dropna=False)
        most_frequent = None
        frequency = None
        if len(value_counts) > 0:
            most_frequent = value_counts.index[0]
            frequency = int(value_counts.iloc[0])

            # Handle various data types for most_frequent
            if most_frequent is None or pd.isna(most_frequent):
                most_frequent = None
            elif not isinstance(most_frequent, str | int | float | bool):
                most_frequent = str(most_frequent)

        profile_info = ProfileInfo(
            column_name=col,
            data_type=str(col_data.dtype),
            null_count=int(col_data.isna().sum()),
            null_percentage=round(col_data.isna().sum() / len(df) * 100, 2),
            unique_count=int(col_data.nunique()),
            unique_percentage=round(col_data.nunique() / len(df) * 100, 2),
            most_frequent=most_frequent,
            frequency=frequency,
        )

        profile_dict[col] = profile_info

    # Note: Correlation and outlier analysis have been simplified
    # since the ProfileResult model doesn't include them

    memory_usage_mb = round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2)

    return ProfileResult(
        profile=profile_dict,
        total_rows=len(df),
        total_columns=len(df.columns),
        memory_usage_mb=memory_usage_mb,
    )


async def group_by_aggregate(
    ctx: Annotated[Context, Field(description="FastMCP context for session access")],
    group_by: Annotated[
        list[str],
        Field(description="List of columns to group by for segmentation analysis"),
    ],
    aggregations: Annotated[
        dict[str, list[str]],
        Field(description="Dict mapping column names to list of aggregation functions"),
    ],
) -> GroupAggregateResult:
    """Group data and compute aggregations for analytical insights.

    Performs GROUP BY operations with multiple aggregation functions
    per column. Essential for segmentation analysis and understanding patterns
    across different data groups.

    Returns:
        Grouped aggregation results with statistics per group

    Aggregation Functions:
        📊 count, mean, median, sum, min, max
        📈 std, var (statistical measures)
        🎯 first, last (positional)
        📋 nunique (unique count)

    Examples:
        # Sales analysis by region
        result = await group_by_aggregate(ctx,
                                        group_by=["region"],
                                        aggregations={"sales": ["sum", "mean", "count"]})

        # Multi-dimensional grouping
        result = await group_by_aggregate(ctx,
                                        group_by=["category", "region"],
                                        aggregations={
                                            "price": ["mean", "std"],
                                            "quantity": ["sum", "count"]
                                        })

    AI Workflow Integration:
        1. Segmentation analysis and market research
        2. Feature engineering for categorical interactions
        3. Data summarization for reporting and insights
        4. Understanding group-based patterns and trends

    """
    # Get session_id from FastMCP context
    session_id = ctx.session_id
    _session, df = get_session_data(session_id)

    # Validate group by columns
    missing_cols = [col for col in group_by if col not in df.columns]
    if missing_cols:
        raise ColumnNotFoundError(missing_cols[0], df.columns.tolist())

    # Validate aggregation columns
    agg_cols = list(aggregations.keys())
    missing_agg_cols = [col for col in agg_cols if col not in df.columns]
    if missing_agg_cols:
        raise ColumnNotFoundError(missing_agg_cols[0], df.columns.tolist())

    # Perform groupby to get group statistics
    grouped = df.groupby(group_by)

    # Create GroupStatistics for each group
    group_stats = {}

    for group_name, group_data in grouped:
        # Convert group name to string for dict key
        if isinstance(group_name, tuple):
            group_key = "_".join(str(x) for x in group_name)
        else:
            group_key = str(group_name)

        # Calculate basic statistics for the group
        # Focus on first numeric column for statistics, or count for non-numeric
        numeric_cols = group_data.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) > 0:
            # Use first numeric column for statistics
            first_numeric = group_data[numeric_cols[0]]
            group_stats[group_key] = GroupStatistics(
                count=len(group_data),
                mean=float(first_numeric.mean()) if not pd.isna(first_numeric.mean()) else None,
                sum=float(first_numeric.sum()) if not pd.isna(first_numeric.sum()) else None,
                min=float(first_numeric.min()) if not pd.isna(first_numeric.min()) else None,
                max=float(first_numeric.max()) if not pd.isna(first_numeric.max()) else None,
                std=float(first_numeric.std()) if not pd.isna(first_numeric.std()) else None,
            )
        else:
            # No numeric columns, just provide count
            group_stats[group_key] = GroupStatistics(count=len(group_data))

    return GroupAggregateResult(
        groups=group_stats,
        group_by_columns=group_by,
        aggregated_columns=agg_cols,
        total_groups=len(group_stats),
    )


async def find_cells_with_value(
    ctx: Annotated[Context, Field(description="FastMCP context for session access")],
    value: Annotated[Any, Field(description="The value to search for (any data type)")],
    *,
    columns: Annotated[
        list[str] | None,
        Field(description="List of columns to search (None = all columns)"),
    ] = None,
    exact_match: Annotated[
        bool,
        Field(description="True for exact match, False for substring search"),
    ] = True,
) -> FindCellsResult:
    """Find all cells containing a specific value for data discovery.

    Searches through the dataset to locate all occurrences of a specific value,
    providing coordinates and context. Essential for data validation, quality
    checking, and understanding data patterns.

    Returns:
        Locations of all matching cells with coordinates and context

    Search Features:
        🎯 Exact Match: Precise value matching with type consideration
        🔍 Substring Search: Flexible text-based search for string columns
        📍 Coordinates: Row and column positions for each match
        📊 Summary Stats: Total matches, columns searched, search parameters

    Examples:
        # Find all cells with value "ERROR"
        results = await find_cells_with_value(ctx, "ERROR")

        # Substring search in specific columns
        results = await find_cells_with_value(ctx, "john",
                                            columns=["name", "email"],
                                            exact_match=False)

    AI Workflow Integration:
        1. Data quality assessment and error detection
        2. Pattern identification and data validation
        3. Reference data location and verification
        4. Data cleaning and preprocessing guidance

    """
    # Get session_id from FastMCP context
    session_id = ctx.session_id
    _session, df = get_session_data(session_id)
    matches = []

    # Determine columns to search
    if columns is not None:
        missing_cols = [col for col in columns if col not in df.columns]
        if missing_cols:
            raise ColumnNotFoundError(missing_cols[0], df.columns.tolist())
        columns_to_search = columns
    else:
        columns_to_search = df.columns.tolist()

    # Search for matches
    for col in columns_to_search:
        if exact_match:
            # Exact matching
            if pd.isna(value):
                # Search for NaN values
                mask = df[col].isna()
            else:
                mask = df[col] == value
        # Substring matching (for strings)
        elif isinstance(value, str):
            mask = df[col].astype(str).str.contains(str(value), na=False, case=False)
        else:
            # For non-strings, fall back to exact match
            mask = df[col] == value

        # Get matching row indices
        matching_rows = df.index[mask].tolist()

        for row_idx in matching_rows:
            cell_value = df.loc[row_idx, col]
            # Convert to CsvCellValue compatible type
            processed_value: CsvCellValue
            if pd.isna(cell_value):
                processed_value = None
            elif hasattr(cell_value, "item"):
                item_value = cell_value.item()
                if isinstance(item_value, str | int | float | bool):
                    processed_value = item_value
                else:
                    processed_value = str(item_value)
            elif isinstance(cell_value, str | int | float | bool):
                processed_value = cell_value
            else:
                # Fallback for complex types - convert to string
                processed_value = str(cell_value)

            matches.append(
                CellLocation(
                    row=int(row_idx),
                    column=col,
                    value=processed_value,
                ),
            )

    return FindCellsResult(
        search_value=value,
        matches_found=len(matches),
        coordinates=matches,
        search_column=columns[0] if columns and len(columns) == 1 else None,
        exact_match=exact_match,
    )


async def get_data_summary(
    ctx: Annotated[Context, Field(description="FastMCP context for session access")],
    *,
    include_preview: Annotated[
        bool,
        Field(description="Include sample data rows in summary"),
    ] = True,
    max_preview_rows: Annotated[
        int,
        Field(description="Maximum number of preview rows to include"),
    ] = 10,
) -> DataSummaryResult:
    """Get comprehensive data overview and structural summary.

    Provides high-level overview of dataset structure, dimensions, data types,
    and memory usage. Essential first step in data exploration and analysis
    planning workflows.

    Returns:
        Comprehensive data overview with structural information

    Summary Components:
        📏 Dimensions: Rows, columns, shape information
        🔢 Data Types: Column type distribution and analysis
        💾 Memory Usage: Resource consumption breakdown
        👀 Preview: Sample rows for quick data understanding (optional)
        📊 Overview: High-level dataset characteristics

    Examples:
        # Full data summary with preview
        summary = await get_data_summary(ctx)

        # Structure summary without preview data
        summary = await get_data_summary(ctx, include_preview=False)

    AI Workflow Integration:
        1. Initial data exploration and understanding
        2. Planning analytical approaches based on data structure
        3. Resource planning for large dataset processing
        4. Data quality initial assessment

    """
    # Get session_id from FastMCP context
    session_id = ctx.session_id
    _session, df = get_session_data(session_id)

    # Create coordinate system
    coordinate_system = {
        "row_indexing": f"0 to {len(df) - 1} (0-based)",
        "column_indexing": "Use column names or 0-based indices",
    }

    # Create shape info
    shape = {"rows": len(df), "columns": len(df.columns)}

    # Create DataTypeInfo objects for each column
    columns_info = {}

    for col in df.columns:
        col_dtype = str(df[col].dtype)
        # Map pandas dtypes to Pydantic model literals
        if "int" in col_dtype:
            mapped_dtype = "int64"
        elif "float" in col_dtype:
            mapped_dtype = "float64"
        elif "bool" in col_dtype:
            mapped_dtype = "bool"
        elif "datetime" in col_dtype:
            mapped_dtype = "datetime64"
        elif "category" in col_dtype:
            mapped_dtype = "category"
        else:
            mapped_dtype = "object"

        columns_info[str(col)] = DataTypeInfo(
            type=cast(
                "Literal['int64', 'float64', 'object', 'bool', 'datetime64', 'category']",
                mapped_dtype,
            ),
            nullable=bool(df[col].isna().any()),
            unique_count=int(df[col].nunique()),
            null_count=int(df[col].isna().sum()),
        )

    # Create data types categorization (convert column names to strings)
    data_types = {
        "numeric": [str(col) for col in df.select_dtypes(include=["number"]).columns],
        "text": [str(col) for col in df.select_dtypes(include=["object"]).columns],
        "datetime": [str(col) for col in df.select_dtypes(include=["datetime"]).columns],
        "boolean": [str(col) for col in df.select_dtypes(include=["bool"]).columns],
    }

    # Create missing data info
    total_missing = int(df.isna().sum().sum())
    missing_by_column = {str(col): int(df[col].isna().sum()) for col in df.columns}
    # Handle empty dataframe
    total_cells = len(df) * len(df.columns)
    missing_percentage = round(total_missing / total_cells * 100, 2) if total_cells > 0 else 0.0

    missing_data = MissingDataInfo(
        total_missing=total_missing,
        missing_by_column=missing_by_column,
        missing_percentage=missing_percentage,
    )

    # Create preview
    if include_preview:
        preview_data = create_data_preview_with_indices(df, num_rows=max_preview_rows)
        # Convert to DataPreview object
        preview = DataPreview(
            rows=preview_data.get("records", []),
            row_count=preview_data.get("total_rows", 0),
            column_count=preview_data.get("total_columns", 0),
            truncated=preview_data.get("preview_rows", 0) < preview_data.get("total_rows", 0),
        )
    else:
        preview = None

    # Calculate memory usage
    memory_usage_mb = round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2)

    return DataSummaryResult(
        coordinate_system=coordinate_system,
        shape=shape,
        columns=columns_info,
        data_types=data_types,
        missing_data=missing_data,
        memory_usage_mb=memory_usage_mb,
        preview=preview,
    )


async def inspect_data_around(
    ctx: Annotated[Context, Field(description="FastMCP context for session access")],
    row: Annotated[int, Field(description="Row index to center the inspection (0-based)")],
    column_name: Annotated[str, Field(description="Name of the column to center on")],
    radius: Annotated[
        int,
        Field(description="Number of rows/columns to include around center point"),
    ] = 2,
) -> InspectDataResult:
    """Inspect data around a specific coordinate for contextual analysis.

    Examines the data surrounding a specific cell to understand context,
    patterns, and relationships. Useful for data validation, error investigation,
    and understanding local data patterns.

    Returns:
        Contextual view of data around the specified coordinates

    Inspection Features:
        📍 Center Point: Specified cell as reference point
        🔍 Radius View: Configurable area around center cell
        📊 Data Context: Surrounding values for pattern analysis
        🎯 Coordinates: Clear row/column reference system

    Examples:
        # Inspect around a specific data point
        context = await inspect_data_around(ctx, row=50,
                                          column_name="price", radius=3)

        # Minimal context view
        context = await inspect_data_around(ctx, row=10,
                                          column_name="status", radius=1)

    AI Workflow Integration:
        1. Error investigation and data quality assessment
        2. Pattern recognition in local data areas
        3. Understanding data relationships and context
        4. Validation of data transformations and corrections

    """
    # Get session_id from FastMCP context
    session_id = ctx.session_id
    _session, df = get_session_data(session_id)

    # Handle column specification
    column = column_name
    if isinstance(column, int):
        if column < 0 or column >= len(df.columns):
            raise InvalidParameterError(
                "column_name",  # noqa: EM101
                column,
                f"integer between 0 and {len(df.columns) - 1}",
            )
        column_name = df.columns[column]
        col_index = column
    else:
        if column not in df.columns:
            raise ColumnNotFoundError(column, df.columns.tolist())
        column_name = column
        col_index_result = df.columns.get_loc(column)
        col_index = col_index_result if isinstance(col_index_result, int) else 0

    # Calculate bounds
    row_start = max(0, row - radius)
    row_end = min(len(df), row + radius + 1)
    col_start = max(0, col_index - radius)
    col_end = min(len(df.columns), col_index + radius + 1)

    # Get column slice
    cols_slice = df.columns[col_start:col_end].tolist()

    # Get data slice
    data_slice = df.iloc[row_start:row_end][cols_slice]

    # Convert to records with row indices
    records = []
    for _, (orig_idx, row_data) in enumerate(data_slice.iterrows()):
        # Handle different index types from iterrows safely
        row_index_val = int(orig_idx) if isinstance(orig_idx, int) else 0
        record: dict[str, CsvCellValue] = {"__row_index__": row_index_val}
        record.update(row_data.to_dict())

        # Handle pandas/numpy types
        for key, value in record.items():
            if key == "__row_index__":
                continue
            if pd.isna(value):
                record[key] = None
            elif isinstance(value, pd.Timestamp):
                record[key] = str(value)
            elif hasattr(value, "item"):
                record[key] = value.item()

        records.append(record)

    # Create DataPreview from the records
    surrounding_data = DataPreview(
        rows=records,
        row_count=len(records),
        column_count=len(cols_slice),
        truncated=False,
    )

    return InspectDataResult(
        center_coordinates={"row": row, "column": column_name},
        surrounding_data=surrounding_data,
        radius=radius,
    )


# ============================================================================
# FASTMCP SERVER SETUP
# ============================================================================


# Create Discovery server
discovery_server = FastMCP(
    "DataBeak-Discovery",
    instructions="Data exploration and profiling server for DataBeak with comprehensive discovery and pattern detection capabilities",
)


# Register the data discovery and profiling functions directly as MCP tools
discovery_server.tool(name="detect_outliers")(detect_outliers)
discovery_server.tool(name="profile_data")(profile_data)
discovery_server.tool(name="group_by_aggregate")(group_by_aggregate)
discovery_server.tool(name="find_cells_with_value")(find_cells_with_value)
discovery_server.tool(name="get_data_summary")(get_data_summary)
discovery_server.tool(name="inspect_data_around")(inspect_data_around)
