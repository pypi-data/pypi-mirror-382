Metadata-Version: 2.4
Name: nanowakeword
Version: 1.1.1
Summary: An intelligent framework for automatically training high--performance, custom wake word models.
Author-email: Arcosoph <arcosoph.ai@gmail.com>
License-Expression: Apache-2.0
Project-URL: Homepage, https://github.com/arcosoph/nanowakeword
Project-URL: Bug Tracker, https://github.com/arcosoph/nanowakeword/issues
Keywords: wakeword,keyword-spotting,pytorch,onnx,tflite,speech-recognition,nanowakeword
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: acoustics
Requires-Dist: audiomentations
Requires-Dist: Cython
Requires-Dist: dp
Requires-Dist: Flask
Requires-Dist: importlib_resources
Requires-Dist: librosa
Requires-Dist: matplotlib
Requires-Dist: mutagen
Requires-Dist: numpy
Requires-Dist: onnx
Requires-Dist: onnxruntime
Requires-Dist: pronouncing
Requires-Dist: pyaudio
Requires-Dist: pydub
Requires-Dist: pytorch_lightning
Requires-Dist: PyYAML
Requires-Dist: Requests
Requires-Dist: rich
Requires-Dist: scikit_learn
Requires-Dist: scipy
Requires-Dist: setuptools
Requires-Dist: sounddevice
Requires-Dist: soundfile
Requires-Dist: speechbrain
Requires-Dist: tensorflow
Requires-Dist: torch
Requires-Dist: torch_audiomentations
Requires-Dist: torchaudio
Requires-Dist: torchinfo
Requires-Dist: torchmetrics
Requires-Dist: tqdm
Dynamic: license-file

# NanoWakeWord

### The Intelligent, One-Command Wake Word Model Trainer

**NanoWakeWord is a next-generation, fully automated framework for creating high-performance, custom wake word models. It's not just a tool; it's an intelligent engine that analyzes your data and crafts the perfect training strategy for you.**

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

</div>

---

## Key Features

*   **Intelligent Auto-Configuration:** NanoWakeWord analyzes your dataset's size, quality, and balance, then automatically generates the optimal model architecture and hyperparameters. No more guesswork!
*   **One-Command Training:** Go from raw audio files (in any format) to a fully trained, production-ready model with a single command.
*   **Pro-active Data Harmonizer:** Automatically detects and fixes imbalances in your dataset by synthesizing high-quality positive and negative samples as needed.
*   **Automatic Pre-processing:** Just drop your raw audio files (MP3, M4A, FLAC, etc.) into the data folders. NanoWakeWord handles resampling, channel conversion, and format conversion automatically.
*   **Professional Terminal UI:** A clean, elegant, and informative command-line interface that makes the training process a pleasure to watch.
*   **Flexible & Controllable:** While highly automated, it provides full control to expert users through a clean `training_config.yaml` file.

## Getting Started

### Prerequisites

*   Python 3.8 or higher
*   Git
*   `ffmpeg` (for audio processing)

### Installation

NanoWakeWord is now also available on PyPI!

```bash
pip install nanowakeword
```
*If you want our full code*

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/arcosoph/nanowakeword.git
    cd nanowakeword
    ```

2.  **Create a virtual environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows, use `.venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements_lock_3_13.txt
    ```
    
4.   **FFmpeg:** You must have FFmpeg installed on your system and available in your system's PATH. This is required for automatic audio preprocessing.
*  **On Windows:** Download from [gyan.dev](https://www.gyan.dev/ffmpeg/builds/) and follow their instructions to add it to your PATH.
*  **On macOS (using Homebrew):** `brew install ffmpeg`
*  **On Debian/Ubuntu:** `sudo apt update && sudo apt install ffmpeg`

## ⚙️ Usage

### Quick Start: The One-Command Magic

This is the recommended way for most users.

1.  **Prepare Your Data:** Place your raw audio files (in any format) in the respective subfolders inside `./training_data/` (`positive/`, `negative/`, `noise/`, `rir/`).

```
training_data/
├── positive/         # Contains examples of your wake word (e.g., "hey_nano.wav")
│   ├── sample1.wav
│   └── user_01.mp3
├── negative/         # Contains other speech/sounds that are NOT the wake word
│   ├── not_wakeword1.m4a
│   └── random_speech.wav
├── noise/            # Contains background noise files (e.g., fan, traffic sounds)
│   ├── cafe.flac
│   └── office_noise.aac
├── rir/              # (Optional but recommended) Contains Room Impulse Response files
│   ├── small_room.ogg
│   └── hall.wav
└── fp_val_data.npy   # (Optional) False positive validation data = long audio without wake words. Used to measure FP/hour.
```

2.  **Run the Trainer:** Execute the following command. The engine will handle everything else.

    ```bash
    nanowakeword-train --training_config ./path/to/config.yaml --auto-config --generate_clips --augment_clips --train_model --overwrite
    ```

### Detailed Workflow

The command above performs the following steps automatically:

1.  **Data Pre-processing:** Converts all audio files in your data directories to the required format (16kHz, mono, WAV).
2.  **Intelligent Configuration (`--auto-config`):** Analyzes your dataset and generates an optimal training plan and hyperparameters.
3.  **Synthetic Data Generation (`--generate_clips`):** If the intelligent engine determines a data imbalance, it synthesizes new audio samples to create a robust dataset.
4.  **Augmentation & Feature Extraction (`--augment_clips`):** Creates thousands of augmented audio variations and extracts numerical features for training.
5.  **Model Training (`--train_model`):** Trains the model using the intelligently generated configuration on the prepared features.

### Command-Line Arguments

| Argument            | Description                                                                          |
| ------------------- | ------------------------------------------------------------------------------------ |
| `--training_config` | **Required.** Path to the base `.yaml` configuration file.                           |
| `--auto-config`     | Enables the intelligent engine to automatically determine the best hyperparameters.  |
| `--generate_clips`  | Activates the synthetic data generation step.                                        |
| `--augment_clips`   | Activates the data augmentation and feature extraction step.                         |
| `--train_model`     | Activates the final model training step.                                             |
| `--overwrite`       | If present, overwrites existing feature files during the augmentation step.          |

## Configuration (`training_config.yaml`)

The `config.yaml` file is the central control center. While `--auto-config` handles most settings, you must specify the essential paths.

```yaml
# ==============================================================================
# NanoWakeWord - Complete Configuration File (v1.1.0)
# ==============================================================================
# This file is for users who want manual control over training instead of --auto-config.
# Set each parameter carefully.
# ==============================================================================

# ⚠️ Important: Fields marked as REQUIRED below must be provided.
# Missing or incorrect values ​​may cause training failure or unexpected results

# ===========================================
# Section 1: Paths & Names
# ===========================================

model_name: my_manual_model_v1 # Give a unique name for your model (REQUIRED)
output_dir: ./trained_models    # All outputs (features, models) will be saved here (REQUIRED)

wakeword_data_path: ./training_data/positive #(REQUIRED)
background_data_path: ./training_data/negative #(REQUIRED)

#(Optional) Path to the .npy file used for false positive measurements.
# If you use this, you will need to prepare this file in advance. You can also try the files in the scripts folder for this purpose.
false_positive_validation_data_path: ./training_data/fp_validation_data.npy #(Optional)

# Extra audio files for data augmentation
background_paths: #(REQUIRED)
  - ./training_data/noise
rir_paths: #(REQUIRED)
  - ./training_data/rir
background_paths_duplication_rate:
  - 1

# =======================================================
# Section 2: Data Generation (only for --generate_clips)
# =======================================================

target_phrase: #(REQUIRED)
  - "jarvis" # For generating synthetic data using TTS

# Number of positive and negative samples to generate for training
n_samples_train: 1000
n_samples_negative_train: 1000

# Number of positive and negative samples to generate for validation
n_samples_val: 150
n_samples_negative_val: 150

# Batch size for text-to-speech (TTS)
tts_batch_size: 128


# =======================================================
# Section 3: Data Augmentation (only for --augment_clips)
# =======================================================

# How many times each original audio clip will be used during augmentation.
# More rounds = more variation, but also more time.
augmentation_rounds: 10

# Number of clips processed per batch during augmentation.
augmentation_batch_size: 128


# ===========================================
# Section 4: Model Architecture
# ===========================================

# Model type: "dnn", "lstm", "gru", "cnn", "rnn" #(REQUIRED)
model_type: dnn # Very fast DNN and very best LSTM #(REQUIRED)

# Size of each model layer (number of neurons).
layer_size: 256

# Total number of layers/blocks in the model.
n_blocks: 3


# =================================================
# Section 5: Training Hyperparameters
# =================================================

# Total number of training steps.
steps: 20000

# Dropout rate (to reduce overfitting). Range: 0.0 to 1.0
dropout_prob: 0.5

# Maximum learning rate for CyclicLR scheduler.
learning_rate_max: 0.0001

# Minimum learning rate for CyclicLR scheduler.
learning_rate_base: 0.00001

# Number of steps for the "up" phase of CyclicLR.
# Learning rate increases from min → max within these steps.
clr_step_size_up: 3600

# (Optional) Number of steps for the "down" phase of CyclicLR.
# Learning rate decreases from max → min within these steps.
# If not set, it defaults to clr_step_size_up (auto-config calculates this).
clr_step_size_down: 5500

# Maximum loss weight for negative samples.
max_negative_weight: 3.0

# Your target maximum False Positives per Hour (FP/hour).
target_false_positives_per_hour: 0.5


# =================================================
# Section 6: Batch Composition
# =================================================

batch_composition:
  # Total number of samples per batch during training.
  batch_size: 64

  # Distribution of sample types in each batch (sum must be 100).
  source_distribution:
    positive: 35
    negative_speech: 45
    pure_noise: 20

```

## Performance and Evaluation

NanoWakeWord is designed to produce high-accuracy models with excellent real-world performance. The models can be trained to achieve high recall rates while maintaining an extremely low number of false positives, making them reliable for always-on applications.

## 📥 Pre-trained Models

To help you get started immediately, Nanowakeword provides a pre-trained, high-performance model ready for use. More community-requested models are also on the way!

### Available Now: "Arcosoph"
This is the official flagship model, developed and trained using Nanowakeword itself. It is highly accurate and serves as a perfect example of the quality you can achieve with this engine.

*   **Wake Word:** "Arcosoph" (pronounced *Ar-co-soph*)
*   **Performance:** Achieves a very low false-positive rate (less than one per 10 hours) while maintaining high accuracy.
*   **How to Use:** Download the model files from the [Hugging Face](https://huggingface.co/arcosoph/nanowakeword-lstm-base/tree/main).

### Coming Soon!
We are planning to release more pre-trained models for common wake words based on community feedback. Some of the planned models include:
*   "Hey Computer"
*   "Okay Nano"
*   "Jarvis"

Stay tuned for updates!

## ⚖️ Our Philosophy

In a world of complex machine learning tools, Nanowakeword is built on a simple philosophy:

1.  **Simplicity First**: You shouldn't need a Ph.D. in machine learning to train a high-quality wake word model. We believe in abstracting away the complexity.
2.  **Intelligence over Manual Labor**: The best hyperparameters are data-driven. Our goal is to replace hours of manual tuning with intelligent, automated analysis.
3.  **Performance on the Edge**: Wake word detection should be fast, efficient, and run anywhere. We focus on creating models that are small and optimized for devices like the Raspberry Pi.
4.  **Empowerment Through Open Source**: Everyone should have access to powerful voice technology. By being fully open-source, we empower developers and hobbyists to build the next generation of voice-enabled applications.

## FAQ

**1. Which Python version should I use?**

> The recommended Python version depends on your preferred output format for the trained model:

> * **For `.onnx` models:** You can use **Python 3.8 to 3.13**. This setup has been tested and is fully supported. A lock file for Python 3.13 (`requirements_lock_3_13.txt`) is provided for reference.

> * **For `.tflite` models:** Due to TensorFlow's dependency limitations, it is highly recommended to use versions below **Python 3.11>**. TensorFlow does not yet officially support Python versions newer than 3.11, so conversion to `.tflite` will fail.

**2. What kind of hardware do I need for training?**
> Training is best done on a machine with a dedicated `GPU`, as it can be computationally intensive. However, training on a `CPU` is also possible, although it will be slower. Inference (running the model) is very lightweight and can be run on almost any device, including a Raspberry Pi 3 or 4.

**3. How much data do I need to train a good model?**
> For a good starting point, we recommend at least 400+ clean recordings of your wake words from a few different voices. You can also create synthetic words using NanoWakeWord. The more data you have, the better your model will be. Our intelligent engine is designed to work well even with small datasets.

**4. Can I train a model for a language other than English?**
> Yes! NanoWakeWord is language-agnostic. As long as you can provide audio samples for your wake words, you can train a model for any language.

## Contributing

Contributions are welcome! If you have ideas for new features, bug fixes, or improvements to the "formula engine," please open an issue or submit a pull request.

## License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

## Acknowledgements

* This project stands on the shoulders of giants. It was initially inspired by the architecture and concepts of the `OpenWakeWord` project.
