import logging

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Union

__version__ = "0.1.36"

from .client import AsyncModelRedClient, ModelRedClient
from .config import DEFAULT_BASE_URL, load_settings
from .exceptions import (
    AuthenticationError,
    AuthorizationError,
    ConflictError,
    ModelRedError,
    NetworkError,
    NotFoundError,
    RateLimitError,
    ServerError,
    SubscriptionLimitError,
    ValidationError,
)
from .resources.assessments import (
    Assessment as _ResourceAssessment,
    AssessmentPriority as _ResourcePriority,
    AssessmentStatus as _ResourceStatus,
)
from .resources.models import Model as _ResourceModel, ModelList as _ResourceModelList
from .resources.probes import Probe as Probe, ProbeIndex as ProbesIndex

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logger = logging.getLogger("modelred")
if not logger.handlers:
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(h)
logger.setLevel(logging.INFO)

# -----------------------------------------------------------------------------
# Constants / Enums
# -----------------------------------------------------------------------------
BASE_URL = DEFAULT_BASE_URL


class ModelProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    AZURE = "azure"
    HUGGINGFACE = "huggingface"
    REST = "rest"
    BEDROCK = "bedrock"
    SAGEMAKER = "sagemaker"
    GOOGLE = "google"
    GROK = "grok"
    OPENROUTER = "openrouter"


AssessmentStatus = _ResourceStatus
Priority = _ResourcePriority


# -----------------------------------------------------------------------------
# Data classes
# -----------------------------------------------------------------------------
@dataclass
class Model:
    id: str
    modelId: str
    provider: str
    modelName: Optional[str]
    displayName: str
    description: Optional[str]
    isActive: bool
    lastTested: Optional[datetime]
    testCount: int
    createdAt: Optional[datetime] = None
    updatedAt: Optional[datetime] = None
    createdByUser: Optional[Dict[str, Any]] = None


@dataclass
class ModelList:
    """List of models with pagination metadata."""

    models: List[Model]
    page: int
    pageSize: int
    total: int
    totalPages: int


@dataclass
class Assessment:
    id: str
    modelId: str
    status: AssessmentStatus
    testTypes: List[str]
    priority: Priority
    progress: int = 0
    results: Optional[Dict[str, Any]] = None
    errorMessage: Optional[str] = None
    createdAt: Optional[datetime] = None
    completedAt: Optional[datetime] = None
    estimatedDuration: Optional[int] = None
    detailedReport: Optional[Dict[str, Any]] = None

    @property
    def has_report(self) -> bool:
        """Check if detailed report is available.

        Returns:
            True if detailedReport is not None, False otherwise

        Example:
            if assessment.has_report:
                score = assessment.detailedReport['overall_score']
        """
        return self.detailedReport is not None

    def get_report(self, default: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Safely get detailed report with optional default.

        Args:
            default: Value to return if detailedReport is None (defaults to empty dict)

        Returns:
            The detailed report or default value

        Example:
            report = assessment.get_report()
            score = report.get('overall_score', 0)
        """
        if self.detailedReport is not None:
            return self.detailedReport
        return default if default is not None else {}

    def get_score(self) -> Optional[float]:
        """Safely get overall score from report.

        Returns:
            Overall score as float or None if report not available

        Example:
            score = assessment.get_score()
            if score is not None:
                print(f"Score: {score}/100")
        """
        if self.detailedReport:
            return self.detailedReport.get("overall_score")
        return None


# -----------------------------------------------------------------------------
# Provider config helpers
# -----------------------------------------------------------------------------
class ProviderConfig:
    """Factory methods for creating provider-specific configuration dictionaries.

    Each provider has different requirements:
    - OpenAI: api_key, model_name, optional base_url, optional organization
    - Anthropic: api_key, model_name
    - Azure: api_key, endpoint, deployment_name, api_version
    - OpenRouter: api_key, model_name, optional base_url
    - Google (Gemini): api_key, model_name, optional generation_config/safety_settings
    - Grok (xAI): api_key, model_name
    - HuggingFace: model_name, api_key, use_inference_api, endpoint_url, optional task
    - REST: uri, method, timeout, api_key, headers, ratelimit_codes, skip_codes, request/response templates
    - Bedrock: region, model_id, AWS credentials (access_key_id, secret_access_key), optional session_token/temperature/max_tokens
    - SageMaker: region, endpoint_name, AWS credentials (access_key_id, secret_access_key), optional content_type/accept/timeout_ms, request/response templates

    All methods return a Dict[str, Any] that can be passed as providerConfig
    when creating a model.
    """

    @staticmethod
    def openai(
        api_key: str,
        model_name: str = "gpt-4o-mini",
        organization: Optional[str] = None,
        base_url: Optional[str] = None,
    ) -> Dict[str, Any]:
        """OpenAI configuration.

        Args:
            api_key: OpenAI API key (starts with sk-)
            model_name: Model identifier (e.g. "gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo")
            organization: Optional organization ID
            base_url: Optional base URL for API endpoint (for OpenAI-compatible APIs)

        Raises:
            ValidationError: If api_key is empty or invalid
        """
        if not api_key or not api_key.strip():
            raise ValidationError("OpenAI provider requires a valid api_key")
        if not model_name or not model_name.strip():
            raise ValidationError("OpenAI provider requires a valid model_name")

        cfg = {"api_key": api_key.strip(), "model_name": model_name.strip()}
        if organization:
            cfg["organization"] = organization
        if base_url:
            cfg["base_url"] = base_url
        return cfg

    @staticmethod
    def anthropic(
        api_key: str, model_name: str = "claude-3-5-sonnet-20241022"
    ) -> Dict[str, Any]:
        """Anthropic Claude configuration.

        Args:
            api_key: Anthropic API key
            model_name: Model identifier (e.g. "claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022")

        Raises:
            ValidationError: If api_key is empty or invalid
        """
        if not api_key or not api_key.strip():
            raise ValidationError("Anthropic provider requires a valid api_key")
        if not model_name or not model_name.strip():
            raise ValidationError("Anthropic provider requires a valid model_name")

        return {"api_key": api_key.strip(), "model_name": model_name.strip()}

    @staticmethod
    def azure(
        api_key: str,
        endpoint: str,
        deployment_name: str,
        api_version: str = "2024-06-01",
    ) -> Dict[str, Any]:
        """Azure OpenAI configuration.

        Args:
            api_key: Azure OpenAI API key
            endpoint: Azure OpenAI endpoint URL (e.g. "https://YOUR_RESOURCE.openai.azure.com")
            deployment_name: Azure deployment name
            api_version: Azure API version (default: "2024-06-01")

        Raises:
            ValidationError: If required parameters are empty or invalid
        """
        if not api_key or not api_key.strip():
            raise ValidationError("Azure provider requires a valid api_key")
        if not endpoint or not endpoint.strip():
            raise ValidationError("Azure provider requires a valid endpoint URL")
        if not deployment_name or not deployment_name.strip():
            raise ValidationError("Azure provider requires a valid deployment_name")

        return {
            "api_key": api_key.strip(),
            "endpoint": endpoint.strip(),
            "deployment_name": deployment_name.strip(),
            "api_version": api_version,
        }

    @staticmethod
    def openrouter(
        api_key: str,
        model_name: str,
        base_url: str = "https://openrouter.ai/api/v1",
    ) -> Dict[str, Any]:
        """OpenRouter configuration.

        Args:
            api_key: OpenRouter API key
            model_name: Model identifier (e.g. "anthropic/claude-3.5-sonnet", "google/gemini-pro-1.5")
            base_url: OpenRouter API base URL (default: "https://openrouter.ai/api/v1")
        """
        return {
            "api_key": api_key,
            "model_name": model_name,
            "base_url": base_url,
        }

    @staticmethod
    def grok(
        api_key: str,
        model_name: str = "grok-beta",
    ) -> Dict[str, Any]:
        """xAI Grok configuration.

        Args:
            api_key: xAI API key
            model_name: Model identifier (default: "grok-beta")
        """
        return {
            "api_key": api_key,
            "model_name": model_name,
        }

    @staticmethod
    def google(
        model_name: str,
        api_key: str,
        *,
        generation_config: Optional[Dict[str, Any]] = None,
        safety_settings: Optional[List[Dict[str, Any]]] = None,
        # legacy args kept for compatibility but ignored:
        project_id: Optional[str] = None,
        location: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Google Gemini (Developer API) configuration.

        Args:
            model_name: Model identifier (e.g. "gemini-2.0-flash-exp", "gemini-1.5-pro")
            api_key: Google AI Studio / Developer API key
            generation_config: Optional generation configuration dict
            safety_settings: Optional safety settings list

        Notes:
            - Vertex AI params (project_id, location) are ignored in current version
        """
        if not api_key:
            raise ValidationError("Google (Developer API) requires api_key")

        # Gentle warning if legacy Vertex hints are supplied
        if project_id or location:
            logger.warning(
                "ProviderConfig.google: ignoring Vertex params (project_id/location) "
                "because SDK is in Developer API mode."
            )

        cfg: Dict[str, Any] = {
            "model_name": model_name,
            "api_key": api_key,
        }
        if generation_config is not None:
            cfg["generation_config"] = generation_config
        if safety_settings is not None:
            cfg["safety_settings"] = safety_settings
        return cfg

    @staticmethod
    def huggingface(
        model_name: str,
        api_key: str = "",
        *,
        use_inference_api: bool = True,
        endpoint_url: str = "https://api-inference.huggingface.co/models",
        task: str = "text-generation",
    ) -> Dict[str, Any]:
        """HuggingFace configuration.

        Args:
            model_name: Model identifier (e.g. "meta-llama/Llama-2-7b-chat-hf")
            api_key: HuggingFace API token (optional for public models)
            use_inference_api: Whether to use HF Inference API (default: True)
            endpoint_url: Inference API endpoint URL
            task: Task type (default: "text-generation")

        Returns:
            Configuration dict for HuggingFace models
        """
        return {
            "api_key": api_key,
            "model_name": model_name,
            "use_inference_api": use_inference_api,
            "endpoint_url": endpoint_url,
            "task": task,
        }

    @staticmethod
    def rest(
        uri: str,
        *,
        api_key: Optional[str] = None,
        method: str = "POST",
        headers: Optional[Dict[str, str]] = None,
        req_template: str = "$INPUT",
        req_template_json_object: Optional[Dict[str, Any]] = None,
        response_json: bool = True,
        response_json_field: str = "text",
        request_timeout: int = 20,
        ratelimit_codes: Optional[List[int]] = None,
        skip_codes: Optional[List[int]] = None,
        verify_ssl: bool = True,
        proxies: Optional[Dict[str, str]] = None,
    ) -> Dict[str, Any]:
        """Custom REST API configuration.

        Args:
            uri: API endpoint URL
            api_key: Optional API key
            method: HTTP method (default: "POST")
            headers: Optional HTTP headers dict
            req_template: Request template string where $INPUT is replaced (default: "$INPUT")
            req_template_json_object: Optional JSON request template
            response_json: Whether response is JSON (default: True)
            response_json_field: JSON field containing response text (default: "text")
            request_timeout: Request timeout in seconds (default: 20)
            ratelimit_codes: HTTP codes to treat as rate limits (default: [429])
            skip_codes: HTTP codes to skip (default: [])
            verify_ssl: Whether to verify SSL certificates (default: True)
            proxies: Optional proxy configuration

        Returns:
            Configuration dict for custom REST APIs

        Example:
            config = ProviderConfig.rest(
                uri="https://api.example.com/generate",
                api_key="your-key",
                headers={"Content-Type": "application/json"},
                req_template_json_object={"prompt": "$INPUT", "max_tokens": 100}
            )
        """
        cfg: Dict[str, Any] = {
            "uri": uri,
            "method": method,
            "req_template": req_template,
            "response_json": response_json,
            "response_json_field": response_json_field,
            "request_timeout": request_timeout,
            "ratelimit_codes": ratelimit_codes or [429],
            "skip_codes": skip_codes or [],
            "verify_ssl": verify_ssl,
            "headers": headers or {},
        }
        if api_key:
            cfg["api_key"] = api_key
        if req_template_json_object:
            cfg["req_template_json_object"] = req_template_json_object
        if proxies:
            cfg["proxies"] = proxies
        return cfg

    @staticmethod
    def bedrock(
        region: str,
        model_id: str,
        access_key_id: str,
        secret_access_key: str,
        *,
        session_token: Optional[str] = None,
        temperature: float = 0,
        max_tokens: int = 1024,
    ) -> Dict[str, Any]:
        """AWS Bedrock configuration.

        Args:
            region: AWS region (e.g. "us-east-1")
            model_id: Bedrock model ID (e.g. "anthropic.claude-v2")
            access_key_id: AWS access key ID
            secret_access_key: AWS secret access key
            session_token: Optional AWS session token
            temperature: Model temperature (default: 0)
            max_tokens: Maximum tokens to generate (default: 1024)

        Returns:
            Configuration dict for AWS Bedrock

        Example:
            config = ProviderConfig.bedrock(
                region="us-east-1",
                model_id="anthropic.claude-v2",
                access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"]
            )
        """
        cfg: Dict[str, Any] = {
            "region": region,
            "model_id": model_id,
            "access_key_id": access_key_id,
            "secret_access_key": secret_access_key,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        if session_token:
            cfg["session_token"] = session_token
        return cfg

    @staticmethod
    def sagemaker(
        region: str,
        endpoint_name: str,
        access_key_id: str,
        secret_access_key: str,
        *,
        session_token: Optional[str] = None,
        content_type: str = "application/json",
        accept: str = "application/json",
        request_json_template: Optional[Dict[str, Any]] = None,
        request_text_template: Optional[str] = None,
        response_json_field: str = "generated_text",
        timeout_ms: int = 20000,
    ) -> Dict[str, Any]:
        """AWS SageMaker configuration.

        Args:
            region: AWS region (e.g. "us-east-1")
            endpoint_name: SageMaker endpoint name
            access_key_id: AWS access key ID
            secret_access_key: AWS secret access key
            session_token: Optional AWS session token
            content_type: Request content type (default: "application/json")
            accept: Accept header (default: "application/json")
            request_json_template: Optional JSON request template
            request_text_template: Optional text request template
            response_json_field: JSON field containing response (default: "generated_text")
            timeout_ms: Request timeout in milliseconds (default: 20000)

        Returns:
            Configuration dict for AWS SageMaker

        Example:
            config = ProviderConfig.sagemaker(
                region="us-west-2",
                endpoint_name="my-model-endpoint",
                access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
                request_json_template={"inputs": "$INPUT"}
            )
        """
        cfg: Dict[str, Any] = {
            "region": region,
            "endpoint_name": endpoint_name,
            "access_key_id": access_key_id,
            "secret_access_key": secret_access_key,
            "content_type": content_type,
            "accept": accept,
            "response_json_field": response_json_field,
            "timeout_ms": timeout_ms,
        }
        if session_token:
            cfg["session_token"] = session_token
        if request_json_template:
            cfg["request_json_template"] = request_json_template
        if request_text_template:
            cfg["request_text_template"] = request_text_template
        return cfg


# -----------------------------------------------------------------------------
# Base client
# -----------------------------------------------------------------------------
class ModelRed:
    """Synchronous ModelRed SDK client.

    Security features:
    - DELETE operations are disabled for user safety
    - Base URL is only configurable via MODELRED_BASE_URL environment variable
    - API keys must be provided explicitly or via MODELRED_API_KEY environment variable
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        timeout: int = 30,
        *,
        _client: Optional[ModelRedClient] = None,
    ) -> None:
        """Initialize ModelRed client.

        Args:
            api_key: Your ModelRed API key (or set MODELRED_API_KEY environment variable)
            timeout: Request timeout in seconds (default: 30)

        Security Note:
            The base_url is ONLY configurable via MODELRED_BASE_URL environment variable.
            This prevents malicious code from redirecting your API traffic.
        """
        settings = load_settings(timeout=timeout)
        self._client = _client or ModelRedClient(api_key=api_key, settings=settings)
        self.timeout = timeout
        self.logger = logger
        self.api_key = self._client.api_key

        # User tier will be auto-detected on first probe access
        self._cached_user_tier: Optional[str] = None

    def _headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "User-Agent": f"ModelRed-PythonSDK/{__version__}",
        }

    # Models
    def create_model(
        self,
        *,
        modelId: str,
        provider: Union[str, ModelProvider],
        displayName: str,
        providerConfig: Dict[str, Any],
        description: Optional[str] = None,
    ) -> Model:
        """Create a new model configuration.

        Args:
            modelId: Unique identifier for this model
            provider: Model provider (use ModelProvider enum)
            displayName: Human-readable display name
            providerConfig: Provider-specific configuration (use ProviderConfig helper methods)
            description: Optional model description

        Returns:
            Created Model object

        Example:
            model = client.create_model(
                modelId="my-gpt4",
                provider=ModelProvider.OPENAI,
                displayName="GPT-4",
                providerConfig=ProviderConfig.openai(
                    api_key=os.environ["OPENAI_API_KEY"],
                    model_name="gpt-4o-mini"
                )
            )
        """
        provider_value = (
            provider.value if isinstance(provider, ModelProvider) else str(provider)
        )
        resource = self._client.models.create(
            model_id=modelId,
            provider=provider_value,
            display_name=displayName,
            provider_config=providerConfig,
            description=description,
        )
        return _to_legacy_model(resource)

    def list_models(
        self,
        *,
        page: int = 1,
        page_size: int = 100,
        search: Optional[str] = None,
        provider: Optional[str] = None,
        status: Optional[str] = None,
        sort_by: Optional[str] = None,
        sort_dir: Optional[str] = None,
    ) -> ModelList:
        """List all models in your organization with pagination and filtering.

        Args:
            page: Page number (default: 1)
            page_size: Number of models per page (default: 100, max: 100)
            search: Search term for filtering models by name/ID
            provider: Filter by provider (openai, anthropic, azure, huggingface, rest,
                     bedrock, sagemaker, google, grok, openrouter)
            status: Filter by status ('active', 'inactive', 'both')
            sort_by: Sort field (displayName, provider, modelId, modelName, isActive,
                    testCount, lastTested, createdAt)
            sort_dir: Sort direction ('asc' or 'desc')

        Returns:
            ModelList object containing models and pagination metadata

        Example:
            # Get first page of active OpenAI models
            result = client.list_models(
                page=1,
                page_size=20,
                provider="openai",
                status="active"
            )
            print(f"Found {result.total} models, showing page {result.page}/{result.totalPages}")
            for model in result.models:
                print(f"- {model.displayName}")
        """
        resource_list = self._client.models.list(
            page=page,
            page_size=page_size,
            search=search,
            provider=provider,
            status=status,
            sort_by=sort_by,
            sort_dir=sort_dir,
        )
        return ModelList(
            models=[_to_legacy_model(m) for m in resource_list.models],
            page=resource_list.page,
            pageSize=resource_list.page_size,
            total=resource_list.total,
            totalPages=resource_list.total_pages,
        )

    def get_model(self, model_identifier: str) -> Model:
        """Retrieve a specific model by ID or modelId.

        Args:
            model_identifier: Model ID or modelId

        Returns:
            Model object
        """
        resource = self._client.models.retrieve(model_identifier)
        return _to_legacy_model(resource)

    # Assessments
    def create_assessment(
        self,
        *,
        model: str,
        test_types: Optional[List[str]] = None,
        preset: Optional[str] = None,
        category: Optional[str] = None,
        severity: Optional[str] = None,
        priority: Union[str, Priority] = Priority.NORMAL,
    ) -> Assessment:
        """Create a new security assessment.

        Args:
            model: Model ID or modelId to assess
            test_types: List of probe keys (mutually exclusive with preset/category/severity)
            preset: Preset collection - "quick", "standard", "extended", "comprehensive"
                   (mutually exclusive with test_types/category/severity)
            category: Filter probes by category (mutually exclusive with preset/test_types)
            severity: Filter probes by severity (mutually exclusive with preset/test_types)
            priority: Assessment priority (default: Priority.NORMAL)

        Returns:
            Created Assessment object (status will be QUEUED initially)

        Presets:
            - quick: 20 critical/high severity probes (~5-10 min)
            - standard: 50% of available critical/high probes (~30-45 min)
            - extended: 75% of available critical/high/medium probes (~60-90 min)
            - comprehensive: All available probes (~2-3 hours)

        Examples:
            # Using preset
            assessment = client.create_assessment(
                model="my-gpt4",
                preset="quick"
            )

            # Using category filter
            assessment = client.create_assessment(
                model="my-gpt4",
                category="medical_ethics"
            )

            # Using severity filter
            assessment = client.create_assessment(
                model="my-gpt4",
                severity="critical"
            )

            # Combining category + severity
            assessment = client.create_assessment(
                model="my-gpt4",
                category="medical_ethics",
                severity="critical"
            )

            # Using custom probes
            assessment = client.create_assessment(
                model="my-gpt4",
                test_types=["reverse_psychology", "role_swap"],
                priority=Priority.NORMAL
            )
        """
        # Validate parameters - only one selection method allowed
        selection_methods = sum(
            [bool(preset), bool(test_types), bool(category or severity)]
        )

        if selection_methods == 0:
            raise ValueError(
                "Must specify one of: 'preset', 'test_types', or 'category/severity'"
            )

        if selection_methods > 1:
            raise ValueError(
                "Cannot mix 'preset', 'test_types', and 'category/severity'. "
                "Choose only one selection method."
            )

        # Handle preset
        if preset:
            test_types = self._get_preset_probes(preset)

        # Handle category/severity filtering
        elif category or severity:
            probes = self.list_probes(category=category, severity=severity)
            test_types = [p.key for p in probes.probes]

            if not test_types:
                raise ValueError(
                    f"No probes found for category='{category}', severity='{severity}'. "
                    f"Check available probes with list_probes()."
                )

        priority_value = _normalise_priority(priority)
        resource = self._client.assessments.create(
            model=model,
            test_types=test_types,
            priority=priority_value,
        )
        return _to_legacy_assessment(resource)

    def get_assessment(self, assessment_id: str) -> Assessment:
        """Retrieve assessment status and results.

        Args:
            assessment_id: Assessment ID

        Returns:
            Assessment object with current status and results (if completed)
        """
        resource = self._client.assessments.retrieve(assessment_id)
        return _to_legacy_assessment(resource)

    def list_assessments(self, limit: Optional[int] = None) -> List[Assessment]:
        """List recent assessments.

        Args:
            limit: Maximum number of assessments to return (optional)

        Returns:
            List of Assessment objects
        """
        resources = self._client.assessments.list(limit=limit)
        return [_to_legacy_assessment(a) for a in resources]

    def wait_for_completion(
        self,
        assessment_id: str,
        *,
        timeout_minutes: int = 60,
        poll_interval: int = 10,
        progress_callback: Optional[Callable[[Assessment], None]] = None,
    ) -> Assessment:
        """Wait for assessment to complete, polling for status updates.

        Args:
            assessment_id: Assessment ID
            timeout_minutes: Maximum time to wait (default: 60)
            poll_interval: Seconds between status checks (default: 10)
            progress_callback: Optional callback function called on status changes

        Returns:
            Completed Assessment object with results

        Example:
            def on_progress(assessment):
                print(f"Status: {assessment.status} - Progress: {assessment.progress}%")

            result = client.wait_for_completion(
                assessment_id,
                timeout_minutes=15,
                progress_callback=on_progress
            )
        """
        wrapped = _wrap_sync_callback(progress_callback)
        resource = self._client.assessments.wait_for_completion(
            assessment_id,
            timeout_seconds=timeout_minutes * 60,
            poll_interval=poll_interval,
            progress_callback=wrapped,
        )
        return _to_legacy_assessment(resource)

    # Probes
    def _get_user_tier(self) -> str:
        """Get user's subscription tier.

        The tier is automatically fetched on the first list_probes() call
        and cached for subsequent operations. If not yet fetched, returns "free"
        as the default (most restrictive).

        Returns:
            User's tier: "free", "starter", "pro", or "enterprise"
        """
        # Return cached tier if already set
        if self._cached_user_tier:
            return self._cached_user_tier

        # Default to free tier if not yet fetched
        # Will be updated on first list_probes() call
        return "free"

    def _filter_probes_by_tier(
        self, probes: ProbesIndex, user_tier: str
    ) -> ProbesIndex:
        """Filter probes based on user's subscription tier.

        Tier hierarchy: free < starter < pro < enterprise
        - free: Only "free" tier probes
        - starter: "free" + "starter" tier probes
        - pro: "free" + "starter" + "pro" tier probes
        - enterprise: All probes (no filtering needed)

        Args:
            probes: ProbesIndex with all probes from backend
            user_tier: User's subscription tier

        Returns:
            ProbesIndex with filtered probes based on tier access
        """
        # Define tier access hierarchy
        tier_access = {
            "free": ["free"],
            "starter": ["free", "starter"],
            "pro": ["free", "starter", "pro"],
            "enterprise": ["free", "starter", "pro", "enterprise"],
        }

        allowed_tiers = tier_access.get(user_tier, ["free"])

        # Filter probes to only include tiers user has access to
        filtered_probes = [p for p in probes.probes if p.tier in allowed_tiers]

        # Return new ProbesIndex with filtered list
        return ProbesIndex(
            probes=filtered_probes,
            probe_categories=probes.probe_categories,
            tier_definitions=probes.tier_definitions,
            severity_levels=probes.severity_levels,
        )

    def list_probes(
        self,
        *,
        category: Optional[str] = None,
        severity: Optional[str] = None,
    ) -> ProbesIndex:
        """List all security probes available for your subscription.

        Probes are automatically filtered based on your subscription tier.
        The backend returns all probes but includes your subscription tier
        in the response metadata, which the SDK uses to filter client-side.

        Tier Access:
            - free: Only "free" tier probes (~20 probes)
            - starter: "free" + "starter" tier probes
            - pro: "free" + "starter" + "pro" tier probes
            - enterprise: All probes including "enterprise" tier (200+ probes)

        Args:
            category: Optional category filter (e.g. "universal", "medical_ethics",
                     "legal_ethics", "financial_compliance", "cyber_operations")
            severity: Optional severity filter ("critical", "high", "medium", "low")

        Returns:
            ProbesIndex containing probes available for your subscription tier

        Example:
            # Get all available probes for your subscription
            probes = client.list_probes()
            print(f"Your tier has access to {len(probes.probes)} probes")

            # Filter by category
            medical_probes = client.list_probes(category="medical_ethics")

            # Filter by severity
            critical_probes = client.list_probes(severity="critical")

            # Combine filters
            critical_medical = client.list_probes(
                category="medical_ethics",
                severity="critical"
            )

            # Use probe keys in assessment
            probe_keys = [p.key for p in probes.probes[:5]]
            assessment = client.create_assessment(
                model="my-model",
                test_types=probe_keys
            )
        """
        # Get all probes from backend (returns all 200+ regardless of tier)
        # Backend includes user_tier in the response metadata
        all_probes = self._client.probes.list(
            category=category,
            tier=None,  # Don't filter on backend (we filter client-side)
            severity=severity,
        )

        # Extract and cache user tier from response metadata
        if all_probes.user_tier and not self._cached_user_tier:
            self._cached_user_tier = all_probes.user_tier
            self.logger.debug(f"Detected user tier: {all_probes.user_tier}")

        # Get user's tier (from cache or response)
        user_tier = self._cached_user_tier or all_probes.user_tier or "free"

        # Filter probes client-side based on tier
        filtered_probes = self._filter_probes_by_tier(all_probes, user_tier)

        return filtered_probes

    def _get_preset_probes(self, preset: str) -> List[str]:
        """Get probe keys for a preset based on user's subscription tier.

        Presets are intelligent and adapt to the user's subscription tier.
        The percentages apply to the probes available in YOUR tier:

        - quick: Top 20 critical/high severity probes from your tier
        - standard: 50% of your tier's critical/high probes (minimum 20)
        - extended: 75% of your tier's critical/high/medium probes
        - comprehensive: 100% of all probes available in your tier

        Example:
            Free tier (20 probes):
                - quick: 20 probes (all critical/high)
                - comprehensive: 20 probes (100% of free tier)

            Enterprise tier (200+ probes):
                - quick: 20 probes (top critical/high)
                - comprehensive: 200+ probes (100% of all tiers)

        Args:
            preset: Preset name ("quick", "standard", "extended", "comprehensive")

        Returns:
            List of probe keys appropriate for the preset and user's tier

        Raises:
            ValueError: If preset is invalid
        """
        valid_presets = ["quick", "standard", "extended", "comprehensive"]
        if preset not in valid_presets:
            raise ValueError(
                f"Invalid preset: {preset}. "
                f"Must be one of: {', '.join(valid_presets)}"
            )

        # Get all available probes for this user's subscription tier
        all_probes = self.list_probes()

        if preset == "quick":
            # Top 20 critical/high severity probes
            critical_high = [
                p for p in all_probes.probes if p.severity in ["critical", "high"]
            ]
            # Sort by severity (critical first, then high)
            sorted_probes = sorted(
                critical_high,
                key=lambda p: (0 if p.severity == "critical" else 1, p.key),
            )
            return [p.key for p in sorted_probes[:20]]

        elif preset == "standard":
            # 50% of critical/high severity probes (minimum 20)
            critical_high = [
                p for p in all_probes.probes if p.severity in ["critical", "high"]
            ]
            sorted_probes = sorted(
                critical_high,
                key=lambda p: (0 if p.severity == "critical" else 1, p.key),
            )
            count = max(20, int(len(sorted_probes) * 0.5))
            return [p.key for p in sorted_probes[:count]]

        elif preset == "extended":
            # 75% of critical/high/medium severity probes
            chm = [
                p
                for p in all_probes.probes
                if p.severity in ["critical", "high", "medium"]
            ]
            severity_order = {"critical": 0, "high": 1, "medium": 2}
            sorted_probes = sorted(
                chm, key=lambda p: (severity_order.get(p.severity, 3), p.key)
            )
            count = int(len(sorted_probes) * 0.75)
            return [p.key for p in sorted_probes[:count]]

        elif preset == "comprehensive":
            # All available probes for this user's subscription tier (100%)
            return [p.key for p in all_probes.probes]

        # Should never reach here due to validation above
        raise ValueError(f"Unknown preset: {preset}")

    def redacted_api_key(self) -> str:
        return self._client.redacted_api_key()

    def close(self) -> None:
        self._client.close()

    def __enter__(self) -> "ModelRed":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        self.close()


class AsyncModelRed:
    """Async ModelRed SDK client.

    Security features:
    - DELETE operations are disabled for user safety
    - Base URL is only configurable via MODELRED_BASE_URL environment variable
    - API keys must be provided explicitly or via MODELRED_API_KEY environment variable
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        timeout: int = 30,
        *,
        _client: Optional[AsyncModelRedClient] = None,
    ) -> None:
        """Initialize async ModelRed client.

        Args:
            api_key: Your ModelRed API key (or set MODELRED_API_KEY environment variable)
            timeout: Request timeout in seconds (default: 30)

        Security Note:
            The base_url is ONLY configurable via MODELRED_BASE_URL environment variable.
            This prevents malicious code from redirecting your API traffic.
        """
        settings = load_settings(timeout=timeout)
        self._client = _client or AsyncModelRedClient(
            api_key=api_key, settings=settings
        )
        self.timeout = timeout
        self.logger = logger
        self.api_key = self._client.api_key

        # User tier will be auto-detected on first probe access
        self._cached_user_tier: Optional[str] = None  # Models

    async def create_model(
        self,
        *,
        modelId: str,
        provider: Union[str, ModelProvider],
        displayName: str,
        providerConfig: Dict[str, Any],
        description: Optional[str] = None,
    ) -> Model:
        """Create a new model configuration (async).

        See ModelRed.create_model() for full documentation.
        """
        provider_value = (
            provider.value if isinstance(provider, ModelProvider) else str(provider)
        )
        resource = await self._client.models.create(
            model_id=modelId,
            provider=provider_value,
            display_name=displayName,
            provider_config=providerConfig,
            description=description,
        )
        return _to_legacy_model(resource)

    async def list_models(
        self,
        *,
        page: int = 1,
        page_size: int = 100,
        search: Optional[str] = None,
        provider: Optional[str] = None,
        status: Optional[str] = None,
        sort_by: Optional[str] = None,
        sort_dir: Optional[str] = None,
    ) -> ModelList:
        """List all models in your organization with pagination and filtering (async).

        See ModelRed.list_models() for full documentation.
        """
        resource_list = await self._client.models.list(
            page=page,
            page_size=page_size,
            search=search,
            provider=provider,
            status=status,
            sort_by=sort_by,
            sort_dir=sort_dir,
        )
        return ModelList(
            models=[_to_legacy_model(m) for m in resource_list.models],
            page=resource_list.page,
            pageSize=resource_list.page_size,
            total=resource_list.total,
            totalPages=resource_list.total_pages,
        )

    async def get_model(self, model_identifier: str) -> Model:
        """Retrieve a specific model by ID or modelId (async)."""
        resource = await self._client.models.retrieve(model_identifier)
        return _to_legacy_model(resource)

    # Assessments
    async def create_assessment(
        self,
        *,
        model: str,
        test_types: Optional[List[str]] = None,
        preset: Optional[str] = None,
        category: Optional[str] = None,
        severity: Optional[str] = None,
        priority: Union[str, Priority] = Priority.NORMAL,
    ) -> Assessment:
        """Create a new security assessment (async).

        See ModelRed.create_assessment() for full documentation.
        """
        # Validate parameters - only one selection method allowed
        selection_methods = sum(
            [bool(preset), bool(test_types), bool(category or severity)]
        )

        if selection_methods == 0:
            raise ValueError(
                "Must specify one of: 'preset', 'test_types', or 'category/severity'"
            )

        if selection_methods > 1:
            raise ValueError(
                "Cannot mix 'preset', 'test_types', and 'category/severity'. "
                "Choose only one selection method."
            )

        # Handle preset
        if preset:
            test_types = await self._get_preset_probes(preset)

        # Handle category/severity filtering
        elif category or severity:
            probes = await self.list_probes(category=category, severity=severity)
            test_types = [p.key for p in probes.probes]

            if not test_types:
                raise ValueError(
                    f"No probes found for category='{category}', severity='{severity}'. "
                    f"Check available probes with list_probes()."
                )

        priority_value = _normalise_priority(priority)
        resource = await self._client.assessments.create(
            model=model,
            test_types=test_types,
            priority=priority_value,
        )
        return _to_legacy_assessment(resource)

    async def get_assessment(self, assessment_id: str) -> Assessment:
        """Retrieve assessment status and results (async)."""
        resource = await self._client.assessments.retrieve(assessment_id)
        return _to_legacy_assessment(resource)

    async def list_assessments(self, limit: Optional[int] = None) -> List[Assessment]:
        """List recent assessments (async)."""
        resources = await self._client.assessments.list(limit=limit)
        return [_to_legacy_assessment(a) for a in resources]

    async def wait_for_completion(
        self,
        assessment_id: str,
        *,
        timeout_minutes: int = 60,
        poll_interval: int = 10,
        progress_callback: Optional[Callable[[Assessment], None]] = None,
    ) -> Assessment:
        """Wait for assessment to complete (async).

        See ModelRed.wait_for_completion() for full documentation.
        """
        wrapped = _wrap_async_callback(progress_callback)
        resource = await self._client.assessments.wait_for_completion(
            assessment_id,
            timeout_seconds=timeout_minutes * 60,
            poll_interval=poll_interval,
            progress_callback=wrapped,
        )
        return _to_legacy_assessment(resource)

    async def list_probes(
        self,
        *,
        category: Optional[str] = None,
        severity: Optional[str] = None,
    ) -> ProbesIndex:
        """List all security probes available for your subscription (async).

        See ModelRed.list_probes() for full documentation.
        """
        # Get all probes from backend (returns all 200+ regardless of tier)
        # Backend includes user_tier in the response metadata
        all_probes = await self._client.probes.list(
            category=category,
            tier=None,  # Don't filter on backend (we filter client-side)
            severity=severity,
        )

        # Extract and cache user tier from response metadata
        if all_probes.user_tier and not self._cached_user_tier:
            self._cached_user_tier = all_probes.user_tier
            self.logger.debug(f"Detected user tier: {all_probes.user_tier}")

        # Get user's tier (from cache or response)
        user_tier = self._cached_user_tier or all_probes.user_tier or "free"

        # Filter probes client-side based on tier
        filtered_probes = self._filter_probes_by_tier(all_probes, user_tier)

        return filtered_probes

    def _get_user_tier(self) -> str:
        """Get user's subscription tier (async version).

        The tier is automatically fetched on the first list_probes() call
        and cached for subsequent operations. If not yet fetched, returns "free"
        as the default (most restrictive).

        Returns:
            User's tier: "free", "starter", "pro", or "enterprise"
        """
        # Return cached tier if already set
        if self._cached_user_tier:
            return self._cached_user_tier

        # Default to free tier if not yet fetched
        # Will be updated on first list_probes() call
        return "free"

    def _filter_probes_by_tier(
        self, probes: ProbesIndex, user_tier: str
    ) -> ProbesIndex:
        """Filter probes based on user's subscription tier (reuses sync version).

        Tier hierarchy: free < starter < pro < enterprise
        """
        tier_access = {
            "free": ["free"],
            "starter": ["free", "starter"],
            "pro": ["free", "starter", "pro"],
            "enterprise": ["free", "starter", "pro", "enterprise"],
        }

        allowed_tiers = tier_access.get(user_tier, ["free"])

        filtered_probes = [p for p in probes.probes if p.tier in allowed_tiers]

        return ProbesIndex(
            probes=filtered_probes,
            probe_categories=probes.probe_categories,
            tier_definitions=probes.tier_definitions,
            severity_levels=probes.severity_levels,
        )

    async def _get_preset_probes(self, preset: str) -> List[str]:
        """Get probe keys for a preset based on user's subscription tier (async).

        Presets are intelligent and adapt to the user's subscription tier.
        The percentages apply to the probes available in YOUR tier.

        See ModelRed._get_preset_probes() for full documentation.
        """
        valid_presets = ["quick", "standard", "extended", "comprehensive"]
        if preset not in valid_presets:
            raise ValueError(
                f"Invalid preset: {preset}. "
                f"Must be one of: {', '.join(valid_presets)}"
            )

        # Get all available probes for this user's subscription tier
        all_probes = await self.list_probes()

        if preset == "quick":
            # Top 20 critical/high severity probes
            critical_high = [
                p for p in all_probes.probes if p.severity in ["critical", "high"]
            ]
            sorted_probes = sorted(
                critical_high,
                key=lambda p: (0 if p.severity == "critical" else 1, p.key),
            )
            return [p.key for p in sorted_probes[:20]]

        elif preset == "standard":
            # 50% of critical/high severity probes (minimum 20)
            critical_high = [
                p for p in all_probes.probes if p.severity in ["critical", "high"]
            ]
            sorted_probes = sorted(
                critical_high,
                key=lambda p: (0 if p.severity == "critical" else 1, p.key),
            )
            count = max(20, int(len(sorted_probes) * 0.5))
            return [p.key for p in sorted_probes[:count]]

        elif preset == "extended":
            # 75% of critical/high/medium severity probes
            chm = [
                p
                for p in all_probes.probes
                if p.severity in ["critical", "high", "medium"]
            ]
            severity_order = {"critical": 0, "high": 1, "medium": 2}
            sorted_probes = sorted(
                chm, key=lambda p: (severity_order.get(p.severity, 3), p.key)
            )
            count = int(len(sorted_probes) * 0.75)
            return [p.key for p in sorted_probes[:count]]

        elif preset == "comprehensive":
            # All available probes for this user's subscription tier (100%)
            return [p.key for p in all_probes.probes]

        raise ValueError(f"Unknown preset: {preset}")

    def redacted_api_key(self) -> str:
        return self._client.redacted_api_key()

    async def close(self) -> None:
        await self._client.close()

    async def __aenter__(self) -> "AsyncModelRed":
        await self._client.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc, tb) -> None:
        await self._client.__aexit__(exc_type, exc, tb)


def _normalise_priority(priority: Union[str, Priority]) -> Priority:
    if isinstance(priority, Priority):
        return priority
    return Priority(str(priority).lower())


def _to_legacy_model(model: _ResourceModel) -> Model:
    return Model(
        id=model.id,
        modelId=model.model_id,
        provider=model.provider,
        modelName=model.model_name,
        displayName=model.display_name,
        description=model.description,
        isActive=model.is_active,
        lastTested=model.last_tested,
        testCount=model.test_count,
        createdAt=model.created_at,
        updatedAt=model.updated_at,
        createdByUser=None,
    )


def _to_legacy_assessment(assessment: _ResourceAssessment) -> Assessment:
    return Assessment(
        id=assessment.id,
        modelId=assessment.model_id,
        status=assessment.status,
        testTypes=assessment.test_types,
        priority=assessment.priority,
        progress=assessment.progress,
        results=None,
        errorMessage=None,
        createdAt=assessment.created_at,
        completedAt=assessment.completed_at,
        estimatedDuration=None,
        detailedReport=assessment.detailed_report,
    )


def _wrap_sync_callback(
    callback: Optional[Callable[[Assessment], None]],
) -> Optional[Callable[[_ResourceAssessment], None]]:
    if callback is None:
        return None

    def _inner(resource_assessment: _ResourceAssessment) -> None:
        callback(_to_legacy_assessment(resource_assessment))

    return _inner


def _wrap_async_callback(
    callback: Optional[Callable[[Assessment], None]],
) -> Optional[Callable[[_ResourceAssessment], None]]:
    if callback is None:
        return None

    def _inner(resource_assessment: _ResourceAssessment) -> None:
        callback(_to_legacy_assessment(resource_assessment))

    return _inner


# -----------------------------------------------------------------------------
# Display Helper Functions
# -----------------------------------------------------------------------------
def print_header(title: str, width: int = 80) -> None:
    """Print a formatted header for display output.

    Args:
        title: The title text to display
        width: Width of the header line (default: 80)

    Example:
        print_header("ASSESSMENT RESULTS")
    """
    print("\n" + "=" * width)
    print(f"📊 {title}")
    print("=" * width)


def print_assessment_summary(assessment: Assessment) -> None:
    """Print assessment summary in a formatted table.

    Args:
        assessment: Assessment object to display

    Example:
        assessment = client.get_assessment("assess_123")
        print_assessment_summary(assessment)
    """
    print_header("ASSESSMENT SUMMARY")

    print(f"\n{'Metric':<30} {'Value':<50}")
    print("-" * 80)
    print(f"{'Assessment ID':<30} {assessment.id:<50}")
    print(f"{'Model':<30} {assessment.modelId:<50}")
    print(f"{'Status':<30} {assessment.status:<50}")
    print(f"{'Priority':<30} {assessment.priority:<50}")
    print(f"{'Progress':<30} {assessment.progress}%")
    print(f"{'Total Probes':<30} {len(assessment.testTypes):<50}")
    print(f"{'Created At':<30} {str(assessment.createdAt or 'N/A'):<50}")
    print(f"{'Completed At':<30} {str(assessment.completedAt or 'N/A'):<50}")


def print_detailed_report(report: Optional[Dict[str, Any]]) -> None:
    """Print detailed assessment report in a formatted table.

    Args:
        report: Detailed report dictionary from assessment

    Example:
        assessment = client.get_assessment("assess_123")
        print_detailed_report(assessment.detailedReport)
    """
    if not report:
        print("\n⚠️  No detailed report available yet")
        return

    print_header("DETAILED REPORT")

    print(f"\n{'Metric':<40} {'Value':<40}")
    print("-" * 80)

    # Try snake_case first (current API format), then fall back to camelCase (legacy)
    overall_score = report.get("overall_score") or report.get("overallScore", "N/A")
    total_tests = report.get("total_tests") or report.get("totalTests", "N/A")
    passed_tests = report.get("passed_tests") or report.get("passedTests", "N/A")
    failed_tests = report.get("failed_tests") or report.get("failedTests", "N/A")

    print(f"{'Overall Score':<40} {overall_score:<40}")
    print(f"{'Total Tests':<40} {total_tests:<40}")
    print(f"{'Passed Tests':<40} {passed_tests:<40}")
    print(f"{'Failed Tests':<40} {failed_tests:<40}")

    # These fields may not be in the current API response, but keep for compatibility
    critical_issues = report.get("critical_issues") or report.get(
        "criticalIssues", "N/A"
    )
    high_severity = report.get("high_severity_issues") or report.get(
        "highSeverityIssues", "N/A"
    )
    medium_severity = report.get("medium_severity_issues") or report.get(
        "mediumSeverityIssues", "N/A"
    )
    low_severity = report.get("low_severity_issues") or report.get(
        "lowSeverityIssues", "N/A"
    )

    print(f"{'Critical Issues':<40} {critical_issues:<40}")
    print(f"{'High Severity Issues':<40} {high_severity:<40}")
    print(f"{'Medium Severity Issues':<40} {medium_severity:<40}")
    print(f"{'Low Severity Issues':<40} {low_severity:<40}")


def print_assessments_list(
    assessments: List[Assessment], title: str = "ALL ASSESSMENTS"
) -> None:
    """Print a list of assessments in a formatted table.

    Args:
        assessments: List of Assessment objects to display
        title: Title for the table (default: "ALL ASSESSMENTS")

    Example:
        assessments = client.list_assessments(model="my-model")
        print_assessments_list(assessments, "ASSESSMENTS FOR MY MODEL")
    """
    print_header(title)

    print(
        f"\n{'ID':<25} {'Status':<15} {'Progress':<10} {'Probes':<10} {'Priority':<10}"
    )
    print("-" * 80)
    for a in assessments:
        print(
            f"{a.id:<25} {a.status:<15} {a.progress:>8}% {len(a.testTypes):>8} {a.priority:<10}"
        )


def print_model_info(model: Model) -> None:
    """Print model information in a formatted table.

    Args:
        model: Model object to display

    Example:
        model = client.get_model("my-model")
        print_model_info(model)
    """
    print_header("MODEL INFORMATION")

    print(f"\n{'Field':<30} {'Value':<50}")
    print("-" * 80)
    print(f"{'Model ID':<30} {model.modelId:<50}")
    print(f"{'Display Name':<30} {model.displayName:<50}")
    print(f"{'Provider':<30} {model.provider:<50}")
    print(f"{'Model Name':<30} {model.modelName or 'N/A':<50}")
    print(f"{'Description':<30} {model.description or 'N/A':<50}")
    print(f"{'Active':<30} {'✅ Yes' if model.isActive else '❌ No':<50}")
    print(f"{'Test Count':<30} {model.testCount:<50}")


def progress_tracker(assessment: Assessment) -> None:
    """Simple progress callback for wait_for_completion.

    Args:
        assessment: Current assessment state

    Example:
        client.wait_for_completion(
            assessment_id,
            timeout_minutes=30,
            poll_interval=5,
            progress_callback=progress_tracker
        )
    """
    status_emoji = {"queued": "⏳", "running": "🔄", "completed": "✅", "failed": "❌"}
    emoji = status_emoji.get(assessment.status, "📊")
    print(f"   {emoji} Progress: {assessment.progress}% - Status: {assessment.status}")


def print_full_results(client: "ModelRed", assessment_id: str) -> None:
    """Print complete assessment results with all details.

    Args:
        client: ModelRed client instance
        assessment_id: Assessment ID to fetch and display

    Example:
        print_full_results(client, "assess_123")
    """
    assessment = client.get_assessment(assessment_id)

    print_assessment_summary(assessment)

    if assessment.detailedReport:
        print_detailed_report(assessment.detailedReport)

    print("\n" + "=" * 80)


# -----------------------------------------------------------------------------
# Public exports
# -----------------------------------------------------------------------------
__all__ = [
    "ModelRed",
    "AsyncModelRed",
    "ModelRedClient",
    "AsyncModelRedClient",
    "Model",
    "ModelList",
    "Assessment",
    "ModelProvider",
    "AssessmentStatus",
    "Priority",
    "ProviderConfig",
    # Display helpers
    "print_header",
    "print_assessment_summary",
    "print_detailed_report",
    "print_assessments_list",
    "print_model_info",
    "progress_tracker",
    "print_full_results",
    # Exceptions
    "ModelRedError",
    "AuthenticationError",
    "AuthorizationError",
    "SubscriptionLimitError",
    "ValidationError",
    "NotFoundError",
    "ConflictError",
    "RateLimitError",
    "ServerError",
    "NetworkError",
]
