{
  "name": "deepseek-r1",
  "display_name": "Deepseek R1",
  "description": "A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)",
  "repository": {
    "type": "git",
    "url": "https://github.com/66julienmartin/MCP-server-Deepseek_R1"
  },
  "homepage": "https://github.com/66julienmartin/MCP-server-Deepseek_R1",
  "author": {
    "name": "66julienmartin",
    "url": "https://github.com/66julienmartin"
  },
  "license": "MIT",
  "categories": [
    "AI Systems"
  ],
  "tags": [
    "Deepseek",
    "LLM"
  ],
  "installations": {
    "npm": {
      "type": "npm",
      "command": "npx",
      "args": [
        "-y",
        "https://github.com/66julienmartin/MCP-server-Deepseek_R1"
      ],
      "env": {
        "DEEPSEEK_API_KEY": "${DEEPSEEK_API_KEY}"
      }
    }
  },
  "arguments": {
    "DEEPSEEK_API_KEY": {
      "description": "API key for authenticating with the Deepseek service.",
      "required": true,
      "example": "your-api-key"
    }
  },
  "tools": [
    {
      "name": "deepseek_r1",
      "description": "Generate text using DeepSeek R1 model",
      "inputSchema": {
        "type": "object",
        "properties": {
          "prompt": {
            "type": "string",
            "description": "Input text for DeepSeek"
          },
          "max_tokens": {
            "type": "number",
            "description": "Maximum tokens to generate (default: 8192)",
            "minimum": 1,
            "maximum": 8192
          },
          "temperature": {
            "type": "number",
            "description": "Sampling temperature (default: 0.2)",
            "minimum": 0,
            "maximum": 2
          }
        },
        "required": [
          "prompt"
        ]
      }
    }
  ]
}