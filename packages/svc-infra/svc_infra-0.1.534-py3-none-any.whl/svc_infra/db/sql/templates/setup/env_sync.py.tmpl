from __future__ import annotations

import os
import logging
from typing import List, Tuple
import sys, pathlib, importlib, pkgutil, traceback

from alembic import context
from sqlalchemy.engine import make_url, URL

from svc_infra.db.sql.utils import (
    _coerce_sync_driver,
    _ensure_ssl_default,
    get_database_url_from_env,
    build_engine,
)

try:
    from svc_infra.db.sql.types import GUID as _GUID  # type: ignore
except Exception:
    _GUID = None


def _render_item(type_, obj, autogen_context):
    if type_ == "type" and (
        (_GUID is not None and isinstance(obj, _GUID))
        or getattr(obj, "__class__", None).__name__ == "GUID"
    ):
        autogen_context.imports.add("from svc_infra.db.sql.types import GUID")
        return "GUID()"
    return False


# ---- Logging ----
config = context.config
if config.config_file_name is not None:
    import logging.config as _lc
    _lc.fileConfig(config.config_file_name)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # ensure our INFO messages show up


# ---- sys.path bootstrap (works even without alembic.ini) ----
prepend = config.get_main_option("prepend_sys_path") or ""
script_loc = config.get_main_option("script_location") or os.path.dirname(__file__)
migrations_dir = pathlib.Path(script_loc).resolve()
project_root = migrations_dir.parent

def _ensure_on_syspath(p: pathlib.Path) -> None:
    s = str(p)
    if s and s not in sys.path:
        sys.path.insert(0, s)

if prepend:
    _ensure_on_syspath(pathlib.Path(prepend))
    src_path = pathlib.Path(prepend) / "src"
    if src_path.exists():
        _ensure_on_syspath(src_path)

# Always add derived roots, too
_ensure_on_syspath(project_root)
if (project_root / "src").exists():
    _ensure_on_syspath(project_root / "src")


# ---- robust x-arg parsing (all Alembic versions) ----
def _x_args_dict() -> dict:
    try:
        return context.get_x_argument(as_dictionary=True)  # type: ignore[arg-type]
    except TypeError:
        try:
            xs = context.get_x_argument()
        except TypeError:
            xs = []
    out: dict = {}
    for item in xs:
        if "=" in item:
            k, v = item.split("=", 1)
            out[k] = v
        else:
            out[item] = ""
    return out


# ---- Resolve DB URL (priority: -x -> env -> helper -> ini) ----
_x = _x_args_dict()
cli_dburl = _x.get("dburl", "").strip()
env_dburl = os.getenv("SQL_URL", "").strip()
try:
    helper_dburl = get_database_url_from_env(required=False) or ""
except Exception:
    helper_dburl = ""
ini_dburl = (config.get_main_option("sqlalchemy.url") or "").strip()

effective_url = cli_dburl or env_dburl or helper_dburl or ini_dburl
if not effective_url:
    raise RuntimeError(
        "No database URL found. Provide via:\n"
        "  • env: SQL_URL=...\n"
        "  • or CLI: alembic -x dburl=...\n"
        "  • or alembic.ini [sqlalchemy.url]\n"
    )

u = make_url(effective_url)
u = _coerce_sync_driver(u)
u = _ensure_ssl_default(u)
config.set_main_option("sqlalchemy.url", u.render_as_string(hide_password=False))


# ---- metadata discovery (prefer ModelBase; strong diagnostics) ----
DISCOVER_PACKAGES: List[str] = ['svc_infra.apf_payments.models']  # seed; merged with FS + env
ENV_DISCOVER = os.getenv("ALEMBIC_DISCOVER_PACKAGES")
if ENV_DISCOVER:
    DISCOVER_PACKAGES = [s.strip() for s in ENV_DISCOVER.split(",") if s.strip()]

def _collect_metadata() -> list[object]:
    """
    Discover SQLAlchemy MetaData objects for Alembic autogenerate.

    Strategy:
      1) Force-import known-infra packages (payments) + any in ALEMBIC_DISCOVER_PACKAGES.
      2) ALSO import top-level packages under project_root and project_root/src.
      3) Import common model-bearing modules: "<pkg>.models", "<pkg>.db", "<pkg>.orm", "<pkg>.entities".
         Additionally attempt importing bare 'models' if it exists on sys.path.
      4) Prefer svc_infra.db.sql.base.ModelBase.metadata after imports.
      5) Collect module-level `metadata` / `Base` / `MetaData` as extras.
      6) De-dup and keep only those with at least one table.
    """
    tried: list[Tuple[str, str]] = []      # (name, "ok"|"err")
    errors: list[Tuple[str, str]] = []     # (name, traceback)
    found: list[object] = []

    def _note(name: str, ok: bool, err: str | None = None):
        tried.append((name, "ok" if ok else "err"))
        if not ok and err:
            errors.append((name, err))

    def _maybe_add(obj: object) -> None:
        md = getattr(obj, "metadata", None) or obj
        if hasattr(md, "tables") and hasattr(md, "schema"):
            found.append(md)

    # 1) Seed with baked + env-provided
    pkgs: list[str] = []
    for p in list(DISCOVER_PACKAGES or []):
        if p and p not in pkgs:
            pkgs.append(p)
    env_pkgs = os.getenv("ALEMBIC_DISCOVER_PACKAGES", "")
    if env_pkgs:
        for p in (x.strip() for x in env_pkgs.split(",") if x.strip()):
            if p not in pkgs:
                pkgs.append(p)

    # 2) Merge filesystem discovery (project_root + src)
    fs_roots: list[pathlib.Path] = []
    for candidate in {project_root, project_root / "src"}:
        if candidate.exists():
            fs_roots.append(candidate)
    for root in fs_roots:
        for p in root.iterdir():
            if p.is_dir() and (p / "__init__.py").exists():
                name = p.name
                if name not in pkgs:
                    pkgs.append(name)

    # Always try a bare 'models' module too (common in apps)
    pkgs = pkgs + [m for m in ["models"] if m not in pkgs]

    # Import each package, then scan common submodules
    for pkg_name in pkgs:
        try:
            pkg = importlib.import_module(pkg_name)
            _note(pkg_name, True, None)
        except Exception:
            _note(pkg_name, False, traceback.format_exc())
            continue

        for attr in ("metadata", "MetaData", "Base", "base"):
            obj = getattr(pkg, attr, None)
            if obj is not None:
                _maybe_add(obj)

        for subname in ("models", "db", "orm", "entities"):
            modname = f"{pkg_name}.{subname}" if "." in pkg_name or pkg_name != "models" else subname
            try:
                sub = importlib.import_module(modname)
                _note(modname, True, None)
            except Exception:
                _note(modname, False, traceback.format_exc())
                sub = None
            if sub is not None:
                for attr in ("metadata", "MetaData", "Base", "base"):
                    obj = getattr(sub, attr, None)
                    if obj is not None:
                        _maybe_add(obj)

        mod_path = getattr(pkg, "__path__", None)
        if not mod_path:
            continue
        for _, name, ispkg in pkgutil.walk_packages(mod_path, prefix=pkg_name + "."):
            if ispkg:
                continue
            if not any(x in name for x in (".models", ".db", ".orm", ".entities")):
                continue
            try:
                mod = importlib.import_module(name)
                _note(name, True, None)
            except Exception:
                _note(name, False, traceback.format_exc())
                continue
            for attr in ("metadata", "MetaData", "Base", "base"):
                obj = getattr(mod, attr, None)
                if obj is not None:
                    _maybe_add(obj)

    # 4) Prefer ModelBase.metadata after all imports (ensures tables registered)
    try:
        from svc_infra.db.sql.base import ModelBase  # type: ignore
        mb_md = getattr(ModelBase, "metadata", None)
        if mb_md is not None and hasattr(mb_md, "tables") and mb_md.tables:
            found.append(mb_md)
            _note("ModelBase.metadata", True, None)
        else:
            _note("ModelBase.metadata(empty)", True, None)
    except Exception:
        _note("ModelBase import", False, traceback.format_exc())

    # 5) Optional: autobind API key model
    try:
        from svc_infra.db.sql.apikey import try_autobind_apikey_model
        try_autobind_apikey_model(require_env=False)
        _note("svc_infra.db.sql.apikey.try_autobind_apikey_model", True, None)
    except Exception:
        _note("svc_infra.db.sql.apikey.try_autobind_apikey_model", False, traceback.format_exc())

    # 6) De-dup with at least one table
    uniq: list[object] = []
    seen: set[int] = set()
    for md in found:
        try:
            if not (hasattr(md, "tables") and md.tables):
                continue
        except Exception:
            continue
        if id(md) not in seen:
            seen.add(id(md))
            uniq.append(md)

    # Diagnostics
    total_tables = 0
    try:
        total_tables = sum(len(getattr(md, "tables", {})) for md in uniq)
    except Exception:
        pass

    context.config.print_stdout(
        f"[alembic env] discovered {len(uniq)} metadata objects with {total_tables} tables total"
    )
    if total_tables == 0:
        context.config.print_stdout("[alembic env] import attempts (ok/err):")
        for name, status in tried:
            context.config.print_stdout(f"  - {status:3s}  {name}")
        # Only print tracebacks for failures (keeps noise manageable)
        for name, tb in errors[:10]:
            context.config.print_stdout(f"  --- import error: {name} ---")
            context.config.print_stdout(tb)

    return uniq


target_metadata = _collect_metadata()


def _want_include_schemas() -> bool:
    # explicit opt-in always wins
    val = _x.get("include_schemas", "") or os.getenv("ALEMBIC_INCLUDE_SCHEMAS", "")
    if str(val).strip() in {"1", "true", "True", "yes"}:
        return True
    # auto if any table has a non-default schema
    try:
        for md in (target_metadata or []):
            for t in getattr(md, "tables", {}).values():
                if getattr(t, "schema", None):
                    return True
    except Exception:
        pass
    return False


def _system_schemas_for(url: str) -> set[str]:
    try:
        backend = make_url(url).get_backend_name().lower()
    except Exception:
        backend = ""
    if backend in {"mysql", "mariadb"}:
        return {"mysql", "performance_schema", "information_schema", "sys"}
    if backend in {"postgresql", "postgres"}:
        return {"pg_catalog", "information_schema"}
    if backend in {"mssql"}:
        return {"INFORMATION_SCHEMA", "sys"}
    if backend in {"snowflake"}:
        return {"INFORMATION_SCHEMA"}
    return set()


def _include_object_factory(url: str):
    """
    Include-object hook with optional guard against dropping tables that exist
    in the DB but are missing from metadata. Set ALEMBIC_SKIP_DROPS=1 to enable.
    """
    sys_schemas = _system_schemas_for(url)
    skip_drops = os.getenv("ALEMBIC_SKIP_DROPS", "").lower() in {"1", "true", "yes"}

    def _include_object(obj, name, type_, reflected, compare_to):
        schema = getattr(obj, "schema", None)
        if schema and str(schema) in sys_schemas:
            return False
        # Keep Alembic version table
        version_table = (
            context.get_x_argument(as_dictionary=True).get("version_table")
            if hasattr(context, "get_x_argument")
            else None
        ) or os.getenv("ALEMBIC_VERSION_TABLE", "alembic_version")
        if type_ == "table" and name == version_table:
            return True
        # Guard: don't drop tables that exist in DB but aren't in metadata
        if skip_drops and type_ == "table" and reflected and compare_to is None:
            return False
        return True

    return _include_object


def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
        include_schemas=_want_include_schemas(),
        include_object=_include_object_factory(url),
        version_table=os.getenv("ALEMBIC_VERSION_TABLE", "alembic_version"),
        version_table_schema=os.getenv("ALEMBIC_VERSION_SCHEMA") or None,
        render_item=_render_item,
    )
    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    url = config.get_main_option("sqlalchemy.url")
    engine = build_engine(url, echo=False)
    with engine.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
            include_schemas=_want_include_schemas(),
            include_object=_include_object_factory(url),
            version_table=os.getenv("ALEMBIC_VERSION_TABLE", "alembic_version"),
            version_table_schema=os.getenv("ALEMBIC_VERSION_SCHEMA") or None,
            render_item=_render_item,
        )
        with context.begin_transaction():
            context.run_migrations()
    engine.dispose()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
