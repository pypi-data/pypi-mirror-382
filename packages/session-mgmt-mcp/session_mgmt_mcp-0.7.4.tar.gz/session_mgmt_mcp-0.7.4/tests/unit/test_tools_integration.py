"""Integration tests for crackerjack:run workflow."""

import pytest
from datetime import datetime, timedelta
from session_mgmt_mcp.tools.agent_analyzer import AgentAnalyzer, AgentType
from session_mgmt_mcp.tools.quality_metrics import QualityMetricsExtractor, QualityMetrics
from session_mgmt_mcp.tools.recommendation_engine import RecommendationEngine


class MockReflectionDatabase:
    """Mock ReflectionDatabase for integration testing."""

    def __init__(self, mock_results: list[dict]):
        self.mock_results = mock_results
        self.stored_conversations: list[dict] = []

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass

    async def search_conversations(
        self,
        query: str,
        project: str | None = None,
        limit: int = 50,
        min_score: float = 0.7,
    ) -> list[dict]:
        """Return mock results."""
        return self.mock_results

    async def store_conversation(self, conversation: dict) -> None:
        """Store conversation for verification."""
        self.stored_conversations.append(conversation)


class TestEndToEndWorkflow:
    """Test complete crackerjack:run workflow integration."""

    @pytest.mark.asyncio
    async def test_complete_workflow_with_failure_and_fix(self):
        """Test full workflow: failure detection → recommendation → learning → fix."""
        # Simulate crackerjack execution with complexity violation
        stdout = ""
        stderr = "Complexity of 20 is too high (threshold: 15)"
        exit_code = 1

        # Step 1: Extract quality metrics
        metrics = QualityMetricsExtractor.extract(stdout, stderr)
        assert metrics.max_complexity == 20
        assert metrics.complexity_violations == 1

        # Step 2: Get agent recommendations
        recommendations = AgentAnalyzer.analyze(stdout, stderr, exit_code)
        assert len(recommendations) == 1
        assert recommendations[0].agent == AgentType.REFACTORING
        assert recommendations[0].confidence == 0.9

        # Step 3: Simulate historical learning (previous successful fix)
        mock_history = [
            {
                "timestamp": (datetime.now() - timedelta(days=2)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"complexity_violations": 1, "max_complexity": 18},
                    "agent_recommendations": [
                        {
                            "agent": "RefactoringAgent",
                            "confidence": 0.9,
                            "reason": "Complexity violation",
                        }
                    ],
                },
            },
            {
                "timestamp": (datetime.now() - timedelta(days=2, hours=-1)).isoformat(),
                "content": "Crackerjack execution: Success",
                "metadata": {"exit_code": 0, "execution_time": 45.2},
            },
        ]

        db = MockReflectionDatabase(mock_history)
        async with db:
            history_analysis = await RecommendationEngine.analyze_history(
                db, project="test-project", days=7, use_cache=False
            )

        # Verify pattern detection
        assert len(history_analysis["patterns"]) == 1
        pattern = history_analysis["patterns"][0]
        assert "complexity" in pattern.pattern_signature
        assert AgentType.REFACTORING in pattern.successful_fixes

        # Verify agent effectiveness
        effectiveness = history_analysis["agent_effectiveness"][0]
        assert effectiveness.agent == AgentType.REFACTORING
        assert effectiveness.success_rate == 1.0  # 100% success in history

        # Step 4: Adjust confidence based on historical effectiveness
        adjusted_recommendations = RecommendationEngine.adjust_confidence(
            recommendations, history_analysis["agent_effectiveness"]
        )

        # Confidence should NOT change - only 1 historical recommendation (<5 minimum)
        # RecommendationEngine requires ≥5 samples before adjusting
        assert adjusted_recommendations[0].confidence == 0.9  # Unchanged

    @pytest.mark.asyncio
    async def test_workflow_with_multiple_issues(self):
        """Test workflow handling multiple concurrent issues."""
        stdout = "would reformat main.py\n5 passed, 3 failed in 2.5s"
        stderr = """
        Complexity of 22 is too high (threshold: 15)
        test.py:15: B603: subprocess call - check for execution
        Found 2 errors in 1 file
        """
        exit_code = 1

        # Extract all quality metrics
        metrics = QualityMetricsExtractor.extract(stdout, stderr)
        assert metrics.max_complexity == 22
        assert metrics.complexity_violations == 1
        assert metrics.security_issues == 1
        assert metrics.tests_failed == 3
        assert metrics.type_errors == 2
        assert metrics.formatting_issues == 1

        # Get top 3 agent recommendations
        recommendations = AgentAnalyzer.analyze(stdout, stderr, exit_code)
        assert len(recommendations) == 3

        # Verify recommendations are sorted by confidence
        confidences = [rec.confidence for rec in recommendations]
        assert confidences == sorted(confidences, reverse=True)

        # Verify specific agents are recommended
        agent_types = {rec.agent for rec in recommendations}
        assert AgentType.REFACTORING in agent_types  # Highest confidence (0.9)

    @pytest.mark.asyncio
    async def test_cache_integration_workflow(self):
        """Test that caching works correctly in workflow."""
        mock_history = [
            {
                "timestamp": (datetime.now() - timedelta(days=1)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"tests_failed": 2},
                    "agent_recommendations": [
                        {
                            "agent": "TestCreationAgent",
                            "confidence": 0.8,
                            "reason": "Test failures",
                        }
                    ],
                },
            },
            {
                "timestamp": (datetime.now() - timedelta(hours=23)).isoformat(),
                "content": "Crackerjack execution: Success",
                "metadata": {"exit_code": 0},
            },
        ]

        db = MockReflectionDatabase(mock_history)

        # First call should hit database
        async with db:
            result1 = await RecommendationEngine.analyze_history(
                db, project="cache-test", days=30, use_cache=True
            )

        # Second call should hit cache (same parameters)
        async with db:
            result2 = await RecommendationEngine.analyze_history(
                db, project="cache-test", days=30, use_cache=True
            )

        # Results should be identical
        assert result1 == result2
        assert result1["patterns"] == result2["patterns"]
        assert result1["agent_effectiveness"] == result2["agent_effectiveness"]

    @pytest.mark.asyncio
    async def test_confidence_adjustment_integration(self):
        """Test confidence adjustment based on multiple execution outcomes."""
        # Simulate mixed success/failure history for SecurityAgent
        mock_history = [
            # Successful fix
            {
                "timestamp": (datetime.now() - timedelta(days=1)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"security_issues": 1},
                    "agent_recommendations": [
                        {
                            "agent": "SecurityAgent",
                            "confidence": 0.8,
                            "reason": "Bandit security issue",
                        }
                    ],
                },
            },
            {
                "timestamp": (datetime.now() - timedelta(hours=23)).isoformat(),
                "content": "Crackerjack execution: Success",
                "metadata": {"exit_code": 0},
            },
            # Failed fix
            {
                "timestamp": (datetime.now() - timedelta(days=2)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"security_issues": 1},
                    "agent_recommendations": [
                        {
                            "agent": "SecurityAgent",
                            "confidence": 0.8,
                            "reason": "Bandit security issue",
                        }
                    ],
                },
            },
            {
                "timestamp": (datetime.now() - timedelta(days=2, hours=-1)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {"exit_code": 1},
            },
        ]

        db = MockReflectionDatabase(mock_history)
        async with db:
            history_analysis = await RecommendationEngine.analyze_history(
                db, project="test-project", days=7, use_cache=False
            )

        effectiveness = history_analysis["agent_effectiveness"][0]
        assert effectiveness.agent == AgentType.SECURITY
        # 1 success + 1 failure = 50% success rate
        assert abs(effectiveness.success_rate - 0.5) < 0.01

        # Test confidence adjustment with 50% success rate
        current_recommendation = [
            {
                "agent": AgentType.SECURITY,
                "confidence": 0.8,
                "reason": "Security issue",
                "quick_fix_command": "python -m crackerjack --ai-fix",
                "pattern_matched": r"B\d{3}:",
            }
        ]

        # Convert dict to AgentRecommendation for adjustment
        from session_mgmt_mcp.tools.agent_analyzer import AgentRecommendation

        recommendations = [
            AgentRecommendation(
                agent=rec["agent"],
                confidence=rec["confidence"],
                reason=rec["reason"],
                quick_fix_command=rec["quick_fix_command"],
                pattern_matched=rec["pattern_matched"],
            )
            for rec in current_recommendation
        ]

        adjusted = RecommendationEngine.adjust_confidence(
            recommendations, history_analysis["agent_effectiveness"]
        )

        # With 50% success rate and <5 samples, confidence should NOT change
        # RecommendationEngine requires ≥5 samples before adjusting
        assert adjusted[0].confidence == 0.8  # Unchanged

    @pytest.mark.asyncio
    async def test_no_historical_data_workflow(self):
        """Test workflow when no historical data exists (first execution)."""
        stdout = ""
        stderr = "Complexity of 18 is too high (threshold: 15)"
        exit_code = 1

        # Get initial recommendations (no history)
        recommendations = AgentAnalyzer.analyze(stdout, stderr, exit_code)
        assert len(recommendations) == 1
        initial_confidence = recommendations[0].confidence

        # No historical data
        db = MockReflectionDatabase([])
        async with db:
            history_analysis = await RecommendationEngine.analyze_history(
                db, project="new-project", days=30, use_cache=False
            )

        # No patterns or effectiveness data
        assert len(history_analysis["patterns"]) == 0
        assert len(history_analysis["agent_effectiveness"]) == 0

        # Confidence should remain unchanged (no adjustment)
        adjusted = RecommendationEngine.adjust_confidence(
            recommendations, history_analysis["agent_effectiveness"]
        )
        assert adjusted[0].confidence == initial_confidence


class TestMetricsQualityIntegration:
    """Test integration between metrics extraction and quality reporting."""

    def test_metrics_to_display_workflow(self):
        """Test complete flow from extraction to display formatting."""
        stdout = """
        coverage: 45.5%
        10 passed, 2 failed in 3.1s
        would reformat main.py
        """
        stderr = """
        Complexity of 18 is too high (threshold 15)
        test.py:15: B603: subprocess call
        Found 2 errors in 1 file
        """

        # Extract metrics
        metrics = QualityMetricsExtractor.extract(stdout, stderr)

        # Verify all metrics extracted
        assert metrics.coverage_percent == 45.5
        assert metrics.max_complexity == 18
        assert metrics.complexity_violations == 1
        assert metrics.security_issues == 1
        assert metrics.tests_passed == 10
        assert metrics.tests_failed == 2
        assert metrics.type_errors == 2
        assert metrics.formatting_issues == 1

        # Convert to dict (for storage)
        metrics_dict = metrics.to_dict()
        assert "coverage_percent" in metrics_dict
        assert "max_complexity" in metrics_dict
        # Non-zero values should be included
        assert "tests_passed" in metrics_dict  # 10 is non-zero, should be included
        assert metrics_dict["tests_passed"] == 10

        # Format for display
        display = metrics.format_for_display()
        assert "📈 **Quality Metrics**:" in display
        assert "✅ Coverage: 45.5%" in display  # Above 42% baseline
        assert "❌ Max Complexity: 18" in display  # Exceeds 15 limit
        assert "❌ Tests Failed: 2" in display

    def test_metrics_empty_to_dict(self):
        """Test that empty metrics are properly handled."""
        metrics = QualityMetrics()

        # All None/zero values should be excluded
        metrics_dict = metrics.to_dict()
        assert not metrics_dict

        # Display should be empty
        display = metrics.format_for_display()
        assert display == ""


class TestRecommendationEngineIntegration:
    """Test RecommendationEngine integration with pattern detection."""

    @pytest.mark.asyncio
    async def test_pattern_signature_uniqueness(self):
        """Test that different error patterns generate unique signatures."""
        # Two different failure scenarios
        history1 = [
            {
                "timestamp": (datetime.now() - timedelta(days=1)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"complexity_violations": 1, "max_complexity": 18},
                },
            }
        ]

        history2 = [
            {
                "timestamp": (datetime.now() - timedelta(days=1)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"security_issues": 2},
                },
            }
        ]

        db1 = MockReflectionDatabase(history1)
        db2 = MockReflectionDatabase(history2)

        async with db1:
            result1 = await RecommendationEngine.analyze_history(
                db1, project="test", days=7, use_cache=False
            )

        async with db2:
            result2 = await RecommendationEngine.analyze_history(
                db2, project="test", days=7, use_cache=False
            )

        # Patterns should have different signatures
        sig1 = result1["patterns"][0].pattern_signature if result1["patterns"] else None
        sig2 = result2["patterns"][0].pattern_signature if result2["patterns"] else None

        assert sig1 != sig2
        assert "complexity" in sig1
        # Pattern signature uses "security:N" format, not "security_issues"
        assert "security" in sig2

    @pytest.mark.asyncio
    async def test_insights_generation(self):
        """Test that historical insights are generated correctly."""
        mock_history = [
            {
                "timestamp": (datetime.now() - timedelta(days=1)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"tests_failed": 3},
                    "agent_recommendations": [
                        {
                            "agent": "TestCreationAgent",
                            "confidence": 0.8,
                            "reason": "Test failures",
                        }
                    ],
                },
            },
            {
                "timestamp": (datetime.now() - timedelta(hours=23)).isoformat(),
                "content": "Crackerjack execution: Success",
                "metadata": {"exit_code": 0},
            },
            {
                "timestamp": (datetime.now() - timedelta(days=2)).isoformat(),
                "content": "Crackerjack execution: Failed",
                "metadata": {
                    "exit_code": 1,
                    "metrics": {"tests_failed": 5},
                    "agent_recommendations": [
                        {
                            "agent": "TestCreationAgent",
                            "confidence": 0.8,
                            "reason": "Test failures",
                        }
                    ],
                },
            },
            {
                "timestamp": (datetime.now() - timedelta(days=2, hours=-1)).isoformat(),
                "content": "Crackerjack execution: Success",
                "metadata": {"exit_code": 0},
            },
        ]

        db = MockReflectionDatabase(mock_history)
        async with db:
            result = await RecommendationEngine.analyze_history(
                db, project="test-project", days=7, use_cache=False
            )

        # Should generate insights about test patterns
        insights = result["insights"]
        assert len(insights) > 0
        # Check for meaningful insight content
        assert any("TestCreationAgent" in insight for insight in insights)
