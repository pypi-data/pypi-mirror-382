{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming data: processing a websocket feed\n",
    "\n",
    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/plugboard-dev/plugboard)\n",
    "\n",
    "This model will run on a **continuous stream** of data provided by [BlueSky's firehose](https://github.com/bluesky-social/jetstream) websocket. We'll subscribe to posts from some business news feeds and then use an LLM to carry out sentiment analysis on each message.\n",
    "\n",
    "To run this model you will need to set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import typing as _t\n",
    "from getpass import getpass\n",
    "\n",
    "import httpx\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from plugboard.component import Component, IOController\n",
    "from plugboard.connector import AsyncioConnector\n",
    "from plugboard.schemas import ConnectorSpec\n",
    "from plugboard.process import LocalProcess\n",
    "from plugboard.library import FileWriter, LLMChat, WebsocketReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll subscribe to BlueSky posts from the following news outlets.\n",
    "\n",
    "* [Reuters](https://bsky.app/profile/reuters.com)\n",
    "* [Bloomberg](https://bsky.app/profile/bloomberg.com)\n",
    "* [CNBC](https://bsky.app/profile/cnbc.com)\n",
    "* [Financial Times](https://bsky.app/profile/financialtimes.com)\n",
    "* [Wall Street Journal](https://bsky.app/profile/wsj.com)\n",
    "* [Yahoo Finance](https://bsky.app/profile/yahoofinance.com)\n",
    "\n",
    "\n",
    "The BlueSky API filters according to DIDs - a unique identifier for each user that we'll need to lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_bluesky_did(client: httpx.AsyncClient, user_name: str) -> str:\n",
    "    response = await client.get(\n",
    "        \"https://bsky.social/xrpc/com.atproto.identity.resolveHandle\", params={\"handle\": user_name}\n",
    "    )\n",
    "    return response.json()[\"did\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_handles = [\n",
    "    \"reuters.com\",\n",
    "    \"bloomberg.com\",\n",
    "    \"cnbc.com\",\n",
    "    \"financialtimes.com\",\n",
    "    \"wsj.com\",\n",
    "    \"yahoofinance.com\",\n",
    "]\n",
    "async with httpx.AsyncClient() as client:\n",
    "    bluesky_dids = await asyncio.gather(\n",
    "        *[fetch_bluesky_did(client, handle) for handle in user_handles]\n",
    "    )\n",
    "# Bluesky uses the \"wantedDids\" parameter to specify the DIDs of the users we want to filter\n",
    "filter_spec = \"&\".join([f\"wantedDids={did}\" for did in bluesky_dids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the DIDs for BlueSky, setup a `WebsocketReader` to stream posts into a Plugboard process. Using the [Jetstream instructions](https://github.com/bluesky-social/jetstream?tab=readme-ov-file#consuming-jetstream) we'll filter on posts from the users we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websocket = WebsocketReader(\n",
    "    name=\"bluesky-feed\",\n",
    "    uri=f\"wss://jetstream2.us-east.bsky.network/subscribe?wantedCollections=app.bsky.feed.post&{filter_spec}\",\n",
    "    parse_json=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a `Component` to extract the post text and timestamp each message received from BlueSky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractMessage(Component):\n",
    "    \"\"\"Extracts text and timestamp from a BlueSky message dictionary.\"\"\"\n",
    "\n",
    "    io = IOController(inputs=[\"message\"], outputs=[\"text\", \"time_stamp\"])\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        try:\n",
    "            # Surround text with quotes so that is is correctly formatted in CSV output\n",
    "            self.text = f'\"{websocket.message[\"commit\"][\"record\"][\"text\"].replace(\"\\n\", \" \")}\"'\n",
    "            self.time_stamp = websocket.message[\"commit\"][\"record\"][\"createdAt\"]\n",
    "        except KeyError:\n",
    "            # Skip messages that aren't correctly formatted\n",
    "            pass\n",
    "\n",
    "\n",
    "extract = ExtractMessage(name=\"extract-message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's setup an LLM component to analyse the messages as they arrive from BlueSky and carry out sentiment analysis. We'll use the LLM in structured-output mode, so that we have known outputs from the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageInformation(BaseModel):\n",
    "    category: _t.Literal[\"markets\", \"companies\", \"economics\", \"other\"]\n",
    "    market_relevance: float = Field(..., ge=0, le=100)\n",
    "    sentiment: _t.Literal[\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are going to be shown headlines from business news services. For each headline, please provide the following:\n",
    "- The category of the headline (markets, companies, economics, other)\n",
    "- The market relevance of the headline to financial markets on a scale of 0 (least relevant) to 100 (most relevant)\n",
    "- The sentiment of the headline (positive, negative, neutral).\n",
    "\"\"\"\n",
    "\n",
    "llm = LLMChat(\n",
    "    name=\"llm\",\n",
    "    system_prompt=system_prompt,\n",
    "    llm_kwargs={\"model\": \"gpt-4o\"},\n",
    "    response_model=MessageInformation,\n",
    "    # Expand the response into separate fields\n",
    "    expand_response=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use the `FileWriter` component to save the output to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chunk size to 1 so that data is saved to disk as each message arrives\n",
    "save = FileWriter(\n",
    "    name=\"save\",\n",
    "    path=\"bluesky-messages.csv\",\n",
    "    chunk_size=1,\n",
    "    field_names=[\"text\", \"time_stamp\", \"category\", \"market_relevance\", \"sentiment\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the `LocalProcess` and connect all of the components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect = lambda in_, out_: AsyncioConnector(spec=ConnectorSpec(source=in_, target=out_))\n",
    "process = LocalProcess(\n",
    "    components=[websocket, extract, llm, save],\n",
    "    connectors=[\n",
    "        # Connect websocket to extract\n",
    "        connect(\"bluesky-feed.message\", \"extract-message.message\"),\n",
    "        # Save the time_stamp and text from the extract component\n",
    "        connect(\"extract-message.time_stamp\", \"save.time_stamp\"),\n",
    "        connect(\"extract-message.text\", \"save.text\"),\n",
    "        # Connect the extracted message to the LLM\n",
    "        connect(\"extract-message.text\", \"llm.prompt\"),\n",
    "        # Connect the LLM outputs to the save component\n",
    "        connect(\"llm.category\", \"save.category\"),\n",
    "        connect(\"llm.market_relevance\", \"save.market_relevance\"),\n",
    "        connect(\"llm.sentiment\", \"save.sentiment\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model. The websocket input will run forever, continuing to stream new data, so when you are ready to stop the process you will need to manually interrupt it. Open the output CSV file to see the data that has been captured. Keep in mind that some of the news sources publish infrequently outside of their business hours, so depending on when you run the code you might need to leave it for a while to collect some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with process:\n",
    "    await process.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
