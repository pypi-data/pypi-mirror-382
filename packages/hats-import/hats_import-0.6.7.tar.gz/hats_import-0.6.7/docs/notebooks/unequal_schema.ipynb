{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unequal schema problems\n",
    "\n",
    "There are a few ways in which parquet files written with slightly different schema can cause issues in the import pipeline. This issue most commonly arises when some portions of the data contain only empty (null) values in a column, but other portions have non-null values and so are interpreted as integer or string values. When we try to merge these partial files together later, the parquet engine does not want to perform a cast between these types and throws an error.\n",
    "\n",
    "For example, at the reduce stage, we're combining several intermediate parquet files for a single spatial tile into the final parquet file. It's possible at this stage that some files will contain only empty (null) values in a column that we expect to be a string field.\n",
    "\n",
    "e.g. \n",
    "\n",
    "#### File1\n",
    "\n",
    "| int_field | string_field | float_field |\n",
    "| --------- | ------------ | ----------  |\n",
    "|         5 |      <empty> |         3.4 |\n",
    "|         8 |      <empty> |         3.8 |\n",
    "\n",
    "which will have a schema like:\n",
    "       \n",
    "    optional int64 field_id=-1 int_field;\n",
    "    optional int32 field_id=-1 string_field **(Null)**;\n",
    "    optional double field_id=-1 float_field;\n",
    "    \n",
    "#### File2\n",
    "    \n",
    "| int_field | string_field | float_field |\n",
    "| --------- |------------- | ----------- |\n",
    "|         6 |      hello   |         4.1 |\n",
    "|         7 |      <empty> |         3.9 |\n",
    "\n",
    "will have a schema like:\n",
    "\n",
    "    optional int64 field_id=-1 int_field;\n",
    "    optional binary field_id=-1 string_field (String);\n",
    "    optional double field_id=-1 float_field;\n",
    "\n",
    "When we try to merge these files together, we see an error like the following:\n",
    "```\n",
    "Key:       4_2666\n",
    "Function:  reduce_pixel_shards\n",
    "args:      ()\n",
    "kwargs:    {...}\n",
    "Exception: \"ArrowNotImplementedError('Unsupported cast from string to null using function cast_null')\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Demonstration\n",
    "\n",
    "Here, we attempt an import with some unequal schema, and see that the attempt fails in the reducing stage, when we're trying to combine partial parquet files into a single file with common metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "\n",
    "from hats_import.pipeline import pipeline_with_client\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "from hats_import.catalog.file_readers import get_file_reader\n",
    "\n",
    "mixed_schema_csv_dir = \"../../tests/data/mixed_schema\"\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_artifact_name=\"mixed_csv_bad\",\n",
    "    input_file_list=[\n",
    "        os.path.join(mixed_schema_csv_dir, \"input_01.csv\"),\n",
    "        os.path.join(mixed_schema_csv_dir, \"input_02.csv\"),\n",
    "    ],\n",
    "    file_reader=\"csv\",\n",
    "    output_path=tmp_path.name,\n",
    "    highest_healpix_order=1,\n",
    ")\n",
    "\n",
    "with Client(n_workers=1, threads_per_worker=1) as client:\n",
    "    try:\n",
    "        pipeline_with_client(args, client)\n",
    "    except:\n",
    "        pass  # we know it's going to fail!!\n",
    "tmp_path.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overcome may of these issues by using a *parquet schema* file. This is a special kind of parquet file that only contains information on the columns (their names, data types, and additional key-value metadata).\n",
    "\n",
    "Let's take a look inside the schema structure and see the field types it expects to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "mixed_schema_csv_parquet = \"../../tests/data/mixed_schema/schema.parquet\"\n",
    "\n",
    "parquet_file = pq.ParquetFile(mixed_schema_csv_parquet)\n",
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a parquet metadata file for this data set, but we'll show you how to create one of your own later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "args = ImportArguments(\n",
    "    output_artifact_name=\"mixed_csv_good\",\n",
    "    input_file_list=[\n",
    "        os.path.join(mixed_schema_csv_dir, \"input_01.csv\"),\n",
    "        os.path.join(mixed_schema_csv_dir, \"input_02.csv\"),\n",
    "    ],\n",
    "    output_path=tmp_path.name,\n",
    "    highest_healpix_order=1,\n",
    "    file_reader=get_file_reader(\"csv\", schema_file=mixed_schema_csv_parquet),\n",
    "    use_schema_file=mixed_schema_csv_parquet,\n",
    ")\n",
    "with Client(n_workers=1, threads_per_worker=1) as client:\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a new parquet schema file\n",
    "\n",
    "There are a few different strategies we can use to create a schema file:\n",
    "\n",
    "* using some string representations of pandas datatypes\n",
    "* using an explicit list of pyarrow data types\n",
    "* and many more!\n",
    "\n",
    "We'll stick to these two, since they exercise the most common code paths through schema generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pandas type strings\n",
    "\n",
    "Something like the `tic_types.csv` file contains a list of the columns that the TIC data will contain, in a table like:\n",
    "\n",
    "```\n",
    "name,type\n",
    "ID,Int64\n",
    "version,str\n",
    "HIP,Int32\n",
    "TYC,str\n",
    "etc...\n",
    "```\n",
    "\n",
    "Such files are a common way to send type information when the data files have no header.\n",
    "\n",
    "In this method, we will use pandas' type parsing to convert these strings into understood data types, and create the relevant parquet metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Fetch the name/type information from a file.\n",
    "type_list_frame = pd.read_csv(\"../static/tic_types.csv\")\n",
    "\n",
    "## For each row, add to a dictionary with name and a pandas series with the parsed data type.\n",
    "## \"str\" is not understood as \"string\", so add a special case.\n",
    "type_map = {\n",
    "    row[\"name\"]: pd.Series(dtype=(\"string\" if row[\"type\"] == \"str\" else row[\"type\"]))\n",
    "    for _, row in type_list_frame.iterrows()\n",
    "}\n",
    "dtype_frame = pd.DataFrame(type_map)\n",
    "\n",
    "## Now write our empty data frame to a parquet file.\n",
    "schema_file = os.path.join(tmp_path.name, \"schema_from_csv_list.parquet\")\n",
    "dtype_frame.to_parquet(schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the parquet file's metadata and see if it matches what we'd expect.\n",
    "\n",
    "You'll notice that that there are A LOT of fields, and this is why you might not want to deal with column-by-column type discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pq.ParquetFile(schema_file)\n",
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explict list of pyarrow data types\n",
    "\n",
    "Here, we know what pyarrow types we want to use for each column. This is helpful if you know you want nullable, or you know you DON'T want to use nullable types, but it requires some deeper knowledge of pyarrow data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "## List all of our columns as pyarrow fields.\n",
    "schema_from_pyarrow = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"ra\", pa.float64()),\n",
    "        pa.field(\"dec\", pa.float64()),\n",
    "        pa.field(\"ra_error\", pa.float64()),\n",
    "        pa.field(\"dec_error\", pa.float64()),\n",
    "        pa.field(\"comment\", pa.string()),\n",
    "        pa.field(\"code\", pa.string()),\n",
    "    ]\n",
    ")\n",
    "schema_file = os.path.join(tmp_path.name, \"schema_from_pyarrow.parquet\")\n",
    "pq.write_metadata(schema_from_pyarrow, schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll check that the generated parquet metadata is what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pq.ParquetFile(schema_file)\n",
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hatsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
