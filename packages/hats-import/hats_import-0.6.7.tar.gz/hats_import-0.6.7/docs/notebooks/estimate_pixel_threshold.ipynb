{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98af180d",
   "metadata": {},
   "source": [
    "# Estimate pixel threshold\n",
    "\n",
    "For best performance, we try to keep catalog parquet files between 200-800MB in size.\n",
    "\n",
    "**Background**\n",
    "\n",
    "When creating a new catalog through the hats-import process, we try to create partitions with approximately the same number of rows per partition. This isn't perfect, because the sky is uneven, but we still try to create smaller-area pixels in more dense areas, and larger-area pixels in less dense areas. We use the argument `pixel_threshold` and will split a partition into smaller healpix pixels until the number of rows is smaller than `pixel_threshold`.\n",
    "\n",
    "We do this to increase parallelization of reads and downstream analysis: if the files are around the same size, and operations on each partition take around the same amount of time, we're not as likely to be waiting on a single process to complete for the whole pipeline to complete.\n",
    "\n",
    "In addition, a single catalog file should not exceed a couple GB - we're going to need to read the whole thing into memory, so it needs to fit!\n",
    "\n",
    "**Objective**\n",
    "\n",
    "In this notebook, we'll go over *one* strategy for estimating the `pixel_threshold` argument you can use when importing a new catalog into hats format.\n",
    "\n",
    "This is not guaranteed to give you optimal results, but it could give you some hints toward *better* results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86458c",
   "metadata": {},
   "source": [
    "## Create a sample parquet file\n",
    "\n",
    "The first step is to read in your survey data in its original form, and convert a sample into parquet. This has a few benefits:\n",
    "- parquet uses compression in various ways, and by creating the sample, we can get a sense of both the overall and field-level compression with real data\n",
    "- using the importer `FileReader` interface now sets you up for more success when you get around to importing!\n",
    "\n",
    "If your data is already in parquet format, just change the `sample_parquet_file` path to an existing file, and don't run the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd94480",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change this path!!!\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "sample_parquet_file = os.path.join(tmp_path.name, \"sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a53db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hats_import.catalog.file_readers import CsvReader\n",
    "\n",
    "### Change this path!!!\n",
    "input_file = \"../../tests/data/small_sky/catalog.csv\"\n",
    "\n",
    "file_reader = CsvReader(chunksize=5_000)\n",
    "\n",
    "next(file_reader.read(input_file)).to_parquet(sample_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124eb444",
   "metadata": {},
   "source": [
    "## Inspect parquet file and metadata\n",
    "\n",
    "Now that we have parsed our survey data into parquet, we can check what the data will look like when it's imported into hats format.\n",
    "\n",
    "If you're just here to get a naive estimate for your pixel threshold, we'll do that first, then take a look at some other parquet characteristics later for the curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "sample_file_size = os.path.getsize(sample_parquet_file)\n",
    "parquet_file = pq.ParquetFile(sample_parquet_file)\n",
    "num_rows = parquet_file.metadata.num_rows\n",
    "\n",
    "## 300MB\n",
    "ideal_file_small = 300 * 1024 * 1024\n",
    "## 1G\n",
    "ideal_file_large = 1024 * 1024 * 1024\n",
    "\n",
    "threshold_small = ideal_file_small / sample_file_size * num_rows\n",
    "threshold_large = ideal_file_large / sample_file_size * num_rows\n",
    "\n",
    "print(f\"threshold between {int(threshold_small):_} and {int(threshold_large):_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23971c38",
   "metadata": {},
   "source": [
    "## Want to see more?\n",
    "\n",
    "I'm so glad you're still here! I have more to show you!\n",
    "\n",
    "The first step below shows us the file-level metadata, as stored by parquet. The number of columns here SHOULD match your expectations on the number of columns in your survey data.\n",
    "\n",
    "The `serialized_size` value is just the size of the total metadata, not the size of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc402acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile(sample_parquet_file)\n",
    "print(parquet_file.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835b6d9",
   "metadata": {},
   "source": [
    "The next step is to look at the column-level metadata. You can check that the on-disk type of each column matches your expectation of the data. Note that for some integer types, the on-disk type may be a smaller int than originally set (e.g. `bitWidth=8` or `16`). This is part of parquet's multi-part compression strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b593b",
   "metadata": {},
   "source": [
    "Parquet will also perform some column-level compression, so not all columns with the same type will take up the same space on disk.\n",
    "\n",
    "Below, we inspect the row and column group metadata to show the compressed size of the fields on disk. The last column, `percent`, show the percent of total size taken up by the column.\n",
    "\n",
    "You *can* use this to inform which columns you keep when importing a catalog into hats format. e.g. if some columns are less useful for your science, and take up a lot of space, maybe leave them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf152f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_cols = parquet_file.metadata.num_columns\n",
    "num_row_groups = parquet_file.metadata.num_row_groups\n",
    "sizes = np.zeros(num_cols)\n",
    "\n",
    "for rg in range(num_row_groups):\n",
    "    for col in range(num_cols):\n",
    "        sizes[col] += parquet_file.metadata.row_group(rg).column(col).total_compressed_size\n",
    "\n",
    "## This is just an attempt at pretty formatting\n",
    "percents = [f\"{s/sizes.sum()*100:.1f}\" for s in sizes]\n",
    "pd.DataFrame({\"name\": parquet_file.schema.names, \"size\": sizes.astype(int), \"percent\": percents}).sort_values(\n",
    "    \"size\", ascending=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hatsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
